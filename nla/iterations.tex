
\begin{Definition}{jacobi}
  The \define{Jacobi iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
     x^{(k+1)}_i = \frac1{a_{ii}}\left( b_i - \sum_{j\neq i} a_{ij}x^{(k)}_j\right).
  \end{gather}
\end{Definition}

\begin{Definition}{gauss-seidel}
  The \define{Gauss-Seidel iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
    x^{(k+1)}_i = \frac1{a_{ii}}
    \left( b_i
      - \sum_{j< i} a_{ij}x^{(k+1)}_j
      - \sum_{j> i} a_{ij}x^{(k)}_j
  \right).
  \end{gather}
\end{Definition}

\begin{Definition}{richardson-iteration}
  The \define{Richardson iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)} - 
    \omega_k\left( \mata\vx^{(k)} - \vb
  \right).
\end{gather}
The relaxation parameter $\omega_k$ must be chosen carefully to obtain
convergence.
\end{Definition}

\begin{Definition}{matrix-iteration}
  We call a \define{matrix iteration} any iterative method of the structure
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg,
  \end{gather}
  with an \define{iteration matrix} $\matm\in\Rnn$ and an inhomogeneity $\vg$.
\end{Definition}

\begin{Lemma}{Jacobi-gs-matrices}
  Let $\mata = \matd +\matl+\matu$ be the decomposition of $\mata$
  into the diagonal, and the strict upper and lower triangles,
  respectively. Let $\vb$ be the right hand side of the linear system
  $\mata\vx=\vb$. Then, Jacobi iteration has the matrix form
  \begin{gather}
    x^{k+1} = \bigl(\id-\matd^{-1}\mata\bigr) x^{k} + \matd^{-1} \vb.
  \end{gather}
  The Gauss-Seidel iteration has the matrix form
  \begin{gather}
    x^{k+1} = \bigl(\id-(\matd+\matl)^{-1}\mata\bigr) x^{k} + (\matd+\matl)^{-1} \vb.
  \end{gather}
\end{Lemma}

\begin{Theorem*}{bfpt}{Banach fixed-point theorem}
  Let $V$ be a vector space with norm $\norm{\cdot}$ and $M$ be a
  closed subset of $V$. Let $F\colon M\to M$ be a \define{contraction},
  that is, there is a \define{contraction number} $q\in[0,1)$ such that
  \begin{gather}
    \norm{F(\vx)-F(\vy)} \le q \norm{\vx-\vy}\qquad\forall \vx,\vy\in M.
  \end{gather}
  Then, there is a unique \define{fixed-point} $\vx^*\in M$ with the property
  \begin{gather}
    F(\vx^*) = \vx^*.
  \end{gather}
  The \define{fixed-point iteration}
  \begin{gather}
    \vx^{(k+1)} = F\bigl(\vx^{(k)}\bigr)
  \end{gather}
  converges to $\vx^*$ for any $\vx^{(0)}\in M$.
%and there holds
%  \begin{gather}
%    \norm{\vx^{(k)}-x^*} \le \frac{q^n}{1-q}\norm{\vx^{(1)}-\vx^{(0)}}.
%  \end{gather}
\end{Theorem*}

\begin{Corollary}{bfp-estimates}
  Let $F$ define a fixed-point iteration with contraction number $q<1$
  and let $x^*$ be the unique fixed-point. Then, the following
  estimates hold:
  \begin{align}
    \norm{x^{(k)} - x^*} &\le \frac{q}{1-q} \norm{x^{(k)}-x^{(k-1)}}\\
    \norm{x^{(k)} - x^*} &\le \frac{q^k}{1-q} \norm{x^{(1)}-x^{(0)}}
  \end{align}
\end{Corollary}  

\begin{Corollary}{matrix-norm-convergence}
  Let $\norm{\matm} < 1$ for some operator norm of a vector norm $\norm{\cdot}$ on $\R^n$. Then, the matrix iteration
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg
  \end{gather}
  converges for any initial value $\vx^{(0)}\in\R^n$.
\end{Corollary}

\begin{Example}{convergence-row-sum}
  If the matrix $\mata\in\Rnn$ is irreducibly diagonally dominant, that is,
  \begin{gather}
    \abs{a_{ii}} \ge \sum_{j=1}^n \abs{a_{ij}},\qquad i=1,\dots,n,
  \end{gather}
  the inequality holds strictly in at least one row, and the matrix is
  irreducible in the sense that for any two indices $i$ and $j$ there
  is a chain of nonzero entries, such that
  \begin{gather}
    a_{i,k_{1}}, a_{k_{1},k_{2}}, a_{k_{2},k_{3}},\dots, a_{k_{m},j}.
  \end{gather}
  Then, the Jacobi and Gauss-Seidel methods are contractions in the
  \putindex{row sum norm} and thus convergent.
\end{Example}

\begin{Example}{matrix-norm-convergence}
  Let $\mata\in\R^{2\times 2}$ be a rotation by 45\textdegree combined
  with a scaling,
  \begin{gather}
    \mata = 0.9
    \begin{pmatrix}
      \cos \tfrac\pi4 &\sin\tfrac\pi4\\
      -\sin\tfrac\pi4&\cos\tfrac\pi4
    \end{pmatrix}.
  \end{gather}
  Its spectral norm is
  \begin{gather}
    \norm{\mata}_2 = 0.9,
  \end{gather}
  while
  \begin{gather}
    \norm{\mata}_1 \ge 0.9\sqrt2 > 1.2
  \end{gather}
  which can be seen by mapping the vector $(1,0)^T$. We conclude, that
  a matrix iteration might be a contraction with respect to one norm,
  but not with respect to another.
\end{Example}

\begin{remark}
  The statement of \slideref{Corollary}{matrix-norm-convergence}
  implies, that it is sufficient to find one vector norm such that the
  iteration is a contraction to prove convergence. The example shows
  that this can only be a sufficient condition, but not a necessary
  one.

  After the following Lemma, we will prove a result which provides us
  with a sufficiant \emph{and} necessary condition.
\end{remark}

\begin{Lemma}{norm-spectral-radius}
  For any matrix $\mata\in\Rnn$ and for any $\epsilon>0$ there exist a
  vector norm $\norm{\cdot}_{\mata,\epsilon}$ and associated operator norm
  denoted by the same symbol, such that
  \begin{gather}
    \rho(\mata) \le \norm{\mata}_{\mata,\epsilon} \le \rho(\mata)+\epsilon.
  \end{gather}
\end{Lemma}

\begin{proof}
  See~\cite[Lemma 3.1]{Rannacher18nla}.
\end{proof}

\begin{Theorem}{matrix-radius-convergence}
  The matrix iteration
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg
  \end{gather}
  converges to a fixed-point $x^*$ for any start vector $x^{(0)}$,
  if and only if for the spectral radius there holds
  \begin{gather}
    \rho(\matm) < 1.
  \end{gather}
  Then, there holds asymptotically only
  \begin{gather}
    \operatorname*{lim\,sup}_{k\to\infty} \frac{\norm{x^{(k+1)}-x^*}}{\norm{x^{(k)}-x^*}}
    \le \rho(\matm).
  \end{gather}
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 3.1]{Rannacher18nla}.
\end{proof}

\begin{Remark}{contraction-vs-convergence}
  While \slideref{Theorem}{matrix-radius-convergence} has the
  mathematically more pleasing statement ``if and only if'', its
  assumptions do not establish a contraction. In particular, the norms
  of iterates may grow in early steps of the iteration and only then
  start decreasing, something which is not desired from a practical
  point of view.

  For practical purposes, we typically investigate contraction with
  respect to a suitably chosen norm.
\end{Remark}

\begin{Theorem}{richardson-convergence}
  Let $\mata$ be symmetric, positive definite, such that
  $\sigma(\mata)\subset [\lambda_{\min},\lambda_{\max}]$. Then, the Richardson method converges for
  \begin{gather}
    \omega_k = \omega < \frac2{\lambda_{\max}}.
  \end{gather}
  The optimal relaxation parameter is
  \begin{gather}
    \omega = \frac1{\lambda_{\min}+\lambda_{\max}}.
  \end{gather}
  The contraction number and spectral radius of the iteration matrix are both
  \begin{gather}
    \rho = \frac{\lambda_{max}-\lambda_{\min}}{\lambda_{max}+\lambda_{\min}}
    =\frac{\cond_2(\mata) -1}{\cond_2(\mata) +1}.
  \end{gather}
\end{Theorem}

\begin{proof}
  Homework.
\end{proof}

\begin{Remark}{richardson-convergence}
  The qualitative statement of
  \slideref{Theorem}{richardson-convergence} holds under the weaker
  assumption that $\mata$ is diagonalizable and that the real part of
  all eigenvalues is positive.

  An estimate for the spectral radius can be obtained even without
  assuming that $\mata$ is diagonalizable.
\end{Remark}

\begin{remark}
  Let $e_k$ be some measure of the error of a fixed-point iteration
  with contraction number $\rho$ after $k$ steps. Then, there holds
  \begin{gather}
    e_k \le \rho^k e_0.
  \end{gather}
  Thus, the number of steps needed to obtain a (relative) reduction of
  the error by a prescribed relative tolerance $\epsilon$, that is, to
  achieve $e_k/e_0\le\epsilon$, is
  \begin{gather}
    \label{eq:krylov:steps-convergence}
    k \ge \frac{\log\epsilon}{\log\rho}.
  \end{gather}
  From this formula, we realize that the number of steps of such a
  method grows linearly with the logarithm of the tolerance. Further,
  it is inverse proportional to the logarithm of the contraction
  number $\rho$.

  Note that logarithms with respect to any base can be used
  in~\eqref{eq:krylov:steps-convergence}, since the quotient is
  independent of the base.
\end{remark}

\begin{Definition}{convergence-rate-logarithmic}
  The (logarithmic) \define{convergence rate} of a contraction with
  \putindex{contraction number} $\rho$ is
  \begin{gather}
    r_c = -\log_{10} \rho.
  \end{gather}
  
  An iteration with convergence rate $r_c$ yields an error reduction
  of $10^{-k}$ in $\nicefrac{k}{r_c}$ iteration steps.
\end{Definition}

\begin{remark}
  The logarithmic convergence rate allows us to directly compare two
  iterative methods. Let's say, there are two iterations $M_1$ and
  $M_2$ and the convergence rate of $M_2$ ist twice the rate of
  $M_1$. Then, given an initial vector, $M_1$ will need twice as many
  steps compared to $M_2$ to reach the same accuracy.

  This implies that $M_2$ is favorable, if the effort for each step is
  less than twice the effort for $M_1$. If it is more than twice, then
  $M_1$ is the faster method in spite of the slower convergence.
\end{remark}

\begin{Corollary}{convergence-rate-logarithmic}
  Let there be iterative methods with numerical efforts $n_i$ and
  logarithmic convergence rates $r_i$. Then, the most efficient method
  is the one where $\nicefrac{n_i}{r_i}$ is smallest.
\end{Corollary}

\begin{remark}
  Typically, it is observed that iterative methods will start with
  faster convergence, but then slow down, such that after some steps
  the error reduction is almost equal to the theoretical contraction
  number. This is due to the fact that eigenvectors corresponding to a
  less dominant eigenvalue are reduced fast and become
  irrelevant. Hence, we can also estimate the contraction number from
  a running iteration.

  Given three consecutive vectors, the contraction property tells us
  that
  \begin{gather}
    \norm*{x^{(k+1)} - x^{(k)}} \le \rho \norm*{x^{(k)}-x^{(k-1)}}.
  \end{gather}
  If we now assume that we are in a regime where this is almost equal, we obtain
  \begin{gather}
    q \gtrapprox \frac{\norm*{x^{(k+1)} - x^{(k)}}}{\norm*{x^{(k)} - x^{(k-1)}}}.
  \end{gather}
  This estimate can be improved if it is monitored over serveral steps
  to gain confidence.

  Note that once $q$ has been estimated, the error can be estimated by
  \slideref{Corollary}{bfp-estimates}, namely
  \begin{gather}
    \norm{x^{(k)} - x^*} \le \frac{q}{1-q} \norm{x^{(k)}-x^{(k-1)}}.
  \end{gather}
  Observe how this estimate deteriorates if $q$ is close to one.
\end{remark}

\begin{Definition}{convergence-rate-observed}
  The \define{observed convergence rate} of $k$ steps of a
  method with error measure $e_k$ is
  \begin{gather}
    \label{eq:iterations:obscon:1}
    \overline{r_c} = \frac1k \log_{10}\frac{e_0}{e_k}.
  \end{gather}
\end{Definition}

\begin{remark}
  The averaging over several steps serves the purpose of making the
  estimate more robust. But, as already discussed in the previous
  remark, the actual contraction in the first steps is often much
  better than in later steps. Therefore, it may be advisable not to
  start with the initial error, but with the error at a later step and
  modify the estimate accordingly.

  The measure for the error may be a computation of the actual error
  for model cases where the solution is known. It might also be an
  estimate like in equation~\eqref{eq:iterations:obscon:1}.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
