
\begin{Definition}{jacobi}
  The \define{Jacobi iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
     x^{(k+1)}_i = \frac1{a_{ii}}\left( b_i - \sum_{j\neq i} a_{ij}x^{(k)}_j\right).
  \end{gather}
\end{Definition}

\begin{Definition}{gauss-seidel}
  The \define{Gauss-Seidel iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
    x^{(k+1)}_i = \frac1{a_{ii}}
    \left( b_i
      - \sum_{j< i} a_{ij}x^{(k+1)}_j
      - \sum_{j> i} a_{ij}x^{(k)}_j
  \right).
  \end{gather}
\end{Definition}

\begin{Definition}{richardson-iteration}
  The \define{Richardson iteration} for a matrix $\mata\in\Rnn$ and a
  right hand side vector $\vb\in \R^n$ generates the iterate
  $\vx^{(k+1)}\in \R^n$ from $\vx^{(k)}\in \R^n$ as follows:
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)} - 
    \omega_k\left( \mata\vx^{(k)} - \vb
  \right).
\end{gather}
The relaxation parameter $\omega_k$ must be chosen carefully to obtain
convergence.
\end{Definition}

\begin{Definition}{matrix-iteration}
  We call a \define{matrix iteration} any iterative method of the structure
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg,
  \end{gather}
  with an \define{iteration matrix} $\matm\in\Rnn$ and an inhomogeneity $\vg$.
\end{Definition}

\begin{Theorem*}{bfpt}{Banach fixed-point theorem}
  Let $V$ be a vector space with norm $\norm{\cdot}$ and $M$ be a
  closed subset of $V$. Let $F\colon M\to M$ be a \define{contraction},
  that is, there is a \define{contraction number} $q\in[0,1)$ such that
  \begin{gather}
    \norm{F(\vx)-F(\vy)} \le q \norm{\vx-\vy}\qquad\forall \vx,\vy\in M.
  \end{gather}
  Then, there is a unique \define{fixed-point} $\vx^*\in M$ with the property
  \begin{gather}
    F(\vx^*) = \vx^*.
  \end{gather}
  The \define{fixed-point iteration}
  \begin{gather}
    \vx^{(k+1)} = F\bigl(\vx^{(k)}\bigr)
  \end{gather}
  converges to $\vx^*$ for any $\vx^{(0)}\in M$.
%and there holds
%  \begin{gather}
%    \norm{\vx^{(k)}-x^*} \le \frac{q^n}{1-q}\norm{\vx^{(1)}-\vx^{(0)}}.
%  \end{gather}
\end{Theorem*}

\begin{Corollary}{bfp-estimates}
  Let $F$ define a fixed-point iteration with contraction number $q<1$
  and let $x^*$ be the unique fixed-point. Then, the following
  estimates hold:
  \begin{align}
    \norm{x^{(k)} - x^*} &\le \frac{q}{1-q} \norm{x^{(k)}-x^{(k-1)}}\\
    \norm{x^{(k)} - x^*} &\le \frac{q^k}{1-q} \norm{x^{(1)}-x^{(0)}}
  \end{align}
\end{Corollary}  

\begin{Corollary}{matrix-norm-convergence}
  Let $\norm{\matm} < 1$ for some operator norm of a vector norm $\norm{\cdot}$ on $\R^n$. Then, the matrix iteration
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg
  \end{gather}
  converges for any initial value $\vx^{(0)}\in\R^n$.
\end{Corollary}

\begin{Example}{matrix-norm-convergence}
  Let $\mata\in\R^{2\times 2}$ be a rotation by 45\textdegree combined
  with a scaling,
  \begin{gather}
    \mata = 0.9
    \begin{pmatrix}
      \cos \tfrac\pi4 &\sin\tfrac\pi4\\
      -\sin\tfrac\pi4&\cos\tfrac\pi4
    \end{pmatrix}.
  \end{gather}
  Its spectral norm is
  \begin{gather}
    \norm{\mata}_2 = 0.9,
  \end{gather}
  while
  \begin{gather}
    \norm{\mata}_1 \ge 0.9\sqrt2 > 1.2
  \end{gather}
  which can be seen by mapping the vector $(1,0)^T$. We conclude, that
  a matrix iteration might be a contraction with respect to one norm,
  but not with respect to another.
\end{Example}

\begin{remark}
  The statement of \slideref{Corollary}{matrix-norm-convergence}
  implies, that it is sufficient to find one vector norm such that the
  iteration is a contraction to prove convergence. The example shows
  that this can only be a sufficient condition, but not a necessary
  one.

  After the following Lemma, we will prove a result which provides us
  with a sufficiant \emph{and} necessary condition.
\end{remark}

\begin{Lemma}{norm-spectral-radius}
  For any matrix $\mata\in\Rnn$ and for any $\epsilon>0$ there exist a
  vector norm $\norm{\cdot}_{\mata,\epsilon}$ and associated operator norm
  denoted by the same symbol, such that
  \begin{gather}
    \rho(\mata) \le \norm{\mata}_{\mata,\epsilon} \le \rho(\mata)+\epsilon.
  \end{gather}
\end{Lemma}

\begin{proof}
  See~\cite[Lemma 3.1]{Rannacher18nla}.
\end{proof}

\begin{Theorem}{matrix-radius-convergence}
  The matrix iteration
  \begin{gather}
    \vx^{(k+1)} = \matm \vx^{(k)} + \vg
  \end{gather}
  converges to a fixed-point $x^*$ for any start vector $x^{(0)}$,
  if and only if for the spectral radius there holds
  \begin{gather}
    \rho(\matm) < 1.
  \end{gather}
  Then, there holds asymptotically only
  \begin{gather}
    \operatorname*{lim\,sup}_{k\to\infty} \frac{\norm{x^{(k+1)}-x^*}}{\norm{x^{(k)}-x^*}}
    < \rho(\matm).
  \end{gather}
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 3.1]{Rannacher18nla}.
\end{proof}

\begin{remark}
  While \slideref{Theorem}{matrix-radius-convergence} has the
  mathematically more pleasing statement ``if and only if'', its
  assumptions do not establish a contraction. In particular, the norms
  of iterates may grow in early steps of the iteration and only then
  start decreasing, something which is not desired from a practical
  point of view.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
