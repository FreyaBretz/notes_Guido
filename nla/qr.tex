\subsection{Definition of the methods}

% \begin{Algorithm*}{subspace-iteration}{Orthogonal subspace iteration}
%   Let $\mata\in\Cnn$, $\matx_0 \in \C^{n\times m}$.\\
%   For $k=0,\ldots$ until convergence repeat
%   \begin{itemize}
%   \item $\matz_k = \mata \matx_k$.
%   \item $\matx_{k+1}$ is orthonormalization of $\matz_k$
%   \end{itemize}
% \end{Algorithm*}

\begin{Algorithm*}{subspace-iteration}{Orthogonal subspace iteration}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$ and choose $\matq_0 \in \C^{n\times m}$ orthonormal.
    \For {$k=1,\ldots$ until convergence}
    \State $\matz_k \gets \mata \matq_{k-1}$
    \State $\matq_k\matr_k \gets \matz_k$ \Comment{QR factorization}
    \State {$\lambda_i \approx \vq_{k,i}^* \mata \vq_{k,i},\qquad i=1,\dots,m$}\
    \Comment{Rayleigh quotient}
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  If the iteration converges, then there is an orthogonal matrix $\matq$ such that $\matq_k\to \matq$.
  From the two assignments of the algorithm, we get
  \begin{gather}
    \matq^* \mata \matq = \matq^* \matz = \matq^*\matq\matr = \matr,
  \end{gather}
  where $\matr\leftarrow\matr_k$, that is, the matrices
  $\mata_k = \matq_k^*\mata\matq_k$ converge to an upper triangular
  matrix with the converged eigenvalues on the diagonal. Our next goal
  ist the modification of the orthogonal subspace iteration, such that
  we compute the sequence $\mata_k$ directly, without the intermediate
  $\matz_k$. To this end, we assume $m=n$, that is, we compute the
  whole spectrum of $\mata$. By lines 2 and 3 of the algorithm, we have
  \begin{gather*}
    \mata_{k-1} = \matq_{k-1}^*\mata\matq_{k-1} = \left(\matq_{k-1}^*\matq_{k}\right)\matr_k,
  \end{gather*}
  where we see that on the right we have obtained the QR factorization of $\mata_{k-1}$.
  On the other hand,
  \begin{align*}
    \mata_k
    &= \matq_k^*\mata\matq\\
    &= \matq_k^*\mata\matq_{k-1}\matq_{k-1}^*\matq_k\\
    &= \matq_k^*\matz_k\matq_{k-1}^*\matq_k\\
    &= \matr_k \left(\matq_{k-1}^*\matq_k\right).
  \end{align*}
  Thus, we see that $\mata_k$ is obtained by multiplying the QR
  factorization of $\mata_{k-1}$ in reverse order. Note that $\matq_k$
  in the following algorithm differs from $\matq_k$ in the previous!
\end{remark}

\begin{Algorithm*}{qr-iteration}{QR iteration}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$ and choose $\matq_0 = \id$.
    \State $\mata_0 \gets \matq_0^*\mata\matq_0$
    \For{$k=1,\dots$ until convergence}
    \State $\matq_k\matr_k \gets \mata_{k-1}$ \Comment{QR factorization}
    \State $\mata_{k} = \matr_k\matq_k$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

%\subsection{Analysis}
\begin{Theorem*}{schur-canonical}{Schur canonical form}
  For every matrix $\mata\in\Cnn$ there are a unitary matrix
  $\matq\in\Cnn$ and an upper triangular matrix $\matr\in\Cnn$ such
  that
  \begin{gather}
    \mata = \matq \matr \matq^*.
  \end{gather}
  The diagonal entries of $\matr$ are the eigenvalues of $A$. The
  column vectors of $\matq$ are called \define{Schur vectors}.
\end{Theorem*}

\begin{Lemma}{schur-canonical-1}
  For any $k\le n$ the span of the Schur vectors
  $\vq_1,\dots,\vq_k$ is invariant under the action of $\mata$.

  For $\matq_k = (\vq_1\dots\vq_k)$ and $\matr_k$ the upper left $k\times k$ block of $\matr$, there holds
  \begin{gather}
    \mata\matq_k = \matq_k \matr_k.
  \end{gather}
\end{Lemma}

\begin{Lemma}{schur-canonical-2}
  The Schur vectors depend on the order chosen for the eigenvalues,
  and in case of geometric multiplicity, the eigenvectors,
  respectively. They are determined up to factors $e^{i\phi}$
\end{Lemma}

\begin{Definition}{dominant-invariant-subspace}
  Let $\mata\in\Cnn$ be a matrix with Schur decomposition
  $\mata = \matq^*\matr\matq$ where the eigenvalues are such that
  \begin{gather}
    \abs{\lambda_1}\ge\abs{\lambda_2}\ge\dots\ge\abs{\lambda_n}.
  \end{gather}
  If $\abs{\lambda_r}>\abs{\lambda_{r+1}}$, we define the
  \define{dominant invariant subspace} of dimension $r$ as
  \begin{gather}
    D_r(\mata) = \spann{\vq_1,\dots,\vq_r}.
  \end{gather}
\end{Definition}

\begin{Theorem}{convergence-subspace-iteration}
  Let $\mata\in\Cnn$ and
  \begin{gather}
    \abs{\lambda_1} >
    \abs{\lambda_2}>\dots>\abs{\lambda_m}>\abs{\lambda}
  \end{gather}
  for all remaining eigenvalues $\lambda\in\sigma(\mata)$. Choose
  $\matq_0 = (\vq^{(0)}_1,\dots,\vq^{(0)}_{m})$ such that these vectors span
  the \putindex{dominant invariant subspace} $D_m(\mata)$. Then, for $j=1,\dots,m$
  \begin{gather}
    \dist\left(D_j(\mata),\ran{\matq^{(k)}}\right)
    = \bigo \left(\abs*{\frac{\lambda_{j+1}}{\lambda_j}}^k\right).
  \end{gather}
  The constant depends on the separation of the eigenvalues and the deviation of $\mata$ from normality.
%   Then, the $j$-th column of $\matx_k$ converges to $\vq_j$ for $j=1,\dots,m$ up to a factor $e^{i\phi}$.
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 7.3-1]{GolubVanLoan83}.
\end{proof}

\begin{Lemma}{qr-1}
  The matrices $\mata_k$ of the QR-iteration have the following properties:
  \begin{enumerate}
  \item $\mata_{k+1} = \matq_k^*\mata_k\matq_k = \matq_k^*\dots\matq_1^*A\matq_1\dots\matq_k$.
  \item $\mata^k=\matq_1\dots\matq_k\matr_k\dots\matr_1$.
  \item If $\mata$ is normal, so is $\mata_k$ for any $k$.
  \item If $\mata$ is complex symmetric, so is $\mata_k$ for any $k$.
  \end{enumerate}
\end{Lemma}

\begin{proof}
  For the first relation, we compute
  \begin{gather}
    \matq_k\mata_{k+1}\matq_k^* = \matq_k\matr_k\matq_k\matq_k^* = \mata_k.
  \end{gather}
  The second, we prove by induction, where
  $\mata = \mata_0 = \matq_1\matr_1$ follows directly from the first
  stek of the algorithm. For the induction step, we abbreviate
  \begin{gather}
    \matp_k = \matq_1\dots\matq_k,\qquad \matu_k = \matr_k\dots\matr_1.
  \end{gather}
  Assuming $\mata^k = \matp_k\matu_k$, we obtain
  \begin{gather}
    \mata^{k+1} = \mata\matp_k\matu_k = \matp_k\mata_{k+1}\matu_k
    = \matp_k\matq_{k+1}\matr_{k+1}\matu_k = \matp_{k+1}\matu_{k+1}.
  \end{gather}
\end{proof}
\subsection{Implementation issues}
\begin{intro}
  In each step of the QR-iteration, a QR factorization of the matrix
  is needed, which requires $\bigo(n^3)$ operations. Thus, the
  complexity of the iteration is highly unfavorable. The following
  discussion will provide us with means to reduce the complexity of
  the QR factorization to $\bigo(n^2)$, in the symmetric case even to
  $\bigo(n)$.
\end{intro}

\begin{Definition}{hessenberg}
  A matrix is in \define{Hessenberg form} or is a \define{Hessenberg
    matrix}, if all its entries below the first subdiagonal are zero. Visually,
  \begin{gather}
    H = 
    \begin{pmatrix}
      *&*&*&*&*&*\\
      *&*&*&*&*&*\\
      0&*&*&*&*&*\\
      0&0&*&*&*&*\\
      0&0&0&*&*&*\\
      0&0&0&0&*&*
    \end{pmatrix}
  \end{gather}
  A symmetric or Hermitian Hessenberg matrix is \define{tridiagonal}.
\end{Definition}

\begin{Theorem}{Hessenberg-qr}
  The QR factorization of a Hessenberg matrix $\matH$ can be obtained
  by $n-1$ givens rotations. The matrix $\matr\matq$ is again in
  Hessenberg form. For a (complex) symmetric matrix $\matH$, the
  matrix $\matr\matq$ is even tridiagonal and (complex) symmetric.
\end{Theorem}

\begin{Corollary}{Hessenberg-qr}
  The complexity of each step of a QR-iteration for Hessenberg matrices is $\bigo(n^2)$. For tridiagonal (complex) symmetric matrices, it is $\bigo(n)$.
\end{Corollary}

\begin{Theorem}{Hessenberg-householder}
  Every matrix $\mata\in\Cnn$ is unitarily similar to a Hessenberg matrix $\matH$, that is,
  \begin{gather}
    \matH = \matq \mata \matq^*.
  \end{gather}
  The matrix $\matq$ can be obtained by $n-2$ \putindex{Householder
    reflections}.
\end{Theorem}

\begin{Algorithm*}{qr-method}{The QR-Method}
  Compute the spectrum of a matrix $\mata\in\Cnn$ by
  \begin{enumerate}
  \item Use $n-2$ Householder transformations to transform $\mata$ to
    Hessenberg form
    \begin{gather}
     \matH = \matq\mata\matq^*.
   \end{gather}
 \item QR-iteration: let $\matH_{0}=\matH$ and perform until convergence
   \begin{align}
     \Omega^{(k)}_{1,2}\times\dots\times\Omega^{(k)}_{n-1,n} \matr &= \matH_k\\
     \matH_{k+1} &= \matr \Omega^{(k)}_{1,2}\times\dots\times\Omega^{(k)}_{n-1,n}.
   \end{align}
 \item Store Householder vectors as well as $r$ and $c$ for each
   Givens rotation if the eigenvectors are desired in the end.
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{hessenberg-qr-convergence}
    Let $\matH\in\Cnn$ be a Hessenberg matrix with eigenvalues such that
  \begin{gather}
    \abs{\lambda_1} >
    \abs{\lambda_2}>\dots>\abs{\lambda_n}.
  \end{gather}
  Then, the sequences of the QR-iteration admit the following estimates:
  \begin{align}
    \dist(\esp{1,\dots,j},\ran{\matq^{(k)}}) &= \bigo \left(\abs*{\frac{\lambda_{j+1}}{\lambda_j}}^k\right),
    \\
    h_{j+1,j}^{(k)} &= \bigo \left(\abs*{\frac{\lambda_{j+1}}{\lambda_j}}^k\right)
                      .
  \end{align}
  Here, $h_{ij}^{(k)}$ are the entries of $\matH_k$.
\end{Theorem}

\begin{proof}
  See~\cite[Theorem 7.3-1]{GolubVanLoan83}.
\end{proof}

\subsection{Shifts and deflation}

\begin{intro}
  The goal of this section is the development and justification of a
  method which accelerates convergence of the QR-iteration and
  reducing the effort at the same time. It is based on shifts, like
  for the simple or inverse power method. But, shifts are much more
  powerful here, since we compute not only ``converging subspace'',
  but also its complement. The presentation follows
  mostly~\cite{GolubVanLoan83}.
\end{intro}

\begin{Theorem}{qr-reduction}
  Let the matrix $\matH^{(k)}\in\Cnn$ in the QR iteration be of the
  form
  \begin{gather}
    \matH^{(k)} =
    \begin{pmatrix}
      \matH_{11} & \mata_{12}\\0 & \matH_{22}
    \end{pmatrix}
  \end{gather}
  with Hessenberg matrices $\matH_{11}\in\C^{p\times p}$,
  $\matH_{22}\in \C^{n-p\times n-p}$ and an arbitrary matrix
  $\mata_{12}\in \C^{p\times n-p}$. Then, the matrix $\matq^{(k)}$
  decouples into two diagonal blocks and $\matH^{(k+1)}$ has the same
  form. Thus, the iteration decouples into two separate iterations.
\end{Theorem}

\begin{Definition}{hessenberg-unreduced}
  A Hessenberg matrix is called \define{unreduced} if all entries on
  the first subdiagonal are nonzero. It is called \define{reduced}
  otherwise.
\end{Definition}

\begin{Algorithm*}{shifted-qr-iteration}{QR iteration with shift}
  
  Let $\matH_1 = \matq_0^*\mata\matq_0\in\Cnn$.\\
  For $k=1,\ldots$ until convergence repeat
  \begin{itemize}
  \item $\matq_k\matr_k = \matH_k - \sigma_k\id$ (QR factorization)
  \item $\matH_{k+1} = \matr_k\matq_k + \sigma_k\id$
  \end{itemize}
\end{Algorithm*}

\begin{Lemma}{shifted-qr-convergence}
  The shifted QR-iteration admits the estimate
  \begin{gather}
    h_{j+1,j}^{(k)} = \bigo \left(\abs*{\frac{\lambda_{j+1}-\sigma}{\lambda_j-\sigma}}^k\right)
  \end{gather}
\end{Lemma}

\begin{Example*}{rayleigh-shift}{Rayleigh shift}
  The Rayleigh quotient for the smallest eigenvalue by magnitude
  converges to $h_{nn}$, as
  \begin{gather}
    \ve_n^* H^{(k)} \ve_n = h_{nn}^{(k)}
  \end{gather}
  and $\vq_n$ is orthogonal to all eigenvectors for eigenvalues of
  greater magnitude. Therefore, using $\sigma_k = h_{nn}^{(k)}$ seems
  a good idea, and often is. But it is not reliable, as in the example
  \begin{gather}
    H =
    \begin{pmatrix}
      0 & 1 \\ 1 & 0
    \end{pmatrix}.
  \end{gather}
\end{Example*}

\begin{Definition*}{wilkinson-shift}{Wilkinson shift}
  Let
  \begin{gather}
    \matm =
    \begin{pmatrix}
      h_{n-1,n-1}^{(k)}&h_{n-1,n}^{(k)}\\h_{n,n-1}^{(k)}&h_{nn}^{(k)}
    \end{pmatrix}.
  \end{gather}
  Then, for $\sigma_k$ use the eigenvalue of $\matm$ which is closer
  to $h_{nn}^{(k)}$.
\end{Definition*}

\begin{Remark}{wilkinson-shift}
  The Wilkinson shift is reliable and the $h_{n,n-1}$ and $h_{nn}$
  converge to zero and the smallest eigenvalue by magnitude,
  respectively. They converge at least quadratically and cubically in
  the symmetric case~\cite[Section 8.2]{GolubVanLoan83}.
\end{Remark}

\begin{Algorithm*}{qr-deflation}{Deflation}
  After each step of the shifted QR-iteration monitor the subdiagonal
  elements of $\matH^{(k)}$. Whenever
  \begin{gather}
    \abs{h_{j,j-1}} \le \eps \bigl(\abs{h_{j-1,j-1}}+\abs{h_{jj}}\bigr)
  \end{gather}
  set $h_{j,j-1}=0$.

  If this happens in the last row, consider $h_{nn}=\lambda_n$
  converged and proceed with a matrix of dimension $n-1\times n-1$.

  If this happens in the center of the matrix, proceed with both
  remaining diagonal blocks separately.
\end{Algorithm*}


\subsection{Methods in real arithmetic}

\begin{Theorem*}{real-schur-form}{The real Schur form}
  For every matrix $\mata\in \Rnn$ there is an orthogonal matrix
  $\matq\in\Rnn$ and a matrix $\matr\in\Rnn$ such that
  \begin{gather}
    \mata = \matq\matr\matq^*,
    \qquad
    \matr =
    \begin{pmatrix}
      R_{11} &* & *&*\\
      &R_{22}&*&*\\
      &&\ddots&*\\
      &&& R_{jj}
    \end{pmatrix},
  \end{gather}
  where the diagonal blocks are either of dimension one containing the
  real eigenvalues or of dimension 2 for complex conjugate eigenvalue
  pairs. The latter correspond to scaled rotation matrices with the
  according eigenvalue pair.
\end{Theorem*}

\begin{Remark}{francis-qr}
  Using double shifts, the QR-iteration can be made to converge to the
  real Schur form using double shifts in real arithmetic. This method
  is also known as the \define{Francis QR step}~\cite[Algorithm
  7.5-1]{GolubVanLoan83}.
\end{Remark}

\begin{Remark*}{real-symmetric-qr}{QR-Iteration for real, symmetric matrices}
  In this case, many things simplify
  \begin{enumerate}
  \item Hessenberg form is tridiagonal
  \item The Schur normal form is
    \begin{gather}
      \mata = \matq^T\matd\matq
    \end{gather}
    with real, diagonal matrix $\matd$
  \item QR factorization uses $\bigo(n)$ operations and $\matr$
    consists only of the main diagonal and one upper diagonal.
  \end{enumerate}
  Accumulating the matrix $\matq$ still needs $\bigo(n^2)$ operations
\end{Remark*}

\subsection{Singular Value Decomposition (SVD)}

\begin{Definition}{svd}
  The \define{singular value decomposition} (\define{SVD}) of a matrix $\mata\in\C^{m\times n}$ is a facorization
  \begin{gather}
    \mata = \matu\matsigma\matv^*
  \end{gather}
  with unitary matrices $\matu\in\C^{m\times m}$ and $\matv\in\Cnn$ as
  well as a real, diagonal matrix $\matsigma$ with diagonal entries
  \begin{gather}
    \sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0,
  \end{gather}
  where $p = \min\{m,n\}$.
\end{Definition}

\begin{Theorem}{svd}
  Every matrix $\mata\in\C^{m\times n}$ admits a singular value
  decomposition. Every real matrix admits a singular value
  decomposition with orthogonal matrices $\matu$, $\matv$.
\end{Theorem}

\begin{Corollary}{svd-rank}
  Let $\mata = \matu\matsigma\matv^*$ be the SVD of $\mata$ with
  \begin{gather}
    \sigma_1 \ge \dots \ge \sigma_r > \sigma_{r+1} = \dots = \sigma_p = 0.
  \end{gather}
  Then, $\rank \mata = r$
\end{Corollary}

\begin{Corollary}{svd-inverse}
  If $\mata\in\Cnn$ is invertible, then
  \begin{gather}
    \mata^{-1} = \matv\matsigma^{-1}\matu^*,
  \end{gather}
  where
  \begin{gather}
    \matsigma^{-1} = \diag\left(\frac1{\sigma_1},\dots,\frac1{\sigma_n}\right).
  \end{gather}
\end{Corollary}

\begin{Remark}{svd-geometry}
  Let
  \begin{gather}
    E = \bigl\{ \vy\in \R^m \big| \vy=\mata\vx, \norm{\vx}_2 = 1 \bigr\},
  \end{gather}
  be the ellipsoid obtained by mapping the unit sphere though
  $\mata$. Then, the column vectors of $\matu$ and the singular values
  $\sigma_i$ are the directions and lengths of the semi-axes of this
  ellipsoid, respectively.
\end{Remark}

\begin{Lemma}{svd-ata}
  The singular values of $\mata$ are the square roots of the
  eigenvalues of $\mata^*\mata$ and of $\mata\mata^*$, respectively. For $m\ge n$ there holds
  \begin{align}
    \matv^*(\mata^*\mata)\matv &= \diag(\sigma_1^2,\dots,\sigma_n^2)
    &&\in \R^{n\times n}\\
    \matu^*(\mata\mata^*)\matu &= \diag(\sigma_1^2,\dots,\sigma_n^2, 0,\dots,0)
    &&\in \R^{m\times m}
  \end{align}
\end{Lemma}

\begin{Theorem*}{implicit-Q}{Implicit Q-theorem}
  Let $\mata\in\Cnn$ arbitrary, let $\matq,\matv\in\Cnn$ unitary such that
  \begin{gather}
    \matq^*\mata\matq = \matH,\qquad \matv^*\mata\matv = \matg,
  \end{gather}
  where $\matH$ and $\matg$ are Hessenberg matrices. Let $k$ denote
  the smallest integer such that $h_{k+1,k} = 0$, or $k=n$ if $\matH$
  is unreduced. Assume $\vv_1 = \vq_1$. Then,
  $\vv_j = e^{i_j\phi}\vq_j$ and $\abs{h_{j,j-1}} = \abs{g_{j,j-1}}$
  for $j=1,\dots,k$. if $k<n$, then $g_{k+1,k} = 0$.
\end{Theorem*}

\begin{proof}
  See \cite[Theorem 7.4-2]{GolubVanLoan83}.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
