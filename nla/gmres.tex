\begin{todo}
  Put this into an appendix
\end{todo}

\begin{Theorem}{normal-equations}
  Let $\mata\in\R^{m\times n}$ with $m\ge n$ and $\vb\in\R^m$. Then, a vector $\vx$ is the solution to the minimization problem
  \begin{gather}
    \norm{\vb-\mata\vx}_2 \stackrel{!}{=} \min,
  \end{gather}
  if an only if $\vx$ solves the \define{normal equations}
  \begin{gather}
    \mata^T\mata\vx = \mata^T \vb.
  \end{gather}
\end{Theorem}

\begin{Lemma}{oblique-normal}
  Let $\mata\in\Rnn$ be invertible. Let $K$ be a subspace of $\Rnn$. Then, the oblique projection
  \begin{gather}
    \vv\in K, \qquad \vb-\mata\vv \perp \mata K,
  \end{gather}
  and the orthogonal projection
  \begin{gather}
    \vw\in K, \qquad \mata^T \vb-\mata^T\mata\vw \perp K,
  \end{gather}
  are identical.
\end{Lemma}

\begin{proof}
  Homework.
\end{proof}

\begin{todo}
  Write in incremental form? Add pseudocode version in incremental form?
\end{todo}

\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy_m}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matv_{m}\vy_m
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{Theorem}{gmres-projection}
  The GMRES method computes the update of the $m$-dimensional oblique
  projection step. It minimizes the residual over the search space
  $\vx^{(0)} +\krylov_m(\mata,\vr^{(0)}$ and there holds
  \begin{gather}
    \label{eq:gmres-norm}
    \norm{\vr^{(m)}}_2 = \norm{\vb-\mata\vx^{(m)}}_2 =
    \norm{\beta\ve_1 - \overline\matH \vy_m}_2.
  \end{gather}
\end{Theorem}

\begin{proof}
  We use the equivalence between minimizing the residual and the
  solution to the normal equations, namely
  \slideref{Theorem}{normal-equations}. According to
  \slideref{Theorem}{arnoldi-projection}, we obtain
  \begin{gather}
    \matv_m^T\mata^T\mata\matv_m
    = \overline\matH_m^T \matv_{m+1}^T\matv_{m+1}\overline\matH_m
    = \overline\matH_m^T\overline\matH_m.
  \end{gather}
  Thus, $\overline\matH_m^T\overline\matH_m$ is the Galerkin
  projection of $\mata^T\mata$ to the subspace generated by
  $\matv_m$. Furthermore, it is symmetric, positive definite, the
  latter assuming $\mata$ is invertible.  Furthermore,
  $\vr^{(0)} = \vb-\mata\vx^{(0)} = \beta\matv_{m+1}\ve_1$, such that
  \begin{gather}
    \matv_m^T\mata^T\vr^{(0)} = \beta\overline\matH_m \ve_1.
  \end{gather}
  Thus, the linear system
  \begin{gather}
    \overline\matH_m^T\overline\matH_m \vy_m = \beta\overline\matH_m \ve_1
  \end{gather}
  is the (orthogonal) Galerkin projection of the normal equations to
  the search space. On the other hand, its solution solves the
  minimization problem in the GMRES algorithm.

  By the minimization property of the orthogonal projection, the error
  after the projection step is minimal in the $\mata^T\mata$ norm,
  which is equal to the Euclidean norm of the residual.

  Finally, we have
  \begin{align}
    \vr^{(m)}
    &= \vb-\mata\vx^{(0)} - \mata\matv_m\vy_m\\
    &=\beta\matv_{m+1}\ve_1 - \matv_{m+1}\overline\matH_m\vy_m\\
    &= \matv_{m+1}\left(\beta\ve_1 - \overline\matH_m\vy_m\right).
  \end{align}
  Since $\matv_{m+1}$ is orthogonal, this implies that
  \begin{gather}
    \norm{\vr^{(m)}}_2 = \norm{\beta\ve_1 - \overline\matH_m\vy_m}_2
  \end{gather}
\end{proof}

\begin{remark}
  \slideref{Algorithm}{gmres} should not be implemented as it is
  presented, but rather in the fashion of an iterative method, where
  $m$ is increased step by step. To this end, we observe that the
  generation of the Arnoldi basis proceeds incrementally, adding new
  vectors, but not changing the previous ones.

  Therefore, $\overline\matH_{m+1}$ differs from $\overline\matH_{m}$
  only by adding an additional row and column. Then, the least-squares
  system is solved by computing the QR factorization of
  $\overline\matH_{m+1}$, which again amounts to updating the last
  column only.
\end{remark}

\begin{remark}
  Equation~\eqref{eq:gmres-norm} implies in particular, that the norm
  of the residual in the GMRES method can be computed without
  computing a ``big'' matrix-vector operation $\mata\vx^{(m)}$, but is
  indeed the misfit of the $m$-dimensional least-squares problem for
  $\vy_m$. This implies on the other hand, that even the solution
  $\vx^{(m)}$ should only be computed as soon as the resiudal is
  sufficiently small.
\end{remark}

\begin{Lemma}{gmres-breakdown}
  If the GMRES method breaks down at step $m$, then
  $\vr^{(m)}=0$. Thus, the GMRES method only encounters a
  \putindex{lucky breakdown}.
\end{Lemma}

\begin{remark}
  The GMRES method encounters a problem if convergence is slow: the
  effort for computing the last basis vector in $\matv_m$ is of order
  $mn$, the effort for solving the least-squares problem is
  $m^3$. Similarly, the storage requirement for $\matv_m$ is $mn$ and
  for $\overline \matH_m$, it is $m^2$. Thus, the algorithm is
  feasible for large sparse matrices only if $m\ll n$.

  Therefore, we introduce two variants of the GMRES algorithm:
  \begin{itemize}
  \item Restarted GMRES simply deletes the whole basis $\matv_m$ and the matrix $\overline\matH_m$ every $k$-th step starts fresh.
  \item Truncated GMRES orthogonalizes only with respect to the last
    $k$ basis vectors.
  \end{itemize}
\end{remark}

\begin{Algorithm*}{gmres-restart}{Restarted GMRES}
  \begin{enumerate}
  \item Choose a maximal dimension $m$ of the Krylov space
  \item Begin with an initial guess $\vx^{(0)}$.
  \item For $k>0$, given $\vx^{(k)}$, perform the GMRES method with
    basis size $m$ to obtain $\vx^{(k+m)}$.
  \item Check the stopping criterion as usual inside the GMRES method
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{gmres-truncated}{Truncated GMRES}
  Run the GMRES method as usual, but in the Arnoldi process, only
  orthogonalize with respect to the last $k$ vectors for fixed $k$.
\end{Algorithm*}

\begin{remark}
  Note that both modifications destroy the minimization property of the method.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
