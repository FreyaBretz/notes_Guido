\begin{intro}
  The facts in this section can be found in~\cite[Chapter
  5]{GolubVanLoan83}. In particular, the QR factorization in section
  5.2, Housholder transformations and Givens rotations in 5.1 there.
\end{intro}

\subsection{Definition and existence}
\begin{Definition}{qr-decomposition}
  The \define{QR factorization} of a matrix $\mata\in\C^{m\times n}$
  with $m\ge n$ is the product
  \begin{gather}
    \mata = \matq\matr,
  \end{gather}
  such that $\matq \in\C^{m\times n}$ is unitary and
  $\matr\in \C^{n\times n}$ is upper triangular.
\end{Definition}

\begin{Lemma}{qr-columns}
  Let $\mata = \matq\matr$. Then, the column vectors of $\mata$ and
  $\matq$ admit the relation
  \begin{gather}
    \va_k = \sum_{i=1}^k r_{ik} \vq_i.
  \end{gather}
  If $r_{ii}\neq 0$ for $i=1,\dots,k$, this relation is uniquely
  invertible. In particular,
  \begin{gather}
    \spann{\vq_1,\dots,\vq_k}
    = 
    \spann{\va_1,\dots,\va_k}.
  \end{gather}
\end{Lemma}

\begin{Algorithm*}{gram-schmidt}{Gram-Schmidt Orthogonalization}
  Let $\vx_1, \dots,\vx_k\in\C^n$ with $k\le n$ be linearly independent.

  For $j=1,\dots,k$
  \begin{align*}
    \vw_j &= \vx_j - \sum_{i=1}^{j-1} \scal(vx_j,\vv_i)\vv_i\\
    \vv_j &= \frac{\vw_j}{\norm{\vw_j}_2}.
  \end{align*}
\end{Algorithm*}

\begin{Theorem}{gram-schmidt}
  If the input vectors $\vx_1, \dots,\vx_k\in\C^n$ are linearly
  independent, then the Gram-Schmidt orthogonalization generates an
  orthonormal set $\vv_1, \dots,\vv_k\in\C^n$. Furthermore, for
  $j=1,\dots,k$ there holds
  \begin{gather}
    \spann{\vv_1, \dots,\vv_j} = \spann{\vx_1, \dots,\vx_j}.
  \end{gather}
\end{Theorem}

\begin{Theorem}{qr-existence}
  Every matrix $\mata\in\C^{m\times n}$ with $m\ge n$ of full rank
  admits a QR factorization. It is unique under the condition that for
  all $i$ there holds $r_{ii} > 0$.
\end{Theorem}

\subsection{Householder transformations}

\begin{Definition}{householder-transformation}
  The \define{Householder transformation}
  associated with a vector $\vw\in\C^n$ is
  \begin{gather}
    \matq_w = \id - 2\frac{\vw\vw^*}{\vw^*\vw}
  \end{gather}
  It is also called \define{Householder matrix} or, particularly in
  the real case, \define{Householder reflection}.
\end{Definition}

\begin{Lemma}{householder-symmetry}
  For any vector $\vw\in\C^n$ the Householder transformation
  $\matq_{\vw}$ is Hermitian and orthogonal, that is,
  \begin{gather}
    \matq^{-1} = \matq^* = \matq.
  \end{gather}
\end{Lemma}

\begin{Lemma}{householder-qr}
  For any vector $\vy\in\C^n$ there are vectors $\vw_\phi\in\C^n$ such
  that $\matq_{\vw_\phi} \vy$ is a multiple of $\ve_1$.

  The vector of choice for numerical stability is
  \begin{gather}
    \vw = \vy + e^{i\phi} \norm{\vy}_2\ve_1,
  \end{gather}
  where $\phi$ is the phase of $y_1$.
\end{Lemma}

\begin{proof}
  The statement of the lemma says that for a suitable vector $\vw$ there is a complex number
  $\alpha$ such that $\matq_{\vw} \vy = \alpha \ve_1$. Since
  $\matq_{\vw}$ preserves the Euclidean norm, we already know
  \begin{gather}
    \abs{\alpha} = \norm{\vy}_2.
  \end{gather}
  There holds
  \begin{gather}
    \alpha\ve_1 = \matq_{\vw} \vy
    = \vy - 2 \frac{\vw\vw^*\vy}{\vw^*\vw}
    = \vy - w \frac{\vw^*\vy}{\vw^*\vw}\vw.
  \end{gather}
  Thus, $\vw$ is in the span of $\vy-\alpha \ve_1$. Since we divide by
  its norm, its length does not matter and we let
  \begin{gather}
    \vw_\phi =
    \begin{pmatrix}
      y_1 - e^{i\phi} \norm{\vy}_2\\y_2\\\vdots\\y_n
    \end{pmatrix},
    \qquad
    \matq_{\vw_\phi}\vy =
    \begin{pmatrix}
      e^{i\phi} \norm{\vy}_2\\0\\\vdots\\0
    \end{pmatrix}.
  \end{gather}
  Since the computation of the first component of $\vw_\phi$ is prone to loss of significance, we choose $\phi$ as the phase of $-y_1$.
\end{proof}

\subsection{Givens rotation}

\begin{Definition}{givens}
  The real \textbf{Givens-Rotation}\index{Givens rotation!real} $\givens_{jk}$ for $j<k$ with angle $\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Rnn.
  \end{gather}
    where $c = \cos\theta$ and $s = \sin\theta$.
  The corresponding mapping $\givens_{jk}\colon x\mapsto y$ is defined by
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{else}
    \end{cases}.
  \end{gather}
\end{Definition}

\begin{remark}
  The entries $c$ and $s$ of the rotation matrix are in rows and
  columns $j$ and $k$.  The identity matrices $\id$ are of
  corresponding dimensions.

  Applying $\givens_{jk}$ to a matrix $\mata$ from the left modifies rows  $j$ and $k$ of $\mata$. Thus, it is a row operation similar to Gauss elimination.

  The action of $\givens_{jk}$ on a vector corresponds to the rotation
  in the plane spanned by $\ve_j$ und $\ve_k$. Thus, it is sufficient to investigated $2\times2$-matrices.

  Note that the notation $\givens_{jk}$ misses the angle $\theta$. This
  is due to the fact that it is always determined such that $y_k=0$.
\end{remark}

\begin{Lemma}{givens-computation}
  Givens rotation $\givens_{jk}^T$ eliminates the second component of the vector
  $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s = \frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}^T
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}

\begin{remark}
  Computation of $r$ in the previous lemma is prone to numerical
  overflow due to computation of $x_j^2$ or $x_k^2$, even if $r$
  itself is within the numerical range. For the implementation, we can
  use the function \lstinline!hypot!. It computes the hypothenuse of a
  right-angled triangle without overflow.
\end{remark}

\begin{Definition}{givens-complex}
  The complex \textbf{Givens-Rotation}\index{Givens rotation!complex} $\givens_{jk}$ for $j<k$ with angles $\varphi,\theta$ is the matrix
  \begin{gather}
      \givens_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-\overline s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Cnn.
  \end{gather}
    where $c = \cos\theta$ and $s = e^{i\varphi}\sin\theta$.
\end{Definition}

\begin{Lemma}{givens-computation-complex}
  Let $u,v\in \C$ such that $u=u_1+iu_2$ and $v=v_1+iv_2$. Then, the second component of $(u,v)^T$ can be eliminated, such that
  \begin{gather}
    \begin{pmatrix}
      c & s\\-\overline s&c
    \end{pmatrix}^*
    \begin{pmatrix}
      u\\v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
  \end{gather}
  by three real Givens rotations
  \begin{gather}
    \begin{pmatrix}
      c_\alpha & s_\alpha\\-s_\alpha & c_\alpha
    \end{pmatrix}^T
    \begin{pmatrix}
      u_1\\u_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_u\\0
    \end{pmatrix}
    ,\quad
    \begin{pmatrix}
      c_\beta & s_\beta\\-s_\beta & c_\beta
    \end{pmatrix}^T
    \begin{pmatrix}
      v_1\\v_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      r_v\\0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      c_\theta & s_\theta \\-s_\theta  & c_\theta
    \end{pmatrix}^T
    \begin{pmatrix}
      r_u\\r_v
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    ,
  \end{gather}
  choosing $\varphi = \beta-\alpha$ and $c=c_\theta$,
  $s=s_\theta e^{i\phi}$.
\end{Lemma}

\begin{proof}
  Note that the first two rotations map a complex number to a real number, hence
  \begin{gather}
    u = r_u e^{-i\alpha}, \qquad v = r_v e^{-i\beta}.
  \end{gather}
  Let now $\varphi = \beta-\alpha$, then,
  \begin{gather}
    \begin{aligned}
      c u - s v
      &= c_\theta e^{-i\alpha} r_u - s_\theta e^{i\phi} e^{-i\beta} r_v
      &&= e^{-i\alpha}\bigl(c_\theta r_u - s_\theta r_v\bigr) = r,
      \\
      c v + \overline s u
      &= c_\theta e^{-i\beta} r_v + s_\theta e^{-i\phi} e^{-i\alpha} r_u
      &&= e^{-i\beta}\bigl(c_\theta r_v + s_\theta r_u\bigr) = 0.      
    \end{aligned}
  \end{gather}
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
