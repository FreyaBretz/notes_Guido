
\subsection{Definition and existence}
\begin{Definition}{qr-decomposition}
  The \define{QR decomposition} of a matrix $\mata\in\C^{m\times n}$
  with $m\ge n$ is the product
  \begin{gather}
    \mata = \matq\matr,
  \end{gather}
  such that $\matq \in\C^{m\times n}$ is unitary and
  $\matr\in \C^{n\times n}$ is upper triangular.
\end{Definition}

\begin{Lemma}{qr-columns}
  Let $\mata = \matq\matr$. Then, the column vectors of $\mata$ and
  $\matq$ admit the relation
  \begin{gather}
    \va_k = \sum_{i=1}^k r_{ik} \vq_i.
  \end{gather}
  If $r_{ii}\neq 0$ for $i=1,\dots,k$, this relation is uniquely
  invertible. In particular,
  \begin{gather}
    \operatorname{span}\{\vq_1,\dots,\vq_k\}
    = 
    \operatorname{span}\{\va_1,\dots,\va_k\}.
  \end{gather}
\end{Lemma}

\begin{Theorem}{qr-existence}
  Every matrix $\mata\in\C^{m\times n}$ with $m\ge n$ of full rank
  admits a QR decomposition. It is unique under the condition that for
  all $i$ there holds $r_{ii} > 0$.
\end{Theorem}

\subsection{Householder transformations}

\begin{Definition}{householder-transformation}
  The \define{Householder transformation}
  associated with a vector $\vw\in\C^n$ is
  \begin{gather}
    \matq_w = \id - 2\frac{\vw\vw^*}{\vw^*\vw}
  \end{gather}
  It is also called \define{Householder matrix} or, particularly in
  the real case, \define{Householder reflection}.
\end{Definition}

\begin{Lemma}{householder-symmetry}
  For any vector $\vw\in\C^n$ the Householder transformation
  $\matq_{\vw}$ is Hermitian and orthogonal, that is,
  \begin{gather}
    \matq^{-1} = \matq^* = \matq.
  \end{gather}
\end{Lemma}

\begin{Lemma}{householder-qr}
  For any vector $\vy\in\C^n$ there are vectors $\vw_\phi\in\C^n$ such
  that $\matq_{\vw_\phi} \vy$ is a multiple of $\ve_1$.

  The vector of choice for numerical stability is
  \begin{gather}
    \vw = \vy + e^{i\phi} \norm{\vy}_2\ve_1,
  \end{gather}
  where $\phi$ is the phase of $y_1$.
\end{Lemma}

\begin{proof}
  The statement of the lemma says that for a suitable vector $\vw$ there is a complex number
  $\alpha$ such that $\matq_{\vw} \vy = \alpha \ve_1$. Since
  $\matq_{\vw}$ preserves the Euclidean norm, we already know
  \begin{gather}
    \abs{\alpha} = \norm{\vy}_2.
  \end{gather}
  There holds
  \begin{gather}
    \alpha\ve_1 = \matq_{\vw} \vy
    = \vy - 2 \frac{\vw\vw^*\vy}{\vw^*\vw}
    = \vy - w \frac{\vw^*\vy}{\vw^*\vw}\vw.
  \end{gather}
  Thus, $\vw$ is in the span of $\vy-\alpha \ve_1$. Since we divide by
  its norm, its length does not matter and we let
  \begin{gather}
    \vw_\phi =
    \begin{pmatrix}
      y_1 - e^{i\phi} \norm{\vy}_2\\y_2\\\vdots\\y_n
    \end{pmatrix},
    \qquad
    \matq_{\vw_\phi}\vy =
    \begin{pmatrix}
      e^{i\phi} \norm{\vy}_2\\0\\\vdots\\0
    \end{pmatrix}.
  \end{gather}
  Since the computation of the first component of $\vw_\phi$ is prone to loss of significance, we choose $\phi$ as the phase of $-y_1$.
\end{proof}

\subsection{Givens rotation}

\begin{Definition}{givens}
  The \define{Givens-Rotation} $\Omega_{jk}$ for $j<k$ with angle $\theta$ is the matrix
  \begin{gather}
      \Omega_{jk} =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    \in\Cnn.
  \end{gather}
  The corresponding mapping $\Omega_{jk}\colon x\mapsto y$ is defined by
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{else}
    \end{cases}
  \end{gather}
  where $c = \cos\theta$ and $s = \sin\theta$.
\end{Definition}

\begin{remark}
  The entries $c$ and $s$ of the rotation matrix are in rows and
  columns $j$ and $k$.  The identity matrices $\id$ are of
  corresponding dimensions.

  Applying $\Omega_{jk}$ to a matrix $\mata$ from the left modifies rows  $j$ and $k$ of $\mata$. Thus, it is a row operation similar to Gauss elimination.

  The action of $\Omega_{jk}$ on a vector corresponds to the rotation
  in the plane spanned by $\ve_j$ und $\ve_k$. Thus, it is sufficient to investigated $2\times2$-matrices.

  Note that the notation $\Omega_{jk}$ misses the angle $\theta$. This
  is due to the fact that it is always determined such that $y_k=0$.
\end{remark}

\begin{Lemma}{givens-computation}
  Givens rotation $\Omega_{jk}^*$ eliminates the second component of the vector
  $(x_j,x_k)^T$ by choosing
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s = \frac{x_k}r.
  \end{gather}
  We obtain
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}

\begin{remark}
  Computation of $r$ in the previous lemma is prone to numerical
  overflow due to computation of $x_j^2$ or $x_k^2$, even if $r$
  itself is within the numerical range. For the implementation, we can
  use the function \lstinline!hypot!. It computes the hypothenuse of a
  right-angled triangle without overflow.
\end{remark}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
