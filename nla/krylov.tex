\subsection{Projection methods}

\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb\vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:krylov:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  For the case $K=L$ we deduce from the optimization property of
  orthogonal projections that $\mata\tilde x$ in~\eqref{eq:krylov:1}
  is the vector in the subspace $\mata K$ closest to $\vb$. Thus,
  $\tilde\vx$ minimizes the \putindex{residual} $\vb-\mata\vy$ over
  all choices $\vy\in K$.

 Note that this does not hold for $K\neq L$.
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+1)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)} + \vy,
  \end{gather}
  where after the choice of subspaces $K$ and $L$ the update $\vy$ is
  determined by the condition
  \begin{align}
    \vy&\in K\\
    \vr^{(k)} - \mata\vy &\perp L.
  \end{align}
\end{Definition}

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel substep is a projection step with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
\end{Example}

\begin{notation}
  We will mostly only consider a single step of the method in
  \slideref{Definition}{projection-step}. Therefore, we will consider
  the step from an initial guess $\vx^{(0)}$ to an approximate solution
  $\tilde \vx$ to simplify the notation.
\end{notation}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\tilde \vx$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \tilde\vx &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matv^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
