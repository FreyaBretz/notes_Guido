\subsection{Projection methods}

\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb\vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:krylov:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  For the case $K=L$ we deduce from the optimization property of
  orthogonal projections that $\mata\tilde x$ in~\eqref{eq:krylov:1}
  is the vector in the subspace $\mata K$ closest to $\vb$. Thus,
  $\tilde\vx$ minimizes the \putindex{residual} $\vb-\mata\vy$ over
  all choices $\vy\in K$.

 Note that this does not hold for $K\neq L$.
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+1)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)} + \vy,
  \end{gather}
  where after the choice of subspaces $K$ and $L$ the update $\vy$ is
  determined by the condition
  \begin{align}
    \vy&\in K\\
    \vr^{(k)} - \mata\vy &\perp L.
  \end{align}
\end{Definition}

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel substep is a projection step with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
\end{Example}

\begin{notation}
  We will mostly only consider a single step of the method in
  \slideref{Definition}{projection-step}. Therefore, we will consider
  the step from an initial guess $\vx^{(0)}$ to an approximate solution
  $\tilde \vx$ to simplify the notation.
\end{notation}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\tilde \vx$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \tilde\vx &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matv^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}

\begin{Theorem}{projection-orthogonal-optimal}
  Let $\mata\in\Rnn$ be symmetric, positive definite. Then,
  $\tilde \vx$ is the result of the orthogonal ($L=K$) projection
  method with initial vector $\vx^{(0)}$ if and only if it minimizes
  the \putindex{A-norm} of the error over the space $\vx^{(0)}+K$. Namely, for the
  solution $\vx$ of $\mata\vx=\vb$ there holds
  \begin{gather}
    \norm{\tilde\vx-\vx}_A = \min_{\vy\in\vx^{(0)}+K} \norm{\vy-\vx}_A.
  \end{gather}
  Here, $\norm\vy_A = \sqrt{\vy^*\mata\vy}$.
\end{Theorem}

\begin{Theorem}{projection-oblique-optimal}
  Let $\mata\in\Rnn$ and $L=\mata K$. Then, $\tilde \vx$ is the result
  of the (oblique) projection method with initial vector $\vx^{(0)}$
  if and only if it minimizes the Euclidean norm of the residual
  $\vb-\mata\tilde\vx$ over the space $\vx^{(0)}+K$. Namely, there
  holds
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}_2
    = \min_{\vy\in\vx^{(0)}+K} \norm{\vb-\mata\vy}_2
  \end{gather}
\end{Theorem}

\begin{Example}{projection-1d}
  A one-dimensional projection method can be characterized by two
  vectors $\vv$ and $\vw$. Then,
  \begin{gather}
    K = \spann\vv,
    \qquad L = \spann\vw.
  \end{gather}
  The new solution is
  \begin{gather}
    \tilde\vx = \vx^{(0)}+\alpha \vv,
  \end{gather}
  where
  \begin{gather}
    \alpha = \frac{\scal(\vr^{(0)},\vw)}{\scal(\mata\vv,\vw)}
  \end{gather}
  is determined from the Galerkin condition $\vr^{(0)}-\mata\vy \perp \vw$
\end{Example}

\begin{Algorithm*}{steepest-descent-algol}{The steepest descent method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ s.p.d.; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\vr,\vr)}{\scal(\mata\vr,\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{steepest-descent}
  Each step of the steepest descent method computes the minimum of
  $F(\vy) = \norm{\vy-\vx}_A^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $-\nabla F(\vx^{(k)})$.
\end{Lemma}

\begin{remark}
  In this algorithm, the operation $\mata\vr$ is applied twice. Since
  in most implementations this will be the part with the most
  computational operations, we introduce an auxiliary vector
  $\vp = \mata\vr$, which can be used in lines 3 and 5. At the cost of
  one additional vector, the numerical effort per step is almost
  cut in half.
\end{remark}

\begin{Remark}{convergence-residual}
  The residual $\vr = \vb - \mata\tilde\vx$ of the current iterate
  $\tilde\vx$ measures the misfit of $\tilde\vx$ in the equation
  $\mata\vx = \vb$. Since the error $\tilde\vx-\vx$ is unknown, we can
  use it as criterion for the accuracy of the current
  solution. Indeed, there holds
  \begin{gather}
    \norm{\vx-\tilde\vx}
    = \norm*{\mata^{-1}\bigl(\mata\vx - \mata\tilde\vx\bigr)}
    = \norm*{\mata^{-1}\bigl(\vb - \mata\tilde\vx\bigr)}
    \le \norm{\mata^{-1}}\norm{\vr}.
  \end{gather}
  Therefore, the criterion for convergence of the algorithm is typically
  \begin{gather}
    \norm{\vr} \le \text{TOL},
  \end{gather}
  where TOL is a tolerance chosen by the user.

  Note, that the computation of $\vr$ is subject to roundoff
  errors. Thus, the tolerance should always be chosen
  \begin{gather}
    \text{TOL} > c\norm{\vb}\eps,
  \end{gather}
  where $\eps$ is the machine accuracy and $c$ should account for
  error accumulation in the matrix-vector product.
\end{Remark}

\begin{Algorithm*}{steepest-descent-python1}{Steepest descent in Python}
  \lstinputlisting{python/steepest-descent.py}
\end{Algorithm*}

\begin{remark}
  Depending on the quality of the compiler/interpreter, the code line
  \begin{lstlisting}[language=Python,numbers=none]
    x += alpha*r
  \end{lstlisting}
  may involve creating an auxiliary vector $\vp = \alpha \vr$ and
  adding this vector to $\vx$.  Obviously, this could be avoided by
  directly implementing
  \begin{lstlisting}[language=Python,numbers=none]
    for i in range(0,n):
      x[i] += alpha*r[i]
  \end{lstlisting}
  Since operations like this are ubuquitous in scientific computing,
  they were standardized early on in the BLAS (basic linear algebra
  subroutines) library. It contains the FORTRAN function
  \begin{lstlisting}[language=Fortran,numbers=none]
    SUBROUTINE DAXPY( n, alpha, x, incx, y, incy)
  \end{lstlisting}
  which computes $\vy\gets\vy+\alpha\vx$ for double precision vectors
  of length $n$ (the increment arguments allow to skip elements). For
  usage in Python, there is a wrapper
  \begin{lstlisting}[language=Python,numbers=none]
    scipy.linalg.blas.daxpy(x, y[, n, a, offx, incx, offy, incy])
  \end{lstlisting}
\end{remark}

\begin{Algorithm*}{steepest-descent-python2}{Steepest descent with daxpy}
  \lstinputlisting{python/steepest-descent-axpy.py}
\end{Algorithm*}

\begin{Algorithm*}{minimal-residual-algol}{The minimal residual method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\mata\vr,\vr)}{\scal(\mata\vr,\mata\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{minimal-residual}
  Each step of the minimal method computes the minimum of
  $F(\vy) = \norm{\vb-\mata\vy}_2^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $\vr$.
\end{Lemma}

\begin{Lemma}{lucky-breakdown}
  The iteration sequences $\{\vx^{(k)}\}$ of the steepest descent and
minimal residual methods, respectively, are well defined except for
the case where $\vx^{(k)}$ is the exact solution $\vx$ and thus
$\vr^{(k)} = 0$.

We refer to this phenomenon as \define{lucky breakdown}, since the
method only fails after the exact solution has been found.
\end{Lemma}

\begin{Lemma*}{kantorovich-inequality}{Kantorovich inequality}
  Let $\mata\in\Rnn$ be symmetric, positive definite with minimal and
  maximal eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, for
  $\vx\in\R^n$ there holds
  \begin{gather}
    \frac{\scal(\mata\vx,\vx)\scal(\mata^{-1}\vx,\vx)}{\scal(\vx,\vx)^2}
    \le \frac{(\lambda_{\min}+\lambda_{\max})^2}{4\lambda_{\min}\lambda_{\max}}
  \end{gather}
\end{Lemma*}

\begin{Theorem}{steepest-descent-convergence}
  Let $\mata\in\R^nn$ be symmetric, positive definite with extremal
  eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, the error
  $\ve^{(k)} = \vx-\vx^{(k)}$ of the steepest descent method admits
  the estimate
  \begin{gather}
    \norm{\ve^{(k+1)}}_A \le \rho \norm{\ve^{(k)}}_A,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho
    = \frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
    = \frac{\cond_2\mata - 1}{\cond_2\mata+1}
    = 1-\frac2{\cond_2\mata+1}
  \end{gather}
\end{Theorem}

\begin{Theorem}{minimal-residual-convergence}
  Let $\mata\in\Rnn$ such that its symmetric part $(\mata+\mata^T)/2$
  is positive definite. Then, the residuals $\vr^{(k)}$ of the minimal
  residual method admit the estimate
  \begin{gather}
    \norm{\vr^{(k+1)}}_2 \le \rho \norm{\vr^{(k)}}_2,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho = \left(1-\frac{\mu^2}{\norm{\mata}^2}\right)^{\nicefrac12}
  \end{gather}
  and $\mu$ is the smallest eigenvalue of $(\mata+\mata^T)/2$.
\end{Theorem}

\begin{remark}
  Let $e_k$ be some measure of the error of a fixed-point iteration
  with contraction number $\rho$ after $k$ steps. Then, there holds
  \begin{gather}
    e_k \le \rho^k e_0.
  \end{gather}
  Thus, the number of steps needed to obtain a (relative) reduction of
  the error by a prescribed relative tolerance $\epsilon$, that is, to
  achieve $e_k/e_0\le\epsilon$, is
  \begin{gather}
    \label{eq:krylov:steps-convergence}
    k \ge \frac{\log\epsilon}{\log\rho}.
  \end{gather}
  From this formula, we realize that the number of steps of such a
  method grows linearly with the logarithm of the tolerance. Further,
  it is inverse proportional to the logarithm of the contraction
  number $\rho$.

  Note that logarithms with respect to any base can be used
  in~\eqref{eq:krylov:steps-convergence}, since the quotient is
  independent of the base.
\end{remark}

\begin{Definition}{convergence-rate-logarithmic}
  The (logarithmic) \define{convergence rate} of a contraction with
  \putindex{contraction number} $\rho$ is
  \begin{gather}
    r_c = -\log_{10} \rho.
  \end{gather}

  The average \define{observed convergence rate} of $k$ steps of a
  method with error measure $e_k$ is
  \begin{gather}
    \overline{r_c} = \frac1k \log_{10}\frac{e_0}{e_k}.
  \end{gather}
\end{Definition}

\begin{remark}
  The logarithmic convergence rate allows us to directly compare two
  iterative methods. Let's say, there are two iterations $M_1$ and
  $M_2$ and the convergence rate of $M_2$ ist twice the rate of
  $M_1$. Then, given an initial vector, $M_1$ will need twice as many
  steps compared to $M_2$ to reach the same accuracy.

  This implies that $M_2$ is favorable, if the effort for each step is
  less than twice the effort for $M_1$. If it is more than twice, then
  $M_1$ is the faster method in spite of the slower convergence.
\end{remark}

\subsection{Krylov spaces}

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-invariant}
\end{Lemma}
\begin{Lemma}{krylov-polynomial}
  The Krylov space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$
  if and only if the grade of $\vv$ with respect to $\mata$ is at
  least $m$. In particular, for any $\vw\in\krylov_m(\mata,\vv)$ there
  is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \vw = p(\mata)\vv.
  \end{gather}

  It is invariant under the action of $\mata$ if and only if the grade
  of $\vv$ does not exceed $m$.
\end{Lemma}

\begin{proof}
  See \cite[Propositions 6.1 \& 6.2]{Saad00}.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matq_m$ be a projector onto $\krylov_m$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matq_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \matq_m p(\mata)\vv = p(\mata_m)\vv
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.
\end{proof}

\subsection{The Arnoldi procedure}

\begin{Algorithm*}{arnoldi-1}{Arnoldi I}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vv_j,\vv_i)$
    \EndFor
    \State $\vw_j \gets \mata \vv_j-\sum_{i=1}^j h_{ij}\vv_i$
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-krylov}
  Assume that the Arnoldi algorithm does not stop before step
  $m$. Then, the vectors $\vv_1,\dots,\vv_m$ form an orthonormal basis
  of $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.4]{Saad00}.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matv_m = (\vv_1,\dots,\vv_m)$ be the matrix of basis vectors
  generated by the Arnoldi method and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ be the Hessenberg matrix of
  entries $h_{ij}$ computed in the algorithm. Let further
  $\mathH_m\in\R^{m\times m}$ be the same matrix without the last
  row. Then, there holds
  \begin{align}
    \mata\matv_m &= \matv_m\matH_m+\vw_m\ve_m^T\\
                 &= \matv_{m+1}\overline{\matH}_m\\
  \end{align}
  and
  \begin{gather}
    \matH_m = \matv_m^T\mata\matv.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.5]{Saad00}.
\end{proof}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
