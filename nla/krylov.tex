\subsection{Projection methods}

\begin{Definition}{galerkin-method}
  Let $\mata\in\Rnn$ and $\vb\vx\in\R^n$ with $\mata\vx=\vb$. Then,
  the vector $\tilde\vx\in\R^n$ is called the \define{Galerkin
    approximation} of $\vx$ in a subspace $K$ orthogonal to a subspace
  $L$, if there holds
  \begin{align}
    \tilde\vx &\in K,\\
    \vb-\mata\tilde\vx &\perp L.
                         \label{eq:krylov:1}
  \end{align}
  This type of approximation is called \define{Galerkin method}, more
  specifically \define{Ritz-Galerkin method} in the case $K=L$ and
  \define{Petrov-Galerkin method} in the case $K\neq L$.
\end{Definition}

\begin{remark}
  For the case $K=L$ we deduce from the optimization property of
  orthogonal projections that $\mata\tilde x$ in~\eqref{eq:krylov:1}
  is the vector in the subspace $\mata K$ closest to $\vb$. Thus,
  $\tilde\vx$ minimizes the \putindex{residual} $\vb-\mata\vy$ over
  all choices $\vy\in K$.

 Note that this does not hold for $K\neq L$.
\end{remark}

\begin{Definition}{projection-step}
  Given a vector $\vx^{(k)}\in\R^n$ and its residual
  $\vb-\mata\vx^{(k)}$. Then, we say that the vector
  $\vx^{(k+1)}\in\R^n$ is obtained by a \define{projection step}, if
  \begin{gather}
    \vx^{(k+1)} = \vx^{(k)} + \vy,
  \end{gather}
  where after the choice of subspaces $K$ and $L$ the update $\vy$ is
  determined by the condition
  \begin{align}
    \vy&\in K\\
    \vr^{(k)} - \mata\vy &\perp L.
  \end{align}
\end{Definition}

\begin{Example}{projection-gauss-seidel}
  The Gauss-Seidel substep is a projection step with the choice
  \begin{gather}
    K=L=\spann{\ve_i}.
  \end{gather}
\end{Example}

\begin{notation}
  We will mostly only consider a single step of the method in
  \slideref{Definition}{projection-step}. Therefore, we will consider
  the step from an initial guess $\vx^{(0)}$ to an approximate solution
  $\tilde \vx$ to simplify the notation.
\end{notation}

\begin{Definition}{projection-method-matrix}
  Let $\matv=(\vv_1,\dots,\vv_m)$ and $\matw=(\vw_1,\dots,\vw_m)$ be bases for
  the subspaces $K$ and $L$, respectively. Then, the solution
  $\tilde \vx$ to the projection step is determined by $\vy\in\R^m$ and
  \begin{align}
    \tilde\vx &= \vx^{(0)} + \matv\vy\\
    \matw^*\mata\matv \vy &= \matv^* \vr^{(0)}.
  \end{align}
  It is thus obtained by solving an $m$-by-$m$ linear system, called
  the projected system or the \define{Galerkin
    equations}. $\matw^*\mata\matv\in\R^{m\times m}$ is the
  \define{projected matrix}.
\end{Definition}

\begin{proof}
  See \cite[Section 5.1.2]{Saad00}.
\end{proof}

\begin{Theorem}{projected-invertible}
  Let one of the following conditions hold:
  \begin{enumerate}
  \item $\mata$ is symmetric, positive definite and $L=K$.
  \item $\mata$ is invertible and $L = \mata K$.
  \end{enumerate}
  Then, the projected matrix $\matw^*\mata\matv$ is invertible for any
  bases $\matv$ and $\matw$ of $K$ and $L$, respectively.
\end{Theorem}

\begin{Theorem}{projection-orthogonal-optimal}
  Let $\mata\in\Rnn$ be symmetric, positive definite. Then,
  $\tilde \vx$ is the result of the orthogonal ($L=K$) projection
  method with initial vector $\vx^{(0)}$ if and only if it minimizes
  the \putindex{A-norm} of the error over the space $\vx^{(0)}+K$. Namely, for the
  solution $\vx$ of $\mata\vx=\vb$ there holds
  \begin{gather}
    \norm{\tilde\vx-\vx}_A = \min_{\vy\in\vx^{(0)}+K} \norm{\vy-\vx}_A.
  \end{gather}
  Here, $\norm\vy_A = \sqrt{\vy^*\mata\vy}$.
\end{Theorem}

\begin{Theorem}{projection-oblique-optimal}
  Let $\mata\in\Rnn$ and $L=\mata K$. Then, $\tilde \vx$ is the result
  of the (oblique) projection method with initial vector $\vx^{(0)}$
  if and only if it minimizes the Euclidean norm of the residual
  $\vb-\mata\tilde\vx$ over the space $\vx^{(0)}+K$. Namely, there
  holds
  \begin{gather}
    \norm{\vb-\mata\tilde\vx}_2
    = \min_{\vy\in\vx^{(0)}+K} \norm{\vb-\mata\vy}_2
  \end{gather}
\end{Theorem}

\begin{Example}{projection-1d}
  A one-dimensional projection method can be characterized by two
  vectors $\vv$ and $\vw$. Then,
  \begin{gather}
    K = \spann\vv,
    \qquad L = \spann\vw.
  \end{gather}
  The new solution is
  \begin{gather}
    \tilde\vx = \vx^{(0)}+\alpha \vv,
  \end{gather}
  where
  \begin{gather}
    \alpha = \frac{\scal(\vr^{(0)},\vw)}{\scal(\mata\vv,\vw)}
  \end{gather}
  is determined from the Galerkin condition $\vr^{(0)}-\mata\vy \perp \vw$
\end{Example}

\begin{Algorithm*}{steepest-descent-algol}{The steepest descent method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$ s.p.d.; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\vr,\vr)}{\scal(\mata\vr,\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{steepest-descent}
  Each step of the steepest descent method computes the minimum of
  $F(\vy) = \norm{\vy-\vx}_A^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $-\nabla F(\vx^{(k)})$.
\end{Lemma}

\begin{remark}
  In this algorithm, the operation $\mata\vr$ is applied twice. Since
  in most implementations this will be the part with the most
  computational operations, we introduce an auxiliary vector
  $\vp = \mata\vr$, which can be used in lines 3 and 5. At the cost of
  one additional vector, the numerical effort per step is almost
  cut in half.
\end{remark}

\begin{Remark}{convergence-residual}
  The residual $\vr = \vb - \mata\tilde\vx$ of the current iterate
  $\tilde\vx$ measures the misfit of $\tilde\vx$ in the equation
  $\mata\vx = \vb$. Since the error $\tilde\vx-\vx$ is unknown, we can
  use it as criterion for the accuracy of the current
  solution. Indeed, there holds
  \begin{gather}
    \norm{\vx-\tilde\vx}
    = \norm*{\mata^{-1}\bigl(\mata\vx - \mata\tilde\vx\bigr)}
    = \norm*{\mata^{-1}\bigl(\vb - \mata\tilde\vx\bigr)}
    \le \norm{\mata^{-1}}\norm{\vr}.
  \end{gather}
  Therefore, the criterion for convergence of the algorithm is typically
  \begin{gather}
    \norm{\vr} \le \text{TOL},
  \end{gather}
  where TOL is a tolerance chosen by the user.

  Note, that the computation of $\vr$ is subject to roundoff
  errors. Thus, the tolerance should always be chosen
  \begin{gather}
    \text{TOL} > c\norm{\vb}\eps,
  \end{gather}
  where $\eps$ is the machine accuracy and $c$ should account for
  error accumulation in the matrix-vector product.
\end{Remark}

\begin{Algorithm*}{steepest-descent-python1}{Steepest descent in Python}
  \lstinputlisting{python/steepest-descent.py}
\end{Algorithm*}

\begin{remark}
  Depending on the quality of the compiler/interpreter, the code line
  \begin{lstlisting}[language=Python,numbers=none]
    x += alpha*r
  \end{lstlisting}
  may involve creating an auxiliary vector $\vp = \alpha \vr$ and
  adding this vector to $\vx$.  Obviously, this could be avoided by
  directly implementing
  \begin{lstlisting}[language=Python,numbers=none]
    for i in range(0,n):
      x[i] += alpha*r[i]
  \end{lstlisting}
  Since operations like this are ubuquitous in scientific computing,
  they were standardized early on in the BLAS (basic linear algebra
  subroutines) library. It contains the FORTRAN function
  \begin{lstlisting}[language=Fortran,numbers=none]
    SUBROUTINE DAXPY( n, alpha, x, incx, y, incy)
  \end{lstlisting}
  which computes $\vy\gets\vy+\alpha\vx$ for double precision vectors
  of length $n$ (the increment arguments allow to skip elements). For
  usage in Python, there is a wrapper
  \begin{lstlisting}[language=Python,numbers=none]
    scipy.linalg.blas.daxpy(x, y[, n, a, offx, incx, offy, incy])
  \end{lstlisting}
\end{remark}

\begin{Algorithm*}{steepest-descent-python2}{Steepest descent with daxpy}
  \lstinputlisting{python/steepest-descent-axpy.py}
\end{Algorithm*}

\begin{Algorithm*}{minimal-residual-algol}{The minimal residual method}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vx,\vb\in\R^n$
    \State $\vr\gets \vb-\mata\vx$
    \Repeat
    \State $\alpha\gets\frac{\scal(\mata\vr,\vr)}{\scal(\mata\vr,\mata\vr)}$
    \State $\vx \gets \vx + \alpha\vr$
    \State $\vr \gets \vr-\alpha\mata\vr$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{minimal-residual}
  Each step of the minimal method computes the minimum of
  $F(\vy) = \norm{\vb-\mata\vy}_2^2$ along the line from the current
  iterate $\vx^{(k)}$ in direction $\vr$.
\end{Lemma}

\begin{Lemma}{lucky-breakdown}
  The iteration sequences $\{\vx^{(k)}\}$ of the steepest descent and
minimal residual methods, respectively, are well defined except for
the case where $\vx^{(k)}$ is the exact solution $\vx$ and thus
$\vr^{(k)} = 0$.

We refer to this phenomenon as \define{lucky breakdown}, since the
method only fails after the exact solution has been found.
\end{Lemma}

\begin{Lemma*}{kantorovich-inequality}{Kantorovich inequality}
  Let $\mata\in\Rnn$ be symmetric, positive definite with minimal and
  maximal eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, for
  $\vx\in\R^n$ there holds
  \begin{gather}
    \frac{\scal(\mata\vx,\vx)\scal(\mata^{-1}\vx,\vx)}{\scal(\vx,\vx)^2}
    \le \frac{(\lambda_{\min}+\lambda_{\max})^2}{4\lambda_{\min}\lambda_{\max}}
  \end{gather}
\end{Lemma*}

\begin{Theorem}{steepest-descent-convergence}
  Let $\mata\in\R^nn$ be symmetric, positive definite with extremal
  eigenvalues $\lambda_{\min}$ and $\lambda_{\max}$. Then, the error
  $\ve^{(k)} = \vx-\vx^{(k)}$ of the steepest descent method admits
  the estimate
  \begin{gather}
    \norm{\ve^{(k+1)}}_A \le \rho \norm{\ve^{(k)}}_A,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho
    = \frac{\lambda_{\max}-\lambda_{\min}}{\lambda_{\max}+\lambda_{\min}}
    = \frac{\cond_2\mata - 1}{\cond_2\mata+1}
    = 1-\frac2{\cond_2\mata+1}
  \end{gather}
\end{Theorem}

\begin{Theorem}{minimal-residual-convergence}
  Let $\mata\in\Rnn$ such that its symmetric part $(\mata+\mata^T)/2$
  is positive definite. Then, the residuals $\vr^{(k)}$ of the minimal
  residual method admit the estimate
  \begin{gather}
    \norm{\vr^{(k+1)}}_2 \le \rho \norm{\vr^{(k)}}_2,
  \end{gather}
  where the contraction number is
  \begin{gather}
    \rho = \left(1-\frac{\mu^2}{\norm{\mata}^2}\right)^{\nicefrac12}
  \end{gather}
  and $\mu$ is the smallest eigenvalue of $(\mata+\mata^T)/2$.
\end{Theorem}

\begin{remark}
  Let $e_k$ be some measure of the error of a fixed-point iteration
  with contraction number $\rho$ after $k$ steps. Then, there holds
  \begin{gather}
    e_k \le \rho^k e_0.
  \end{gather}
  Thus, the number of steps needed to obtain a (relative) reduction of
  the error by a prescribed relative tolerance $\epsilon$, that is, to
  achieve $e_k/e_0\le\epsilon$, is
  \begin{gather}
    \label{eq:krylov:steps-convergence}
    k \ge \frac{\log\epsilon}{\log\rho}.
  \end{gather}
  From this formula, we realize that the number of steps of such a
  method grows linearly with the logarithm of the tolerance. Further,
  it is inverse proportional to the logarithm of the contraction
  number $\rho$.

  Note that logarithms with respect to any base can be used
  in~\eqref{eq:krylov:steps-convergence}, since the quotient is
  independent of the base.
\end{remark}

\begin{Definition}{convergence-rate-logarithmic}
  The (logarithmic) \define{convergence rate} of a contraction with
  \putindex{contraction number} $\rho$ is
  \begin{gather}
    r_c = -\log_{10} \rho.
  \end{gather}

  The average \define{observed convergence rate} of $k$ steps of a
  method with error measure $e_k$ is
  \begin{gather}
    \overline{r_c} = \frac1k \log_{10}\frac{e_0}{e_k}.
  \end{gather}
\end{Definition}

\begin{remark}
  The logarithmic convergence rate allows us to directly compare two
  iterative methods. Let's say, there are two iterations $M_1$ and
  $M_2$ and the convergence rate of $M_2$ ist twice the rate of
  $M_1$. Then, given an initial vector, $M_1$ will need twice as many
  steps compared to $M_2$ to reach the same accuracy.

  This implies that $M_2$ is favorable, if the effort for each step is
  less than twice the effort for $M_1$. If it is more than twice, then
  $M_1$ is the faster method in spite of the slower convergence.
\end{remark}

\subsection{Krylov spaces}

\begin{Definition}{krylov-space}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$, we define the
  \define{Krylov space}
  \begin{gather}
    \krylov_m = \krylov_m(\mata,\vv)
    = \spann{\vv,\mata\vv,\dots,\mata^{m-1}\vv}.
  \end{gather}
\end{Definition}

\begin{Definition}{grade-of-v}
  For a matrix $\mata\in\Rnn$ and a vector $\vv\in\R^n$ we define the
  \define{grade} of $\vv$, more precise the grade of $\vv$ with
  respect to $\mata$ as the minimal grade of a polynomial $p$ such
  that
  \begin{gather}
    p(\mata)\vv = 0
  \end{gather}
\end{Definition}

\begin{Lemma}{krylov-polynomial}
  Let $\mu$ be the grade of $\vv$ with respect to $\mata$.  The Krylov
  space $\krylov_m(\mata,\vv)$ is isomorphic to $\P_{m-1}$ if and only
  if $m\le \mu$. In particular, for any $\vw\in\krylov_m(\mata,\vv)$
  there is a unique polynomial $p\in\P_{m-1}$ such that
  \begin{gather}
    \label{eq:krylov-polynomial-1}
    \vw = p(\mata)\vv.
  \end{gather}

  The Krylov space $\krylov_m(\mata,\vv)$ is invariant under the
  action of $\mata$ if and only if $m\ge\mu$.
\end{Lemma}

\begin{proof}
  See \cite[Propositions 6.1 \& 6.2]{Saad00}.
  The definition of $\krylov_m(\mata,\vv)$ implies that any vector $\vw$ in this space has a representation
  \begin{gather}
    \vw = \sum_{i=0}^{m-1} \alpha_i \mata^i\vv =: p_{\vw}(\mata) \vv.
  \end{gather}
  Since the vectors in this sum span $\krylov_m$, its definition is
  $m$ if and only if they are linearly independent, which in turn is
  the case if and only if there is a nonzero polynomial
  $p_0\in\P_{m-1}$ such that
  \begin{gather}
    p_0(\mata)\vv = 0.
  \end{gather}
  By assumption, such a polynomial exists if and only if
  $\mu<m$. Thus, we have proven that $\dim \krylov_m = m$ if and only
  if $m\le \mu$ and thus the mapping induced
  by~\eqref{eq:krylov-polynomial-1} is an isomorphism.

  On the other hand, $\krylov_m$ is invariant, if and only if the
  vectors spanning $\krylov_{m+1}$ are linearly dependent. We have
  just proven that this is the case if and only if $\mu<m+1$ or
  $\mu\le m$.
\end{proof}

\begin{Lemma}{krylov-projector}
  Let $\matq_m$ be a projector onto $\krylov_m$ and define
  \begin{align}
    \mata_m\colon \krylov_m &\to \krylov_m\\
    \vv&\mapsto \matq_m \mata.
  \end{align}
  Then, for any polynomial $q\in\P_{m-1}$ there holds
  \begin{gather}
    q(\mata)\vv = q(\mata_m)\vv.
  \end{gather}
  For any polynomial $p\in\P_m$, there holds
  \begin{gather}
    \matq_m p(\mata)\vv = p(\mata_m)\vv
  \end{gather}
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.3]{Saad00}.
\end{proof}

\subsection{The Arnoldi procedure and GMRES}

\begin{Algorithm*}{arnoldi-1}{Arnoldi Gram-Schmidt}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j$
    \For{$i=1,\dots,j$}
    \State $h_{ij} \gets \scal(\mata\vv_j,\vv_i)$
    \State $\vw_j \gets \vw_j - h_{ij}\vv_i$
    \EndFor
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \If{$h_{j+1,j}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{Lemma}{arnoldi-krylov}
  Assume that the Arnoldi algorithm does not stop before step
  $m$. Then, the vectors $\vv_1,\dots,\vv_m$ form an orthonormal basis
  of $\krylov_m(\mata,\vv_1)$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.4]{Saad00}.
\end{proof}

\begin{Theorem}{arnoldi-projection}
  Let $\matv_m = (\vv_1,\dots,\vv_m)$ be the matrix of basis vectors
  generated by the Arnoldi method and let
  $\overline{\matH}_m\in\R^{m+1\times m}$ be the Hessenberg matrix of
  entries $h_{ij}$ computed in the algorithm. Let further
  $\matH_m\in\R^{m\times m}$ be the same matrix without the last
  row. Then, there holds
  \begin{align}
    \mata\matv_m &= \matv_m\matH_m+\vw_m\ve_m^T\\
                 &= \matv_{m+1}\overline{\matH}_m\\
  \end{align}
  and
  \begin{gather}
    \matH_m = \matv_m^T\mata\matv.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Proposition 6.5]{Saad00}.
\end{proof}

\begin{remark}
  The next step of the Arnoldi method can always be computed if
  $h_{j+1,j}\neq 0$ in line 9. If it cannot be, we speak of a
  \define{breakdown} of the method.
\end{remark}

\begin{Lemma}{arnoldi-breakdown}
  The Arnoldi method breaks down at step $j$ if and only if the subspace $\krylov_j$ is invariant under $\mata$ or equivalently, the minimal polynomial of $\vv_1$ is of degree $j$.
\end{Lemma}

\begin{proof}
  See \cite[Proposition 6.6]{Saad00}.
\end{proof}

\begin{Remark}{arnoldi-householder}
  The orthogonalization in Arnoldi's method is better performed by
  Householder reflections. This corresponds to performing a
  QR-factorization on the matrix
  \begin{gather}
    \left(\vv_1,\mata\vv_1, \mata^2\vv_1,\dots,\mata^{m-1}\vv_1\right).
  \end{gather}
  It does not produche the matrix $\matH$ in the same obvious way as
  the Gram-Schmidt version, but \cite[Algorithm 6.3]{Saad00} shows how
  to generate it.
\end{Remark}

\begin{Theorem}{arnoldi-linear-system}
  Given an initial vector $\vx^{(0)}$ and
  $\vr^{(0)} = \vb - \mata \vx^{(0)}$. Let $\beta=\norm{\vr^{(0)}}$
  and choose $\vv_1 =\nicefrac{\vr^{(0)}}{\beta}$ in Arnoldi's
  method. Then, the Galerkin approximation
  \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained in two steps. Solve in $\R^m$
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
  \end{gather}
  and let
  \begin{gather}
    \vx^{(m)} = \vx^{(0)} + \matv_m \vy_m.
  \end{gather}
\end{Theorem}

\begin{Theorem}{arnoldi-linear-residual}
  Let $\vx^{(m)}$ be the Galerkin solution in
  \slideref{Theorem}{arnoldi-linear-system}. Then, there holds
  \begin{gather}
    \vb-\mata\vx^{(m)} = -h_{m+1,m} \ve_m^T \vy_m \vv_{m+1},
  \end{gather}
  and therefore
  \begin{gather}
    \norm{\vb-\mata\vx^{(m)}}_2 = h_{m+1,m} \abs{\ve_m^T \vy_m}.
  \end{gather}  
\end{Theorem}


\begin{Algorithm*}{gmres}{Generalized Minimal Residual}
  Given $\vx^{(0)}\in\R^n$, the ``iterate'' $\vx^{(m)}$ of the
  \define{GMRES} method is computed by the following steps.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ of
    $\krylov_{m+1}(\mata,\vr^{(0)})$ and the matrix $\overline\matH_m\in\R^{m+1\times m}$.
  \item Solve the least squares problem
    \begin{gather}
      \norm{\beta\ve_1 - \overline\matH_m \vy}_2 \stackrel{!}{=} \min
    \end{gather}
    in $\R^m$ with $\beta = \norm{\vr^{(0)}}_2$.
  \item Let
    \begin{gather}
      \vx^{(m)} = \vx^{(0)} + \matv_{m}\vy
    \end{gather}
  \end{enumerate}
\end{Algorithm*}

\begin{remark}
  The GMRES method is not really an iterative method. Indeed, the
  intermediate iterates $\vx^{(k)}$ may not even be computed, since
  the norm of the residual can be computed by
  \slideref{Theorem}{arnoldi-linear-residual}.
\end{remark}

\begin{Theorem}{gmres-breakdown}
  If the GMRES method breaks down at step $m$, then
  $\vr^{(m)}=0$. Thus, the GMRES method only encounters a
  \putindex{lucky breakdown}.
\end{Theorem}

\begin{remark}
  The GMRES method encounters a problem if convergence is slow: the
  effort for computing the last basis vector in $\matv_m$ is of order
  $mn$, the effort for solving the least-squares problem is
  $m^3$. Similarly, the storage requirement for $\matv_m$ is $mn$ and
  for $\overline \matH_m$, it is $m^2$. Thus, the algorithm is
  feasible for large sparse matrices only if $m\ll n$.

  Therefore, we introduce two variants of the GMRES algorithm:
  \begin{itemize}
  \item Restarted GMRES simply deletes the whole basis $\matv_m$ and the matrix $\overline\matH_m$ every $k$-th step starts fresh.
  \item Truncated GMRES orthogonalizes only with respect to the last
    $k$ basis vectors.
  \end{itemize}
\end{remark}

\begin{Algorithm*}{gmres-restart}{Restarted GMRES}
  \begin{enumerate}
  \item Choose a maximal dimension $m$ of the Krylov space
  \item Begin with an initial guess $\vx^{(0)}$.
  \item For $k>0$, given $\vx^{(km)}$, perform the GMRES method with
    basis size $m$ to obtain $\vx^{((k+1)m)}$.
  \item Check the stopping criterion as usual inside the GMRES method
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{gmres-truncated}{Truncated GMRES}
  Run the GMRES method as usual, but in the Arnoldi process, only
  orthogonalize with respect to the last $k$ vectors for fixed $k$.
\end{Algorithm*}

\begin{remark}
  Note that both modifications destroy the minimization property of the method.
\end{remark}

\subsection{The Lanczos procedure and the conjugate gradient method}

\begin{Lemma}{arnoldi-symmetric}
  If the matrix $\mata$ is symmetric, the vectors of the Arnoldi
  procedure admit the three-term recurrence relation
  \begin{gather}
    \beta_{j+1}\vv_{j+1} = \mata \vv_j - \alpha_j \vv_j - \beta_j\vv_{j-1}
  \end{gather}
  with $\vv_0 = 0$, $\beta_1 = 0$ and
  \begin{gather}
    \alpha_j =  h_{jj}, \qquad \beta_j = h_{j-1,j},
    \qquad j=1,\dots
  \end{gather}
  In particular, orthogonalization is only necessary with respect to
  the previous two basis vectors.
\end{Lemma}

\begin{proof}
  See \cite[Section 6.6.1]{Saad00}.
\end{proof}

\begin{Algorithm*}{lanczos}{Lanczos}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$, $\vv_0=0$, $\beta_1=0$.
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j - \beta_j \vv_{j-1}$
    \State $\alpha_{j} \gets \scal(\vw_j,\vv_j)$
    \State $\vw_j \gets \vw_j - \alpha_j\vv_i$
    \State $\beta_{j+1} \gets \norm{\vw_j}_2$
    \If{$\beta_{j+1}=0$} \textbf{stop}\EndIf
    \State $\vv_{j+1} = \nicefrac{\vw_j}{\beta_{j+1}}$
    \EndFor
  \end{algorithmic}  
\end{Algorithm*}

\begin{Lemma*}{lanczos-linear}{Lanczos solver}
  The Galerkin approximation   \begin{align}
    x^{(m)} &\in x^{(0)} + \krylov_m(\mata,r^{(0)})\\
    b-\mata x^{(m)} &\perp \krylov_m(\mata,r^{(0)}),
  \end{align}
  is obtained as $\vx^{(m)} = \vx^{(0)} + \matv_m \vy_m$, where $\matv_m$ is the basis obtained from the Lanczos process and $\vy_m$ solves
  \begin{gather}
    \matH_m\vy_m = \beta\ve_1,
    \qquad \matH_m =
    \begin{pmatrix}
      \alpha_1 & \beta_2\\
      \beta_2 & \alpha_2 & \ddots\\
      &\ddots&\ddots&\beta_m\\
      &&\beta_{m}&\alpha_m
    \end{pmatrix}.
  \end{gather}
  For the residual, there holds
  \begin{gather}
    \vr^{(m)} = \vb-\mata\vx^{(m)} = -\beta_{m+1} \ve_m^T \vy_m \vv_{m+1}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  These are the statements of
  \slideref{Theorem}{arnoldi-linear-system} and
  \slideref{Theorem}{arnoldi-linear-residual}. Since the Lanczos
  process is just a specialization of the Arnoldi process, they still
  hold.
\end{proof}

\begin{remark}
  The main difference between Arnoldi and Lanczos is the fact that
  every step of the Lanczos process onle requires the previous two
  vectors, while Arnoldi requires storing the whole history.

  This implies, that Lanczos itself is much closer to the concept of
  an iterative method than Arnoldi, weren't it for the computation of
  $\vy_m$.

  We will now develop a new algorithm based on the observation that
  also the corrections for $\vx^{(m)}$ can be computed incrementally.
\end{remark}

\begin{Lemma}{lanczos-incremental}
  Given the vector $\vx^{(m)}$ in the Lanczos solver, the vector
  $\vx^{(m+1)}$ can be computed incrementally by an update of the form
  \begin{gather}
    \vx^{(m+1)} = \vx^{(m)} + \zeta_{m+1}\vp^{(m+1)},
  \end{gather}
  where the vector $\vp^{(m+1)}$ itself is computed only using the
  vectors $\vp^{(m)}$ and $\vv^{(m+1)}$.
\end{Lemma}

\begin{proof}
  See \cite[Section 6.7.1]{Saad00}.

  First, we compute the LU factorization of $\matH_m$ as
  \begin{gather}
    \matH_m =
    \begin{pmatrix}
      1\\\lambda_2&1\\
      &\ddots&\ddots\\
      &&\lambda_m&1
    \end{pmatrix}
    \times
    \begin{pmatrix}
      \eta_1&\beta_2\\
      &\ddots&\ddots\\
      &&\eta_{m-1}&\beta_m\\
      &&&\eta_m
    \end{pmatrix},
  \end{gather}
  where
  \begin{gather}
    \lambda_k = \frac{\beta_k}{\eta_{k-1}},
    \qquad
    \eta_k = \alpha_k - \lambda_k \beta_k.
  \end{gather}
  We note that LU decomposition works from top to bottom and left to
  right, such that already processed rows do not change when adding
  more steps to the algorithm.

  Now we introduce matrices and vectors
  \begin{gather}
    \matp_m = \matv_m \matu_m^{-1} \in \R^{n\times m},
    \qquad \vz_m = \matl_m^{-1} \beta \ve_1\in \R^m,
  \end{gather}
  such that
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \matv_m \matu_m^{-1} \matl_m^{-1} \beta \ve_1\\
    &= \vx^{(0)} + \matp_m \vz_m
  \end{align}
  With $\zeta_k$ following the recursion
  $\zeta_k = - \lambda_k \zeta_{k-1}$, we realize
  \begin{gather}
    \vz_m =
    \begin{pmatrix}
      \zeta_1\\\vdots\\\zeta_m
    \end{pmatrix}
    =
    \begin{pmatrix}
      \vz_{m-1}\\\zeta_m
    \end{pmatrix}.
  \end{gather}
  Let $\vp_k$ be the column vectors of $\matp_m$. Writing the
  definition of $\matp_m$ in the form $\matp_m \matu_m = \matv_m$, we
  obtain the relation
  \begin{gather}
    \label{eq:nla:krylov:lanczos-p}
    \vp_k = \frac1{\eta_k}\bigl(\vv_k - \beta_k \vp_{k-1}\bigr).
  \end{gather}
  Thus,
  \begin{gather}
    \matp_m =
    \begin{pmatrix}
      \vp_1,\dots,\vp_m
    \end{pmatrix}
    =
    \begin{pmatrix}
      \matp_{m-1},\vp_m
    \end{pmatrix}.
  \end{gather}
  There holds
  \begin{align}
    \vx^{(m)}
    &= \vx^{(0)} + \sum_{k=1}^m \zeta_k \vp_k,\\
    &= \vx^{(m-1)} + \zeta_m \vp_m.
  \end{align}
\end{proof}

\begin{remark}
  Thus, we have proven a simple update formula for the iterates
  $\vx^{(m)}$ based on an additional vector $\vp_m$, which is again
  obtained by an update formula.

  We have used an LU decomposition without pivoting though in the
  derivation of this algorithm, which might suffer from numerical
  instability. Therefore, we will now analyze the main properties of
  the sequences $\vv_m$, $\vp_m$ and $\vr_m$ and then write a new
  algorithm to obtain these sequences.
\end{remark}

\begin{Lemma}{lanczos-orthogonality}
  The sequences of vector $\vr_m = \vb-\mata\vx^{(m)}$ and $\vp_m$
  constructed in the preceding lemma have the following orthogonality
  properties:
  \begin{xalignat}2
    \scal(\vr_i,\vr_k) &= 0 & i&\neq k\\
    \scal(\mata\vp_i,\vp_k) &= 0 & i&\neq k.
  \end{xalignat}
\end{Lemma}

\begin{proof}
  Orthogonality of the residuals follows from
  \slideref{Lemma}{lanczos-linear}, since $\vr_m$ is aligned with
  $\vv_{m+1}$.

  The second orthogonality relation is equivalent to the fact that
  $\matp_m^T\mata\matp_m$ is diagonal.
  \begin{align}
    \matp_m^T\mata\matp_m
    &= \matu_m^{-T}\matv_m^T\mata\matv_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matH_m\matu_m^{-1}\\
    &= \matu_m^{-T}\matl_m.
  \end{align}
  The first term is symmetric.  The last is the product of two lower
  triangular matrices and thus lower triangular. Thus, the matrix must
  be diagonal.
\end{proof}

\begin{Algorithm*}{cg}{Conjugate Gradient Method}
  \begin{algorithmic}[1]
    \State $\vr_0 = b-\mata \vx_0$
    \State $\vp_0 = \vr_0$
    \For{$j=0,1,\dots$}
    \State $\alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vp_j)}$
    \State $\vx_{j+1} = \vx_j + \alpha_j \vp_j$
    \State $\vr_{j+1} = \vr_j - \alpha_j \mata \vp_j$
    \State $\beta_j =\frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_j,\vr_j)}$
    \State $\vp_{j+1} = r_{j+1} + \beta_j \vp_j$
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  Note the index shift for the vectors $\vp_j$!
\end{remark}

\begin{Lemma}{cg-orthogonality}
  The sequences $\vx_j$ and $\vr_j$ produced by the cg method coincide
  with those of the Lanczos solver (in exact arithmetic). The sequence
  $\vp_j$ of the cg method coincides up to scalar factors with the one
  in \slideref{Lemma}{lanczos-incremental}.
\end{Lemma}

\begin{proof}
  Note that the vectors $\vp_j$ refers to the cg algorithm, the
  vectors from \slideref{Lemma}{lanczos-incremental} will be
  distinguished clearly.

  We start with the update formula
  $\vx_{j+1} = \vx_j + \alpha_j \vp_j$, which immediately implies
  \begin{gather}
    \vr_{j+1} = \vr_j - \alpha_j \mata\vp_j.
  \end{gather}
  Orthogonality between $\vr_{j+1}$ and $\vr_j$ implies
  \begin{gather}
    \alpha_j = \frac{\scal(\vr_j,\vr_j)}{\scal(\mata\vp_j,\vr_j)}.
  \end{gather}
  Since $\vr_j$ is a multiple of $\vv_{j+1}$, the update
  formula~\eqref{eq:nla:krylov:lanczos-p} for $\vp_{j+1}$ (after index
  shift) implies that the direction is a linear combination of
  $\vr_{j+1}$ and $\vp_j$. After arbitrary scaling,
  \begin{gather}
    \vp_{j+1} = \vr_{j+1} + \beta_j \vp_{j}.
  \end{gather}
  A-orthogonality implies
  \begin{gather}
    \beta_j = -\frac{\scal(\mata \vp_j,\vr_{j+1})}{\scal(\mata \vp_j,\vp_j)}.
  \end{gather}
  Note that the orthogonality relation determines $\vp_{j+1}$ uniquely
  up to scaling in this subspace. After the scaling has been chosen,
  $\vp_{j+1}$ the orthogonality relation for $\vr_{j+1}$ uniquely
  determines the update for $\vx_{j+1}$, such that the sequences
  indeed coincide.

  The definition of the coefficients still differs from the cg
  algorithm. But, we observe
  \begin{gather}
    \scal(\mata\vp_j,\vr_j) = \scal(\mata\vp_j,\vp_j-\beta_{j-1}\vp_{j-1})
    = \scal(\mata\vp_j,\vp_j).
  \end{gather}
  Similarly, from the update formula for the residual, we get
  \begin{gather}
    \mata\vp_j = \frac1{\alpha_j}\bigl(\vr_j-\vr_{j+1}\bigr),
  \end{gather}
  and thus,
  \begin{gather}
    \beta_j = \frac1{\alpha_j}\frac{\scal(\vr_{j+1},\vr_{j+1}-\vr_j)}{\scal(\mata\vp_j,\vp_j)} = \frac{\scal(\vr_{j+1},\vr_{j+1})}{\scal(\vr_{j},\vr_{j})}.
  \end{gather}
  Note that these forms avoid inner products of possibly almost
  orthogonal vectors.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
