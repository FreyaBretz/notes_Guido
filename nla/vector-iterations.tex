%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Problem}{intro-problem-vector-iterations}
	Consider the following matrix
	\begin{gather*}
	\mata =
	\begin{pmatrix}
	\cos\phi & -\sin\phi\\
	\sin\phi &  \cos\phi
	\end{pmatrix}^T
	\begin{pmatrix}
	1 & \\
	& c
	\end{pmatrix}
	\begin{pmatrix}
	\cos\phi & -\sin\phi\\
	\sin\phi &  \cos\phi
	\end{pmatrix}
	\end{gather*}
	with parameters $\phi\in[0,2\pi]$ and $c\in(0,1)$.
	\begin{enumerate}
		\item Determine (think, don't compute!) the eigenvalues and eigenvectors of $\mata$.
		\item (Programming) Write a program which computes the sequence
		$\vx^{(n)}\in\R^2$ defined as
		\begin{align*}
		\vx^{(n)} &= \mata \vx^{(n-1)}, \\
		\vx^{(0)} &= \vx^{*},
		\end{align*}
		for $\vx^{*} = (1,\ 0)^T$, $c = 0.1$, and
		$\phi=\frac\pi4$. Try playing with different values of those
		parameters.
		\item Is there a limit of $\vx^{(n)}$? What is about the case
		$c=1$?
		\item Compute the limit: $\lim_{n\to\infty}\mata^n$.
	\end{enumerate}
\end{Problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simple iterations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  The eigenspace of a dominant eigenvalue seems to get amplified
  whenever we multiply with the matrix $\mata$. Hence, we should be a
  able to approximate this eigenspace by a simple iteration.
\end{intro}

\begin{Algorithm*}{vector-iteration}{Vector iteration (power method)}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$, initial vector $\vv^{(0)}\in\C^n$ with $\norm{\vv}=1$
    \For{$k=1,\dots$ until convergence}
    \State $\vy^{(k)} \gets \mata \vv^{(k-1)}$
    \State $\vv^{(k)} = \frac{\vy^{(k)}}{\norm{\vy^{(k)}}}$
    \State $\alpha_k = \frac{\bigl(\vy^{(k)}\bigr)_i}{\bigl(\vv^{(k)}\bigr)_i}$ where $\abs*{\bigl(\vv^{(k)}\bigr)_i} = \norm*{\vv^{(k)}}_\infty $
    \EndFor
%    \State $\alpha = \frac{\vy^{(k_\max +1)}}{\vv^{(k_{\max})}_i}$
%    where $\abs{\vv^{(k_{\max})}_i}$ is maximal
  \end{algorithmic}
\end{Algorithm*}

\begin{Theorem}{vector-iteration}
  Let $\mata\in\Cnn$ be diagonalizable such that $\lambda_1$ is the
  unique dominant eigenvalue. Let furthermore the
  component of $v^{(0)}$ in direction of the first eigenvector be
  nonzero. Then, the factors
  \[\alpha_k := \frac{\vy^{(k+1)}_i}{\vv_i^{(k)}} \; \text{where} \; |\vv_i^{(k)|}| \; \text{is maximal}\]
  and vectors $v^{(k)}$ of the
  vector iteration converge to the eigenvalue $\lambda_1$ and its
  associated eigenvector. Moreover, there holds
    %\State $\alpha = \frac{\vy^{(k_\max +1)}}{\vv^{(k_{\max})}_i}$
    %where $\abs{\vv^{(k_{\max})}_i}$ is maximal
  \begin{align}
    \abs{\alpha_{k+1}-\lambda_1}
    &\le \frac{\abs{\lambda_1}}{\abs{\lambda_2}} \abs{\alpha_{k}-\lambda_1}\\
    \norm{v^{(k+1)}-u_1}
    &\le \frac{\abs{\lambda_1}}{\abs{\lambda_2}} \norm{v^{(k)}-u_1}
  \end{align}
\end{Theorem}

\begin{remark}
  The previous theorem actually holds eveon for matrices, which are
  not diagonalizable, but then the proof becomes more involved. Since
  we are not interested in eigenvalues, which are not semi-simple, we
  decided for the simpler version.

  The proof can be easily improved to cover the case that $\lambda_1$
  is not a single eigenvalue. In exact arithmetic, there will be no
  change of the result. When computing with floating point numbers,
  there will be a small glitch. It is left to the reader to find it.
\end{remark}

\begin{remark}
  The algorithm can be improved in several ways. First, we saw in the
  proof that it generates a converging sequence of vectors. Threfore,
  we can save the effort of computing eigenvalues inside the iteration
  and postpone it until the end.

  Second, if $\mata$ is a Hermitian matrix, the computation of the
  eigenvalue can be improved by usig the Rayleigh quotient.

  Using both statements, we obtain an optimized version of the vector
  iteration.
\end{remark}

\begin{Algorithm*}{vector-iteration-rayleigh}{Power method (optimized)}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$ Hermitian, initial vector $\vv^{(0)}\in\C^n$ with $\norm{\vv}=1$
    \For{$k=1,\dots$ until convergence}
    \State $\vy^{(k)} \gets \mata \vv^{(k-1)}$
    \State $\vv^{(k)} = \frac{\vy^{(k)}}{\norm{\vy^{(k)}}}$
    \EndFor
    \State $\lambda_1 \approx R_\mata(\vv^{(k_{\max})} = \scal(\mata\vv^{(k_{\max})},\vv^{(k_{\max})})$
  \end{algorithmic}
\end{Algorithm*}


\begin{todo}
  GM:
  Why is it required, that the eigenvalue is single? That is not included in the previous Theorem? \\
  Espacially a diagonalizable matrix can have multiple eigenvalues
\end{todo}
\begin{Remark}{vector-iteration}
  The proof actually requires, that the entry defining $\alpha_k$
  remains the same during the iteration, at least during the steps
  used for detecting convergence.

  The result does not actually require that $\mata$ is diagonalizable,
  as long as $\lambda_1$ is single and of largest modulus.
\end{Remark}

\begin{todo}
  GM:
  Fixed a Symbol
\end{todo}
\begin{Lemma}{Rayleigh-approximation}
  Let $(\lambda,\vv)$ be an eigenpair of the Hermitian matrix
  $\mata\in\Cnn$, and let $\vw\in\C^n$. Then, there is a constant $C$ depending only on the matrix $\mata$ such that there holds
  \begin{gather}
    \abs{R_\mata(\vw)-\lambda} \le C \norm{\vw-\vv}^2,
  \end{gather}
  where $R_\mata(\vw)$ is the \putindex{Rayleigh quotient} from
  \slideref{Definition}{rayleigh-quotient}.
\end{Lemma}

\begin{Algorithm*}{shifted-vector-iteration}{Shifted vector iteration}
  The vector iteration can be applied to the matrix $\mata-\sigma\id$
  for some $\sigma\in\C$.

  Then, $\alpha_k$ converges to the eigenvalue $\lambda$ such that
  $\lambda-\sigma$ has largest modulus. $v^{(k)}$ converges to an
  eigenvector for this eigenvalue.
\end{Algorithm*}

\begin{todo}
  GM: added the shifted matrix polynomial
\end{todo}
\begin{Definition}{shifted-matrix-polynomial}{Shifted Matrix Polynomial}
  Let \(\mata \in \C^{n \times n}\) be a square matrix.
  The \define{matrix polynomial} of degree \(m\), with shift parameter \(\rho_1, \ldots, \rho_m\) is defined by
  \[ \mathbf{p(A)} := \left( A - \varrho_1 \id \right) \dots \left( A - \rho_m\right).\]
\end{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Subspace iterations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  
\end{intro}

\begin{todo}
  GM:
  Moved the inverse iteration back after the QR Iteration, and instead inserted the subspace iteration.\\
  Insert an exercise to show why the shifted matrix polynomial is required.
\end{todo}

\begin{Algorithm*}{subspace-iteration-polynomial}{Orthogonal subspace iteration}
  \begin{algorithmic}[1]
    \Require $\mata\in\Cnn$ and choose $\matx_0 \in \C^{n\times m}$.
    \For {$k=1,\ldots$ until convergence}
    \State $\matq_k\matr_k \gets \matx_{k-1}$ \Comment{QR factorization}
    \State $\matx_{k} \gets p(\mata) \matq_{k}$
    \EndFor
    \State {$\lambda_i \approx \vq_{k_{\max},i}^* \mata \vq_{k_{\max},i},\qquad i=1,\dots,m$}\
    \Comment{Rayleigh quotient}
  \end{algorithmic}
\end{Algorithm*}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
