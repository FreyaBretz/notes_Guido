\usepackage{algpseudocode}
\lstset{language=Python,numbers=left,resetmargins=true,xleftmargin=8pt,basicstyle=\small,numberstyle=\scriptsize}
\usetikzlibrary{svg.path}
\excludecomment{solution}
\input{mixed/fig/tikzsettings}

\title{Numerical Linear Algebra}
\author{Guido Kanschat}
\date{\today}

\def\esp#1{V_{#1}}

\begin{document}
\maketitle
\tableofcontents
\chapter{Dense Algebraic Eigenvalue Problems}
\begin{intro}
  We refer to problems in linear algebra which allow storing the
  complete matrix as \define{dense linear algebra}. They are typically
  characterized by dimensions into the hundreds, possibly thousands,
  and any entry in the matrix may have a nonzero value. Such matrices
  are typically stored as a rectangular or quadratic array of numbers,
  and we can perform manipulations based on the matrix entry.

  In contrast, we will turn to \define{sparse linear algebra} in the
  later chapters, where dimensions got up to millions and billions
  ($10^9$). Since currently no computer on earth can store a matrix
  with $10^{18}$ entries, those matrices will be characterized by the
  fact that each row only contains very few nonzero entries, or that
  the matrix is not stored, but only available algorithmically in the
  form of a function performing the action $\vx\mapsto \mata
  \vx$. Thus, access to and manipulation of matrix entries is not
  possible, and we have to focus on methods only using the properties
  of the matrix as a linear mapping.
\end{intro}

\section{Mathematical background}
\subsection{Definition of Eigenvalue Problems}
\input{def-evp}
\section{Well-posedness of the EVP and bounds on eigenvalues}
\input{conditioning}

\section{Vector iterations}
\input{vector-iterations}

\section{Subspace iterations and the QR method}
\input{qr}

%\input{orthopoly}

\chapter{Solving Large Sparse Linear Systems}

\section{Motivation: discretization of partial differential equations}

\input{sparse}

\section{Basic iterative methods}
\input{iterations}

\section{Krylov-space methods}
\input{Krylov}

\chapter{Large Sparse Eigenvalue Problems}

\appendix
\chapter{Basics from Linear Algebra}
\section{Bases and matrices}
\subsection{Matrix notation for bases}
\input{bases}
\subsection{Similarity transformations}
\input{similarity}
\section{Inner products and orthogonality}
\input{inner}
\section{Projections}
\input{projections}

\chapter{Basics from Numerical Analysis}

\section{QR decomposition}
\input{qr-decomposition}

\section{Matrix norms and spectral radius}

\begin{Definition}{spectral-radius}
  The \define{spectral radius} of a matrix $\mata\in\Cnn$ is the
  maximal absolute value of its eigenvalues, that is
  \begin{gather}
    \rho(\mata) = \max_{\lambda\in\sigma(\mata)} \abs{\lambda}.
  \end{gather}
\end{Definition}

\begin{Lemma*}{spectral-radius}{Properties of the spectral radius}
  For any matrix norm $\norm\cdot$ and for any matrix $\mata\in\Cnn$ there holds
  \begin{gather}
    \rho(\mata) = \lim_{k\to\infty} \norm{\mata^k}^{1/k}.
  \end{gather}

  The sequence $\vx^{(k)} = A^k\vx$ converges to zero for all
  $\vx\in\C^n$ if and only if $\rho(\mata)<1$.

  For any $\epsilon>0$
  there is a matrix norm $\norm\cdot$ such that
  \begin{gather}
    \norm{\mata} \le (1+\epsilon) \rho(\mata) \qquad \forall \mata\in\Cnn.
  \end{gather}
\end{Lemma*}

\bibliographystyle{alpha}
\bibliography{all}
\printindex

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
