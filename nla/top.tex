\usepackage{algpseudocode}
\lstset{language=Python,numbers=left,resetmargins=true,xleftmargin=8pt,basicstyle=\small,numberstyle=\scriptsize}
\usetikzlibrary{svg.path}
\excludecomment{solution}
\input{mixed/fig/tikzsettings}

\title{Numerical Linear Algebra}
\author{Guido Kanschat}
\date{\today}

\def\esp#1{V_{#1}}

\begin{document}
\maketitle
\tableofcontents
\chapter{Dense Algebraic Eigenvalue Problems}
\begin{intro}
  We refer to problems in linear algebra which allow storing the
  complete matrix as \define{dense linear algebra}. They are typically
  characterized by dimensions into the hundreds, possibly thousands,
  and any entry in the matrix may have a nonzero value. Such matrices
  are typically stored as a rectangular or quadratic array of numbers,
  and we can perform manipulations based on the matrix entry.

  In contrast, we will turn to \define{sparse linear algebra} in the
  later chapters, where dimensions got up to millions and billions
  ($10^9$). Since currently no computer on earth can store a matrix
  with $10^{18}$ entries, those matrices will be characterized by the
  fact that each row only contains very few nonzero entries, or that
  the matrix is not stored, but only available algorithmically in the
  form of a function performing the action $\vx\mapsto \mata
  \vx$. Thus, access to and manipulation of matrix entries is not
  possible, and we have to focus on methods only using the properties
  of the matrix as a linear mapping.
\end{intro}

\section{Mathematical background}
\subsection{Definition of Eigenvalue Problems}
\input{def-evp}
\section{Well-posedness of the EVP and bounds on eigenvalues}
\input{conditioning}

\section{Vector iterations}
\input{vector-iterations}

\section{Subspace iterations and the QR method}
\input{qr}

%\input{orthopoly}

\chapter{Solving Large Sparse Linear Systems}

\section{Motivation: discretization of partial differential equations}

\input{sparse}

\section{Basic iterative methods}
\input{iterations}

\section{Krylov-space methods}
\input{Krylov}

\chapter{Large Sparse Eigenvalue Problems}

\begin{intro}
  In this chapter we study algorithms for finding a small number
  $n_{\text{ev}}$ of eigenvalues and corresponding eigenvectors of a
  large matrix $\mata\in\Rnn$, where $n$ is large, for instance
  $n>10^6$. Like in the previous chapter, this is achieved by
  projecting thw peoblem into smaller subspaces, where we can solve
  the eigenvalue problem using the methods of the first chapter.
\end{intro}

\section{Projected subspace iteration}
\input{ev-projection}

\section{The Arnoldi/Lanczos method}
\input{ev-arnoldi}


\appendix
\chapter{Basics from Linear Algebra}
\section{Bases and matrices}
\subsection{Matrix notation for bases}
\input{bases}
\subsection{Similarity transformations}
\input{similarity}
\section{Inner products and orthogonality}
\input{inner}
\section{Projections}
\input{projections}

\chapter{Basics from Numerical Analysis}

\section{QR decomposition}
\input{qr-decomposition}

\section{Matrix norms and spectral radius}

\begin{Definition}{spectral-radius}
  The \define{spectral radius} of a matrix $\mata\in\Cnn$ is the
  maximal absolute value of its eigenvalues, that is
  \begin{gather}
    \rho(\mata) = \max_{\lambda\in\sigma(\mata)} \abs{\lambda}.
  \end{gather}
\end{Definition}

\begin{Lemma*}{spectral-radius}{Properties of the spectral radius}
  For any matrix norm $\norm\cdot$ and for any matrix $\mata\in\Cnn$ there holds
  \begin{gather}
    \rho(\mata) = \lim_{k\to\infty} \norm{\mata^k}^{1/k}.
  \end{gather}

  The sequence $\vx^{(k)} = A^k\vx$ converges to zero for all
  $\vx\in\C^n$ if and only if $\rho(\mata)<1$.

  For any $\epsilon>0$
  there is a matrix norm $\norm\cdot$ such that
  \begin{gather}
    \norm{\mata} \le (1+\epsilon) \rho(\mata) \qquad \forall \mata\in\Cnn.
  \end{gather}
\end{Lemma*}

\section{Chebyshev polynomials}

\begin{Definition}{chebyshev-polynomials}
  The \define{Chebyshev polynomials} $\pchebyshev_k$ are defined by the
  three-term recurrence relation
  \begin{gather}
    \pchebyshev_{k}(x) = 2x \pchebyshev_{k-1}(x) - \pchebyshev_{k-2}(x),
  \end{gather}
  with $T_0 \equiv 1$ and $T_1(x) = x$.
  They are orthogonal with respect to the inner product
  \begin{gather}
    \scal(p,q) = \int_{-1}^1 \tfrac1{\sqrt{1-x^2}} \,p(x)q(x)\dx.
  \end{gather}
\end{Definition}

\begin{Lemma}{chebyshev-representation}
  Chebyshev polynomials admit the trigonometric representation
  \begin{gather}
    \pchebyshev_k(x) = \cos(k \operatorname{arccos} x).
  \end{gather}
  
  Furthermore, there holds
    \begin{gather}
    \pchebyshev_k = \frac12
    \left(
      \left(x-\sqrt{x^2-1}\right)^k
      +
      \left(x+\sqrt{x^2-1}\right)^k
    \right),\qquad\abs{x}\ge 1.
  \end{gather}
\end{Lemma}

\begin{Lemma}{chebyshev-abscissae}
  The Chebyshev polynomial $\pchebyshev_n$ has $n$ roots in the
  interval $(-1,1)$ at
  \begin{gather}
    x_k = \cos\left(\pi\frac{(k-\nicefrac12)}{n}\right),
    \qquad k=1,\dots,n.
  \end{gather}
  It alternatingly assumes the values $\pm1$ at the \define{Chebyshev abscissae}
  \begin{gather}
    x_k = \cos\left(\pi\frac kn\right),
  \end{gather}
  wiyh $\pchebyshev_n(1) = 1$ and $\pchebyshev_n(-1) = (-1)^n$.
\end{Lemma}

\begin{Theorem}{chebyshev-minimal-1}
  For every polynomial $p\in \P_n$ with leading coefficient 1 there is $x\in[-1,1]$ such that $\abs{p(x)} \ge \nicefrac1{2^{n-1}}$. There holds
  \begin{gather}
   \frac1{2^{n-1}} \pchebyshev_n(x)
   = \operatorname*{arg min}_{\substack{p\in\P_n\\p = x^n+\cdots}}
   \max_{x\in[-1,1]}\abs{p(x)}.
  \end{gather}
\end{Theorem}

\begin{proof}
  See \cite[Satz 7.19]{DeuflhardHohmann08}.  The three-term recursion
  for Chebyshev polynomials implies that the leading coefficient of
  $\pchebyshev_n$ is $2^{n-1}$.

  Assume that there is another polynomial $p\in \P_n$ with leading
  coefficient $2^{n-1}$ such that
  \begin{gather}
    \max_{x\in[-1,1]} \abs{p(x)} < 1.
  \end{gather}
  Then, $q_n = \pchebyshev_n-p \in \P_{n-1}$. Furthermore, for the
  Chebyshev abscissae $\tilde x_j = \cos(\nicefrac{j\pi}{n})$ with
  $j=0,\dots,n$ there holds
  \begin{xalignat}4
    \pchebyshev_n(\tilde x_j) &= 1,
    & p(\tilde x_j) &< 1
    & q_n(\tilde x_j) &> 0,
    & j&\text{ gerade}\\
    \pchebyshev_n(\tilde x_j) &= -1,
    & p(\tilde x_j) &> -1
    & q_n(\tilde x_j) &< 0,
    & j&\text{ ungerade}.
  \end{xalignat}
  Therefore, 
  $q_n$ changes sign at at least $n$ points and thus has at least $n$ roots.
  From
  $q_n\in\P_{n-1}$ we deduce $q_n=0$ and thus $p=\pchebyshev_n$ as a contradiction.
  After scaling by $2^{n-1}$ we obtain
  \begin{gather}
    \operatorname*{min}_{\substack{p\in\P_n\\p = x^n+\cdots}}
   \max_{x\in[-1,1]}\abs{p(x)} \ge 1,
 \end{gather}
 with equality for the scaled Chebyshev polynomial.
\end{proof}

\begin{Lemma}{chebyshev-growth}
  Let
  \begin{gather}
    K = \bigl\{ p\in \P_n \;\big|\; \max_{x\in[-1,1]} \abs{p(x)} =1 \bigr\}.
  \end{gather}
  Then,
  \begin{gather}
    \abs{\pchebyshev_n(x)} \ge p(x) \qquad \forall p\in K, \quad \forall \abs{x} > 1.
  \end{gather}
\end{Lemma}

\begin{proof}
  We conduct the proof for $x>1$ where $\pchebyshev_n(x) > 0$.
  Let $\tilde p\in K$ such that
  $\abs{\tilde p(y)} \ge \pchebyshev_n(y)$ for some $y>1$. Let
  $\gamma = \pchebyshev_n(y)/\tilde p(y)$ and
  $p(x) = \tilde p(x)\gamma$, such that
  $q(x) = p(x) - \pchebyshev_n(x) \in \P_n$ has a root in $y$.
  Furthermore,
  \begin{gather}
    \max_{x\in[-1,1]} \abs{p(x)} =\gamma < 1
  \end{gather}

  Thus, $q(x)$ has alternating sign in the $n+1$ Chebyshev abscissae and due to continuity $n$ roots in
  $(-1,1)$. Hence, it has $n+1$ roots and therefore
  $q \equiv 0$. We conclude $p \equiv \pchebyshev_n$ and since
  $\norm{p}_{\infty;[0,1]} = 1$ there holds $\tilde p = p$.
\end{proof}

\begin{Corollary}{chebyshev-minimal-2}
  Let $[a,b]$ be an interval with $0 < a$. Then, the polynomial
  \begin{gather}
    \widehat \pchebyshev_n(x)
    = \frac{\pchebyshev_n\left(\frac{a+b-2x}{b-a}\right)}%
    {\pchebyshev_n\left(\frac{a+b}{b-a}\right)}
  \end{gather}
  solves the minimization problem
  \begin{gather}
    \widehat \pchebyshev_n(x)
    = \operatorname*{argmin}_{\substack{p\in\P_n\\p(0) = 1}}
    \max_{x\in[a,b]}{\abs{p(x)}}.
  \end{gather}
  There holds
  \begin{align}
    \label{eq:chebyshev-cg1}
    \max_{x\in[a,b]}{\abs{\widehat \pchebyshev_n(x)}}
    &= \left(\pchebyshev_n\left(\frac{a+b}{b-a}\right)\right)^{-1}\\
    & \le 2 \left(\frac{\sqrt b-\sqrt a}{\sqrt b + \sqrt a}\right)^n.
  \end{align}
\end{Corollary}

\begin{proof}
  The Chebysshev polynomial $\pchebyshev_n$ is the one with minimal maximum on $[-1,1]$ and maximal growth outside this interval. Thus, if we transform it to the interval $[a,b]$ by mapping
  \begin{gather}
    x \mapsto \frac{a+b-2x}{b-a},
  \end{gather}
  it is the polynomial with maximal absolute value 1 inside $[a,b]$
  with maximal value at 0. Dividing by this value, it solves the
  stated minimization problem and \eqref{eq:chebyshev-cg1} holds.

  To further estimate this value note that
  \begin{align}
    \pchebyshev_n(x)
    &= \frac12
      \left(x-\sqrt{x^2-1}\right)^n
      +
      \left(x+\sqrt{x^2-1}\right)^n\\
    &\ge \left(x+\sqrt{x^2-1}\right)^n.
  \end{align}
  Entering $x = \frac{a+b}{b-a}$ yields
  \begin{align}
    \frac{a+b+\sqrt{(a+b)^2-(b-a)^2}}{b-a}
    &= \frac{(\sqrt a + \sqrt b)^2}{b-a}\\
    &= \frac{\sqrt b + \sqrt a}{\sqrt b-\sqrt a}.
  \end{align}
\end{proof}

\bibliographystyle{alpha}
\bibliography{all}
\printindex

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
