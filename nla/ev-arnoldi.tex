
\begin{intro}
  In this section, we investigate Krylov space projection methods in
  more detail. All results hold, with the due simplifications for the
  Lanczos method as well. Note that the \define{Arnoldi factorization}
  in \slideref{Definition}{arnoldi-factorization} is a reformulation
  of \slideref{Theorem}{arnoldi-projection} and of
  \slideref{Lemma}{arnoldi-w}.
\end{intro}

\begin{notation}
  As with all methods for computing eigenvalues of large, sparse
  systems, we will only compute a subset of the eigenvalues. An
  important concept of the methods developed below is the idea ow
  \define{wanted eigenvalues} and \define{unwanted eigenvalues}.

  Given a set of Ritz values or computed eigenvalues, we will always
  assume that we can decide which of them are wanted. The complement
  set is then unwanted.

  Note that there might not be a clear mathematical definition of
  wanted eigenvalues, as long as at any step of the iteration we can
  select them. In particular, a Ritz value flagged as wanted in one
  step of the iteration might be unwanted in the next step, when
  another one fits the criteria better.

  Typical criteria are greatest, smallest, greatest/smallest real
  value, and greatest/smallest by magnitude.
\end{notation}

\begin{Algorithm*}{ev-arnoldi}{Arnoldi for eigenvalues}
  Given an initial vector $\vv_1$ with $\norm{\vv_1}=1$.
  \begin{enumerate}
  \item Compute the Arnoldi basis $\matv_{m+1}$ and the projected matrix
    \begin{gather}
      \matH_m = \matv_m^*\mata\matv_m.
    \end{gather}
  \item Compute the Schur form of $\matH_m$ by QR iteration
    \begin{gather}
      \matr_m = \matq_m^* \matH_m\matq_m.
    \end{gather}
  \item Use the Ritz values $\lambda_i^{(m)}=r_{ii}^{(m)}$ .
  \item Compute the approximate Schur vectors $\matv_m\matq_m$.
  \end{enumerate}
\end{Algorithm*}

\begin{remark}
  The previous algorithm only shows the basic version of Arnoldi's
  method for eigenvalues in complex arithmetic. Modifications which we
  have applied to the QR iteration can be applied here as well. In
  particular,
  \begin{enumerate}
  \item If the matrix $\mata$ is normal, we can compute the
    eigenvalues and eigenvectors of $\matH_m$ in step 2, such that
    step 4 yields the Ritz vectors right away. This is not advisable
    if $\mata$ is not normal, since eigenvector computation might be
    unstable.
  \item The method can be applied in real arithmetic. If then $\mata$
    is nonsymmetric and has compex eigenvalues, $\matr_m$ is the real
    Schur form and the computation of Ritz values is modified
    accordingly.
  \end{enumerate}
\end{remark}

\begin{Definition}{arnoldi-factorization}
  The result of $m$ steps of the Arnoldi algorithm is the
  \define{Arnoldi factorization}
  \begin{gather*}
    \label{eq:ev-arnoldi-projection}
    \mata\matv_m = \matv_m\matH_m + h_{m+1,m}\vv_{m+1}\ve_m^*,
  \end{gather*}
  where $\matv_m\in\C^{n\times m}$ is the matrix of mutually
  orthogonal \define{Arnoldi vectors} and $\matH_m\in\R^{m\times m}$
  is Hessenberg. There holds
  \begin{gather}
    \matv_m^*\vv_{m+1} = 0.
  \end{gather}
\end{Definition}

\begin{Lemma}{ev-arnoldi-a-posteriori}
  Let $(\lambda_i^{(m)},\vy_i^{(m)})$ be an eigenpair of the projected
  matrix $\matH_m$ of Arnoldi's method. Then, the Ritz vector
  $\vu_i^{(m)} = \matv_m \vy_i^{(m)}$ has a residual represented as
  \begin{gather}
    \left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}
    = h_{m+1,m}\ve_m^*\vy_i^{(m)}\vv_{m+1}.
  \end{gather}
  In particular, its norm is
  \begin{gather}
    \norm*{\left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}}_2
    =  \abs{h_{m+1,m}\ve_m^*\vy_i^{(m)}}.
  \end{gather}
  Note that $\ve_m^*\vy_i^{(m)}$ is the $m$-th entry of
  $\vy_i^{(m)}$. In particular, we can estimate the residual of the
  Ritz pair without computing the Ritz vector.
\end{Lemma}

\begin{proof}
  See also~\cite[Proposition 6.8]{Saad11}.
  Multiplying~\eqref{eq:ev-arnoldi-projection} from the
  right with $\vy_i^{(m)}$, we obtain
  \begin{align}
    \mata\matv_m\vy_i^{(m)}
    &= \matv_m\matH_m\vy_i^{(m)} + h_{m+1,m}\vv_{m+1}\ve_m^*\vy_i^{(m)}\\
    &=\lambda_i^{(m)} \matv_m\vy_i^{(m)}
      + h_{m+1,m}\bigl(\ve_m^*\vy_i^{(m)}\bigr)\vv_{m+1},
  \end{align}
  which is the result as soon as we enter $\vu_i^{(m)} = \matv_m \vy_i^{(m)}$.
\end{proof}

\begin{Corollary}{ev-arnoldi-breakdown}
  If the Arnoldi algorithm breaks down, the generated Ritz vectors are
  eigenvectors.
\end{Corollary}

\begin{remark}
  While this breakdown was ``lucky'' for linear solvers, it is not
  entirely so in the case of eigenvalue computations. On the one hand,
  we have the preceding corollary, which states that the computed Ritz
  vectors are actually eigenvectors and the Ritz values are
  eigenvalues of $\mata$. This is good, if all desired eigenpairs are
  in this set.

  On the other hand, a breakdown actually
  breaks the algorithm if the number of desired eigenvalues is higher
  than the dimension of the Krylov space. In this case, we will have
  to insert a new initial vector during the computation of the Krylov
  space.

  As always with numerical computations, a clear breakdown is
  unlikely, but a near breakdown may happen more likely. Detecting
  this is related to loss of orthogonality.
\end{remark}

\begin{Definition}{arnoldi-locking}
  In the case of a breakdown at step $m$ of the Arnoldi algorithm,
  we restart the algorithm after \define{locking} the wanted eigenvalues.

  \begin{enumerate}
  \item Choose the wanted eigenvalues from the computed ones. Store
    the corresponding eigenvectors $\vu_1,\dots,\vu_k$.
  \item Choose a new initial vector $\vv_1$ which is orthogonal to all
    previously converged eigenvectors $\vu_1,\dots,\vu_m$, as the
    eigenvalues $\lambda_{k+1},\dots,\lambda_m$ are unwanted.
  \item Build the Arnoldi basis, but orthogonalize not only with
    respect to the vectors $\vv_1,\dots$, but also with respect to
    $\vu_1,\dots,\vu_k$.
  \end{enumerate}
  This concept can be repeated until the desired number of
  eigenvectors has been found. This may also involved removing
  eigenvectors of previous runs, if their eigenvalues are not wanted
  anymore.
\end{Definition}

\begin{intro}
  The size of the matrix $\matv_k$ of the Arnoldi algorithm is growing
  in each step. Thus, if the algorithm is converging slowly, memory
  and reorthogonalization become a limiting factor. Thus, we have to
  avoid growth of this basis. The two most obvious options have
  considerable drawbacks:
  \begin{enumerate}
  \item Restarting after $m$ steps by choosing a single column vector
    from $\matv_m$ or a single linear combination of them eliminates
    most information on the spectral structure gathered in these $m$
    steps. Indeed, we would have to make sure that the new initial
    vector has dominant components in the wanted eigenspaces and
    small components in the others.
  \item A windowing technique only considering the last $m$ vectors
    from $\matv_k$ might eliminate the desired eigenvectors, which
    then have to reenter in the following steps. Furthermore, the
    spectrum of $\matH_m$ is not related to the spectrum of $\mata$
    anymore, if the basis $\matv_m$ is not orthogonal. Hence, we
    do not consider this option.
  \end{enumerate}
\end{intro}

\begin{intro}
  In the previous chapter, we have seen the restart of GMRES in
  \slideref{Algorithm}{gmres-restart}. When this algorithm is applied
  here, we have to make sure that the initial vector for the restart
  is ``rich'' in components of the wanted eigenspaces, but has small
  components in the unwanted directions. In principle, we could use a
  technique like in the restart after a breakdown, but it comes at a
  high cost. Hence, we consider a different option.
\end{intro}

\begin{Definition}{polynomial-filtering}
  By \define{polynomial filtering} we denote a method for prerocessing
  the initial vector for an Arnoldi process by applying a matrix
  polynomial. Given a vector $\tilde\vv$, the initial vector is obtained by
  \begin{gather}
    \vw = p(\mata) \tilde \vv,\qquad \vv_1 = \frac{\vw}{\norm{\vw}}.
  \end{gather}
  The polynomial should be chosen such that its values are large on
  the wanted eigenvalues and small on the unwanted ones.
\end{Definition}

\begin{intro}
  We will now describe how we can use the Arnoldi factorization to
  implement polynomial filtering in a fairly cheap way. To this end,
  let us assume that the filter polynomial is given in factored form,
  namely
  \begin{gather}
    p(x) = (x-\sigma_p)\dots(x-\sigma_2)(x-\sigma_1).
  \end{gather}
  We apply the first factor not only to the initial vector $\vv_1$,
  but to the whole basis $\matv_m$. Hence, we obtain
  \begin{align}
    (\mata-\sigma_1\id_n)\matv_m
    &= \mata\matv_m - \sigma_1\matv_m\\
    &= \matv_m\matH_m - \sigma_1 \matv_m + h_{m+1,m} \vv_{m+1} \ve_m^*\\
    &= \matv_m (\matH_m-\sigma_1\id_m) + \vv_{m+1} \vw_m^*,
  \end{align}
  where we abbreviated $\vw_{m+1} =  h_{m+1,m} \ve_m$.
  Now, we apply a step of the shifted QR iteration in
  \slideref{Algorithm}{shifted-qr-iteration} to obtain
  \begin{gather}
    \matq_1\matr_1 = \matH_m-\sigma_1\id_m,
    \qquad
    \matH_m^{(1)} = \matr_1\matq_1+\sigma_1\id_m.
  \end{gather}
  Hence,
  \begin{gather}
    \label{eq:ev-arnoldi:1}
    (\mata-\sigma_1\id_n)\matv_m = \matv_m \matq_1\matr_1
    + \vv_{m+1} \vw_{m+1}^*.
  \end{gather}
  Multiplying with $\matq_1$ from the right yields
  \begin{gather}
    (\mata-\sigma_1\id_n)\matv_m\matq_1 = \matv_m \matq_1 \matr_1\matq_1
    + \vv_{m+1} \vw_{m+1}^* \matq_1.
  \end{gather}
  Finally, adding $\sigma_1\matv_m\matq_1$ from both sides, we obtain
  \begin{gather}
    \mata\matv_m\matq_1 = \matv_m \matq_1 \matH_m^{(1)}
    + \vv_{m+1} \vw_{m+1}^* \matq_1.
  \end{gather}
  Letting $\matv_m^{(1)} = \matv_m\matq_1$, which is still an orthogonal set since $\matq_1$ is an orthogonal matrix, and
  $\vw_{m+1}^{(1)} = \matq_1^*\vw_{m+1}$, we have
  \begin{gather}
    \label{eq:ev-arnoldi:2}
    \mata\matv_m^{(1)} = \matv_m^{(1)} \matH_m^{(1)}
    + \vv_{m+1} \bigl[\vw_{m+1}^{(1)}\bigr]^*.
  \end{gather}
  Multiplying~\eqref{eq:ev-arnoldi:1} with
  $\ve_1$, we see that
  \begin{gather}
    (\mata-\sigma_1\id) \vv_1 = \matv_m\matq_1 \matr_1\ve_1
    = r_{11} \matv_m\matq_1 \ve_1 = r_{11} \matv_m^{(1)} \ve_1
    = r_{11} \vv_1^{(1)},
  \end{gather}
  we see that $\vv_1^{(1)}$ is parallel to
  $(\mata-\sigma_1\id) \vv_1$, which means, it is obtained from
  $\vv_1$ by filtering with the polynomial $x-\sigma_1$.
  
  The transformation $\matH_m^{(1)} = \matq_1^*\matH_{m}\matq_1$ can
  be implemented by $m-1$ Givens rotations according to
  \slideref{Algorithm}{Hessenberg-qr-2}. Hence, the last two entries
  of $\vw_{m+1}^{(1)}$ are nonzero, which is the only difference
  between this representation and the Arnoldi factorization. This
  means on the other hand, that the first $m-1$
  columns~\eqref{eq:ev-arnoldi:2} are indeed an Arnoldi factorization
  starting with $\vv_1^{(1)}$.

  We can repeat the same process with
  $\matq_2\matr_2 = \matH_m^{(1)} - \sigma_2\id_m$ to obtain
  \begin{gather}
    \mata\matv_m^{(2)} = \matv_m^{(2)} \matH_m^{(2)}
    + \vv_{m+1} \bigl[\vw_{m+1}^{(2)}\bigr]^*,
  \end{gather}
  where $\matv_m^{(2)} = \matv_m^{(1)}\matq_2$,
  $\matH_m^{(2)} = \matq_2^*\matH_{m}^{(1)}\matq_2$, and
  $\vw_{m+1}^{(2)} = \matq_2^*\vw_{m+1}^{(2)}$.
  With the same arguments as before,
  \begin{gather}
    \vv_1^{(2)} \sim (\mata-\sigma_2\id)\vv_1^{(1)}
    \sim (\mata-\sigma_2\id) (\mata-\sigma_1\id)\vv_1.
  \end{gather}
  Hence, we have constructively proven the following lemma.
\end{intro}

\begin{Lemma}{filtering-restart}
  Let $\matv_m$ and $\matH_m$ be the result of $m$ steps of the
  Arnoldi method with initial vector $\vv_1$. Let $\matH_m^{(p)}$ be
  result of $p$ steps of the shifted QR iteration with shift
  parameters $\sigma_1,\dots,\sigma_p$ and resulting orthogonal
  transformations $\matq_1,\dots,\matq_p$ applied to $\matH_m$.

  Then, the first $m-p$ columns of
  $\matv_m^{(p)} = \matv_m\matq_1\dots\matq_p$ and of $\matH_m^{(p)}$
  correspond to the result of $m-p$ steps of the Arnoldi method with
  initial vector
  \begin{gather}
    \hat\vv_1 = (\mata-\sigma_p\id)\dots(\mata-\sigma_2\id)(\mata-\sigma_1\id)
    \vv_1.
  \end{gather}
\end{Lemma}

\begin{intro}
  The algorithm induced by the preceding lemma allows us to apply
  polynomial filtering to an already existing Arnoldi factorization in
  a fairly cheap manner. The result is a factorization of shorter
  length ``as if'' it had been started with the filtered initial
  vector. That's why the algorithm is called \define{implicit
    restart}.

  The question remaining is the choice of filter parameters
  $\sigma_i$. If these are good approximations to eigenvalues of
  $\mata$, then we expect the filtering to effectively suppress
  eigenspaces for these eigenvalues. The approximations themselves can
  also be obtained from the Arnoldi factorization by using the Ritz
  values.
\end{intro}

% \begin{Lemma}{arnoldi-truncation}
%   Let $m=k+p$ and let the result of $m$ Arnoldi steps be given by
%   \begin{gather}
%     \mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*.
%   \end{gather}
%   This factorization can be truncated by $p$ shifted QR steps to a
%   Arnoldi factorization
%     \begin{gather}
%     \tilde\mata\tilde\matv_k = \tilde\matv_k\tilde\matH_k + \tilde\vr_k\ve_k^*,
%   \end{gather}
%   where $\tilde\matv_k$ consists of the first $k$ columns of $\matv_m\matq$, $\tilde\matH_k$ is the upper left $k\times k$-block of $\matq^*\matH_m\matq$ and
%   \begin{gather}
%     \tilde\vr_k = \beta_k\matv_m\matq \ve_{k+1} + q_{mk} \vr_m ,
%   \end{gather}
%   and $\matq=\matq_1\dots\matq_p$ is the product of the factors in the
%   shifted QR steps.
% \end{Lemma}

% \begin{proof}
%   See~\cite[Section 4.5.1]{BaiDemmelDongarraRuhevanderVorst00}.
% \end{proof}

\begin{Algorithm*}{implicit-restart}{Implicit restart with locking and purging}
  Given the Arnoldi basis $\matv_m$ and the projected matrix $\matH_m$
  as well as the list of previously locked eigenpairs.
  \begin{enumerate}
  \item Compute the eigenvalues and eigenvectors.
  \item For each converged eigenvalue (\slideref{Lemma}{ev-arnoldi-a-posteriori}), compare to the locked eigenvalues.
  \begin{enumerate}
  \item If the newer one is preferred, \define{purge} the old one from the locked list and \define{lock} the new pair.
  \item If the newer one is not preferred, do not lock it, but remove
    it in the next step
  \end{enumerate}
\item Use all converged eigenvalues and additional unwanted
  eigenvalues of $\matH_m$, a total of $p$ for the implicit restart of
  \slideref{Lemma}{filtering-restart}. The result is an Arnoldi basis
  of length $k=m-p$.
\item Run the necessary $p=m-k$ Arnoldi steps to obtain a Krylov space of
  dimension $m$ again and repeat from step 1.
  \end{enumerate}
\end{Algorithm*}

\begin{Algorithm*}{iram}{Implicitly restarted Arnoldi method}
  Choose the maximal lenxth of the Arnoldi basis $m$, the length of
  the truncated basis $k$ and let $p=m-k$.
  \begin{algorithmic}[1]
    \Require Initial vector $\vv\in\C^n$ with $\norm{\vv}_2=1$.
    \State Compute Arnoldi factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Repeat
    \State Compute eigenvalues and eigenvectors of $\matH_m$
    \State Lock and purge eigenvectors of $\mata$.
    \State Select shifts $\sigma_1,\dots,\sigma_p$
    \For {$j=1,\dots, p$}
    \State $\matH_m \gets \matq_j^*\matH_m\matq_j$
    \Comment{$\matq_j\matr_j = \matH_m - \sigma_j\id$}
    \EndFor
    \State $\matq \gets \matq_1\dots\matq_p$
    \State $\matv_k = \bigl[\matv_m\matq\bigr](:,1:k)$
    \State $\matH_k = \matH_m(1:k,1:k)$
    \State $\vr_k= [\matH_m]_{k+1,k}\vv_{k+1} + q_{mk}\vr_m$
    \State Continue $p$ steps Arnoldi factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Until convergence
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  Like with the Francis QR iteration, the goal of the implicitly
  restarted Arnoldi method is not so much global convergence of the
  matrix $\matH_m$ to an upper triangular matrix, but the consecutive
  locking of sufficiently many eigenvectors.
  Hence, the method has converged, if the desired number $M$ of wanted
  eigenpairs have been locked.

  The selection of wanted eigenpairs is due to a relative criterion in
  a given set of eigenvalues. Hence, once $M$ eigenpairs have been
  locked, we cannot be sure that we have all eigenvalues in the wanted
  set. As an example, if we want the 5 greatest eigenvalues, we may
  not have caught number 3, but have included number 6 instead. In
  order to address this problem, the algorithm is run even after
  enough eigenpairs have been locked and is terminated if no purging
  has occurred for 2--3 iterations.

  Mathematically, there cannot be a proof that we have obtained all
  eigenvalues fitting the criterion. But evidence shows that round-off
  errors make it very likely that the wanted eigenvalues are found.
\end{remark}

\begin{remark}
  Locking and purging of eigenpairs requires bookkeeping of two sets
  of vectors, the locked eigenvectors $\vu_1,\dots,\vu_\ell$, and the
  Arnoldi basis $\vv_1,\dots,\vv_n$, which is again split into the two
  sets of vectors to be kept after truncation and the remainder.

  Standard implementations use a single array of vectors and store the
  locked vectors in the beginning, then the Arnoldi basis. Hence, the
  matrix $\matH_k$ after truncation has the form
  \begin{gather}
    \matH_k =
    \begin{pmatrix}
      \tau_1\\&\ddots\\
      &&\tau_\ell\\
      &&& \tilde \matH_k
    \end{pmatrix},
  \end{gather}
  where the Hessenberg matrix $\tilde \matH_k$ is of dimension
  $k-\ell$.  Hence, $p+k = m-\ell$. In particular, $k$ in the
  algorithm must be chosen not less than the number of desired
  eigenpairs.

  Alternatively, the locked vectors can be stored separately. Then,
  the size of the truncated basis can be chosen independently
  according to considerations of memory consumption and convergence.
\end{remark}

\begin{remark}
  We have so far neglected two important cases. First, if the initial
  vector of the Arnoldi method does not have any component in the
  direction of a desired eigenvector, the method in exact arithmetic
  will not be able to produce this eigenvector. Here, roundoff errors
  come to our rescue. They will produce spurious components in
  direction of such an eigenvector and due to the amplification in
  each step, will become dominant.
  
  Second, if $\mata$ has multiple eigenvalues,
  \slideref{Algorithm}{iram} will only identify a single vector in its
  eigenspace. This is due to the fact that the algorithm cannot
  distinguish between vectors in the same eigenspace and operates on
  each of them equally. Nevertheless, once we lock such an
  eigenvector, it is treated differently and a possible second
  eigenvector for the same eigenvalue may come up in subsequent
  iterations due to roundoff errors.
\end{remark}

\begin{remark}
  \slideref{Algorithm}{iram} is formulated under the assumption, that
  the Arnoldi factorization in the last line does not suffer a
  breakdown or near breakdown. In the first case, we should apply
  locking and purging according to
  \slideref{Definition}{arnoldi-locking} right away. In the second
  case, we expect that the orthogonalization is not accurate and
  reorthogonalization will be necessary.
\end{remark}

\begin{Algorithm*}{arnoldi-reorthogonalization}{Arnoldi with reorthogonalization}
  \algtext*{EndIf}
  \begin{algorithmic}[1]
    \Require $\mata\in\Rnn$; $\quad\vv_1\in\R^n, \norm{\vv_1}_2 = 1$
    \For{$j=1,\dots,m$}
    \State $\vw_j \gets \mata\vv_j$
    \State $\vh_{j} \gets \matv_j^*\vw_j$
    \State $\vw_j \gets \vw_j - \matv_j\vh_{j}$
    \If{$\norm{\vw_j} < \eta \norm{\vh_j}$} \Comment{Reorthogonalization}
    \State $\vg = \matv_j^*\vw_j$
    \State $\vw_j \gets \vw_j - \matv_j\vg$
    \State $\vh \gets \vh + \vg$.
    \EndIf
    \If{$\norm{\vw_j} < \eta \norm{\vg}$}\Comment{Failure}
    \State Choose random $\vv_{j+1}$
    \State $h_{j+1,j} \gets 0$
    \Else
    \State $h_{j+1,j} \gets \norm{\vw_j}_2$
    \State $\vv_{j+1} = \nicefrac{\vw_j}{h_{j+1,j}}$
    \EndIf
    \EndFor
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  In exact arithmetic, $\matv_j\vh_j$ and $\vw_j$ are the projections
  of $\mata\vv_j$ onto the range of $\matv_j$ and its orthogonal
  complement, respectively. Thus, if $\theta$ is the angle between
  $\mata\vv_j$ and the range of $\matv_j$, we have
  \begin{gather}
    \cot\theta = \frac{\norm{\vw_j}}{\norm{\vh_j}}.
  \end{gather}
  Thus, the condition in line 5 of the algorithm enforces a
  reorthogonalization, if the angle $\theta$ is too small.

  \slideref{Algorithm}{arnoldi-reorthogonalization} is taken from the
  literature. After a student had implemented it, I have had several
  discussions and it is not clear whether the criterion for breakdown
  is applied in the right way.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
