\begin{Lemma}{ev-arnoldi-a-posteriori}
  Let $(\lambda_i^{(m)},\vy_i^{(m)})$ be an eigenpair of the projected
  matrix $\matH_m$ of Arnoldi's method. Then, the Ritz vector
  $\vu_i^{(m)} = \matv_m \vy_i^{(m)}$ has a residual represented as
  \begin{gather}
    \left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}
    = h_{m+1,m}\ve_m^*\vy_i^{(m)}\vv_{m+1}.
  \end{gather}
  In particular, its norm is
  \begin{gather}
    \norm*{\left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}}_2
    = h_{m+1,m} \abs{\ve_m^*\vy_i^{(m)}}.
  \end{gather}
\end{Lemma}

\begin{proof}
  See~\cite[Proposition 6.8]{Saad11}.
\end{proof}

\begin{Corollary}{ev-arnoldi-breakdown}
  If the Arnoldi algorithm breaks down, the generated Ritz vectors are
  eigenvectors.
\end{Corollary}

\begin{remark}
  Reorthogonalization will be necessary once the Krylov space
  approximates an invariant subspace, which in turn is a sign for
  convergence and thus desirable. Then, $\vr$ will be almost linearly
  dependent and its norm after orthogonalization will be small,
  indicating cancellation.

  Note that this differs from the conjugate gradient method, where the
  Lanczos algorithm was modified as to avoid cancellation. But there
  is a more fundamental difference between the two algorithms. When we
  solve a linear system, we only compute a single vector, which is
  represented by the current state of the iteration. For the
  eigenvalue problem, we must keep the whole set of vectors $\matv_j$
  and we must make sure they are orthogonal with sufficient accuracy
  to compute the eigenvector approximation.
\end{remark}

\begin{remark}
  Like the vector iteration, this algorithm will put a focus on large
  eigenvalues. Thus, we can also apply the shift and invert technology
  in order to approximate other eigenvalues. To this end, line 3 of
  the algorithm is replaced by
  \begin{algorithmic}
    \State Solve $(\mata-\mu\id)\vr = \vv_j$.
  \end{algorithmic}
\end{remark}

\begin{intro}
  The size of the matrix $\matv_k$ of the Lanczos algorithm is growing
  in each step. Thus, if the algorithm is converging slowly, memory
  and reorthogonalization become a limiting factor. Thus, we have to
  avoid growth of this basis. The two most obvious options have
  considerable drawbacks:
  \begin{enumerate}
  \item Restarting after $m$ steps by choosing a single column vector
    from $\matv_m$ or a single linear combination of them eliminates
    most information on the spectral structure gathered in these $m$
    steps.
  \item A windowing technique only considering the last $m$ vectors
    from $\matv_k$ might eliminate the desired eigenvectors, which
    then have to reenter in the following steps.
  \end{enumerate}
  To the rescue comes the so called implicit restart, which will be
  presented after some preliminary considerations.
\end{intro}

\begin{Definition}{lanczos-factorization}
  The result of $k$ steps of the Lanczos algorithm is the
  \define{Lanczos factorization}
  \begin{gather}
    \mata\matv_k = \matv_k\matH_k + \vr_k\ve_k^*,
  \end{gather}
  where $\matv_k\in\C^{n\times k}$ is the matrix of mutually
  orthogonal \define{Lanczos vectors} and $\matH_k\in\R^{k\times k}$
  is symmetric and tridiagonal. There holds
  \begin{gather}
    \matv_k^*\vr_k = 0.
  \end{gather}
\end{Definition}

\begin{Lemma}{lanczos-truncation}
  Let $m=k+p$ and let the result of $m$ Lanczos steps be given by
  \begin{gather}
    \mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*.
  \end{gather}
  This factorization can be truncated by $p$ shifted QR steps to a
  Lanczos factorization
    \begin{gather}
    \tilde\mata\tilde\matv_k = \tilde\matv_k\tilde\matH_k + \tilde\vr_k\ve_k^*,
  \end{gather}
  where $\tilde\matv_k$ consists of the first $k$ columns of $\matv_m\matq$, $\tilde\matH_k$ is the upper left $k\times k$-block of $\matq^*\matH_m\matq$ and
  \begin{gather}
    \tilde\vr_k = \beta_k\matv_m\matq \ve_{k+1} + q_{mk} \vr_m ,
  \end{gather}
  and $\matq=\matq_1\dots\matq_p$ is the product of the factors in the
  shifted QR steps.
\end{Lemma}

\begin{proof}
  See~\cite[Section 4.5.1]{BaiDemmelDongarraRuhevanderVorst00}.
\end{proof}

\begin{Algorithm*}{irlm}{Implicitly restarted Lanczos method}
  \begin{algorithmic}[1]
    \Require Initial vector $\vv\in\C^n$ with $\norm{\vv}_2=1$.
    \State Compute Lanczos factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Repeat
    \State Compute $\sigma(\matH_m)$
    \State Select shifts $\mu_1,\dots,\mu_p$
    \State $\matq = \id_m$
    \For {$j=1,\dots, p$}
    \State Factorize $\matq_j\matr_j = \matH_m - \mu_j\id$
    \State $\matH_m \gets \matq_j^*\matH_m\matq_j$
    \State $\matq \gets \matq\matq_j$
    \EndFor
    \State $\matv_k = \bigl[\matv_m\matq\bigr](:,1:k)$
    \State $\matH_k = \matH_m(1:k,1:k)$
    \State $\vr_k= [\matH_m]_{k+1,k}\vv_{k+1} + q_{mk}\vr_m$
    \State Continue $p$ steps Lanczos factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Until $T_k$ diagonal
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  In every step of the outer loop of \slideref{Algorithm}{irlm} we
  apply a polynomial $\psi_\nu$ of $\mata$ to the initial
  vector. Decomposing this vector with respect to eigenvectors of
  $\mata$, we see that each component gets multiplied with
  $\psi_\nu(\lambda_i)$. Thus, after scaling by the dominant
  eigenvector, these components get weighted with factors
  \begin{gather}
    \prod_\nu\frac{\psi_\nu(\lambda_i)}{\psi_\nu(\lambda_1)}.
  \end{gather}
  By the choice of the shifts, these polynomials ensure that the first
  $k$ desired eigenvalues become dominant, while the latter ones will
  be suppressed.
\end{remark}

\begin{remark}
  Once an eigenpair of $\mata$ has been identified to sufficient
  accuracy, we should keep it and continue the algorithm without
  changing this pair anymore. This technique is called
  \define{locking} of converged eigenvalues.

  Let $\tau$ be an eigenvalue of $\matH_k$ with corresponding
  \putindex{Ritz vector} $\vv_1$ such that
  $\norm{\mata\vv_1 - \tau\vv_1}$ sufficiently small. Then, we will
  consider $\tau$ converged and $\vv_1$ is a good approximation to an
  eigenvector. Thus, the algorithm can be continued without
  consideration of $\tau$ and $\vv_1$.

  Let $\matq$ be an orthogonal matrix such that
  \begin{gather}
    \tilde \matH_k = \matq^*
    \begin{pmatrix}
      \tau \\&\hat \matH_k
    \end{pmatrix} \matq.
  \end{gather}

  Similar to implicit restart, we can use this matrix $\matq$ to
  modify the basis $\matv_k$ to obtain a new set of Lanczos vectors
  such that $\vv_1$ is the first of them. We can now continue the
  implicitly restarted Lanczos method using the matrix $\hat\matH_k$
  and the vectors $\vv_2,\dots,\vv_k$. Just in the case of
  reorthogonalization, we will involve the vector $\vv_1$ as well.
\end{remark}

\begin{remark}
  We have so far neglected two important cases. First, if the initial
  vector of the Lanczos method does not have any component in the
  direction of a desired eigenvector, the method in exact arithmetic
  will not be able to produce this eigenvector. Here, roundoff errors
  come to our rescue. They will produce spurious components in
  direction of such an eigenvector and due to the amplification in
  each step, will become dominant.
  
  Second, if $\mata$ has multiple eigenvalues,
  \slideref{Algorithm}{irlm} will only identify a single vector in its
  eigenspace. This is due to the fact that the algorithm cannot
  distinguish between vectors in the same eigenspace and operates on
  each of them equally. Nevertheless, once we lock such an
  eigenvector, it is treated differently and a possible second
  eigenvector for the same eigenvalue may come up in subsequent
  iterations due to roundoff errors.
\end{remark}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
