\begin{Lemma}{ev-arnoldi-a-posteriori}
  Let $(\lambda_i^{(m)},\vy_i^{(m)})$ be an eigenpair of the projected
  matrix $\matH_m$ of Arnoldi's method. Then, the Ritz vector
  $\vu_i^{(m)} = \matv_m \vy_i^{(m)}$ has a residual represented as
  \begin{gather}
    \left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}
    = h_{m+1,m}\ve_m^*\vy_i^{(m)}\vv_{m+1}.
  \end{gather}
  In particular, its norm is
  \begin{gather}
    \norm*{\left(A-\lambda_i^{(m)}\identity\right)\vu_i^{(m)}}_2
    = h_{m+1,m} \abs{\ve_m^*\vy_i^{(m)}}.
  \end{gather}
\end{Lemma}

\begin{proof}
  See~\cite[Proposition 6.8]{Saad11}.
\end{proof}

\begin{Corollary}{ev-arnoldi-breakdown}
  If the Arnoldi algorithm breaks down, the generated Ritz vectors are
  eigenvectors.
\end{Corollary}

\begin{Algorithm*}{ev-lanczos}{Lanczos algorithm for eigenvalues}
  \begin{algorithmic}[1]
    \Require $\vr\in\C^n$ and $\beta_0=\norm{\vr}_2$
    \For {$j=1,\dots$ until convergence}
    \State $\vv_j \gets \frac{\vr}{\beta_{j-1}}$
    \State $\vr \gets \mata\vv_j$
    \State $\vr \gets \vr-\beta_{j-1}\vv_{j-1}$
    \State $\alpha_j = \scal(\vr,\vv_j)$
    \State $\vr \gets \vr-\alpha_j\vv_j$
    \State Reorthogonalization if necessary
    \State $\beta_j \gets \norm{\vr}_2$
    \EndFor
    \State Compute Ritz values $\matH_j = \maty\matd^{(j)}\maty^*$
    \State Compute Ritz vectors $\matx=\matv_j\maty$
  \end{algorithmic}
\end{Algorithm*}

\begin{remark}
  Reorthogonalization will be necessary once the Krylov space
  approximates an invariant subspace, which in turn is a sign for
  convergence and thus desirable. Then, $\vr$ will be almost linearly
  dependent and its norm after orthogonalization will be small,
  indicating cancellation.

  Note that this differs from the conjugate gradient method, where the
  Lanczos algorithm was modified as to avoid cancellation. But there
  is a more fundamental difference between the two algorithms. When we
  solve a linear system, we only compute a single vector, which is
  represented by the current state of the iteration. For the
  eigenvalue problem, we must keep the whole set of vectors $\matv_j$
  and we must make sure they are orthogonal with sufficient accuracy
  to compute the eigenvector approximation.
\end{remark}

\begin{remark}
  Like the vector iteration, this algorithm will put a focus on large
  eigenvalues. Thus, we can also apply the shift and invert technology
  in order to approximate other eigenvalues. To this end, line 3 of
  the algorithm is replaced by
  \begin{algorithmic}
    \State Solve $(\mata-\mu\id)\vr = \vv_j$.
  \end{algorithmic}
\end{remark}

\begin{intro}
  The size of the matrix $\matv_k$ of the Lanczos algorithm is growing
  in each step. Thus, if the algorithm is converging slowly, memory
  and reorthogonalization become a limiting factor. Thus, we have to
  avoid growth of this basis. The two most obvious options have
  considerable drawbacks:
  \begin{enumerate}
  \item Restarting after $m$ steps by choosing a single column vector
    from $\matv_m$ or a single linear combination of them eliminates
    most information on the spectral structure gathered in these $m$
    steps.
  \item A windowing technique only considering the last $m$ vectors
    from $\matv_k$ might eliminate the desired eigenvectors, which
    then have to reenter in the following steps.
  \end{enumerate}
  To the rescue comes the so called implicit restart, which will be
  presented after some preliminary considerations.
\end{intro}

\begin{Definition}{lanczos-factorization}
  The result of $k$ steps of the Lanczos algorithm is the
  \define{Lanczos factorization}
  \begin{gather}
    \mata\matv_k = \matv_k\matH_k + \vr_k\ve_k^*,
  \end{gather}
  where $\matv_k\in\C^{n\times k}$ is the matrix of mutually
  orthogonal \define{Lanczos vectors} and $\matH_k\in\R^{k\times k}$
  is symmetric and tridiagonal. There holds
  \begin{gather}
    \matv_k^*\vr_k = 0.
  \end{gather}
\end{Definition}

\begin{Lemma}{lanczos-truncation}
  Let $m=k+p$ and let the result of $m$ Lanczos steps be given by
  \begin{gather}
    \mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*.
  \end{gather}
  This factorization can be truncated by $p$ shifted QR steps to a
  Lanczos factorization
    \begin{gather}
    \tilde\mata\tilde\matv_k = \tilde\matv_k\tilde\matH_k + \tilde\vr_k\ve_k^*,
  \end{gather}
  where $\tilde\matv_k$ consists of the first $k$ columns of $\matv_m\matq$, $\tilde\matH_k$ is the upper left $k\times k$-block of $\matq^*\matH_m\matq$ and
  \begin{gather}
    \tilde\vr_k = \beta_k\matv_m\matq \ve_{k+1} + q_{mk} \vr_m ,
  \end{gather}
  and $\matq=\matq_1\dots\matq_p$ is the product of the factors in the
  shifted QR steps.
\end{Lemma}

\begin{proof}
  See~\cite[Section 4.5.1]{BaiDemmelDongarraRuhevanderVorst00}.
\end{proof}

\begin{Algorithm*}{irlm}{Implicitly restarted Lanczos method}
  \begin{algorithmic}[1]
    \Require Initial vector $\vv\in\C^n$ with $\norm{\vv}_2=1$.
    \State Compute Lanczos factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Repeat
    \State Compute $\sigma(\matH_m)$
    \State Select shifts $\mu_1,\dots,\mu_p$
    \State $\matq = \id_m$
    \For {$j=1,\dots, p$}
    \State Factorize $\matq_j\matr_j = \matH_m - \mu_j\id$
    \State $\matH_m \gets \matq_j^*\matH_m\matq_j$
    \State $\matq \gets \matq\matq_j$
    \EndFor
    \State $\matv_k = \bigl[\matv_m\matq\bigr](:,1:k)$
    \State $\matH_k = \matH_m(1:k,1:k)$
    \State $\vr_k= [\matH_m]_{k+1,k}\vv_{k+1} + q_{mk}\vr_m$
    \State Continue $p$ steps Lanczos factorization
    $\mata\matv_m = \matv_m\matH_m + \vr_m\ve_m^*$
    \Until $T_k$ diagonal
  \end{algorithmic}
\end{Algorithm*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
