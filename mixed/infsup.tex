
\section{Finite-dimensional problems}
\begin{intro}
  So far, our power horse for well-posedness was the Lax-Milgram
  lemma, which can be applied under the conditions
  \begin{xalignat}2
    a(u,v) &\le M \norm{u}\norm{v} & \forall u,v&\in V\\
    \label{eq:infsup:elliptic}
    a(u,u) &\ge \ellipa \norm{u}^2 & \forall u&\in V.
  \end{xalignat}
  The second condition can also be rewritten in terms of the
  \putindex{Rayleigh quotient} as
  \begin{gather*}
    0 < \ellipa = \inf_{u\in V}\frac{a(u,u)}{\norm{u}^2}.
  \end{gather*}
  Restricting this to a finite dimensional space, the notation usually
  changes from
  \begin{gather}
    a(u,v) = f(v)
    \qquad\text{to}\qquad
    v^TA u = v^T f,
  \end{gather}
  where $A\in \R^{n\times n}$ is the matrix associated with the
  bilinear form. The bound for the Rayleigh quotient means nothing but
  that the real parts of all eigenvalues of $A$ are bounded from below
  by $\ellipa$. Thus, a matrix $A$ for which we can apply the
  Lax-Milgram lemma is positive definite. And the statement of the
  lemma in finite dimension is, that a positive definite matrix is
  invertible. We know from linear algebra that this is true, but we
  also know that the condition is all but necessary.
\end{intro}

\begin{intro}
  Why did we replace this clear theorem by the weaker Lax-Milgram
  lemma, when we studied elliptic partial differential equations?  For
  the first condition, it should be noted that spectral properties of
  operators between spaces of infinite dimension are much harder to
  obtain. Further, we do not need information on the whole spectrum,
  but only on the eigenvalue closest to zero. Therefore, we used a
  simple estimate in order to avoid discussing the spectrum at
  all. But, there is an important difference between
  Theorem~\ref{Theorem:la-invertible} and the
  estimate~\eqref{eq:infsup:elliptic}: the assumption of the theorem
  is qualitative, $\lambda \neq 0$, while the assumption of Lax-Milgram
  is quantitative,
  \begin{gather*}
    \Re\lambda \ge \ellipa> 0.
  \end{gather*}
  The following problem shows why such a change is necessary.
\end{intro}

\begin{Problem}{unbounded-inverse}
  On the space $\ell_2(\R)$ define the operator $A$ by its eigenvalue
  decomposition
  \begin{align*}
    A: \ell_2(\R) &\to \ell_2(\R)\\
    e_k & \mapsto \tfrac1k e_k.
  \end{align*}
  Here, $\{e_k\}$ is the orthogonal basis of unit vectors of the form
  \begin{gather*}
    \arraycolsep0.1em
    \begin{array}{cccccccc}
      e_k =(0&,\ldots,&0&,&1&,&0&,\ldots)^T.\\
      &&&&\uparrow\\
      &&&&k
    \end{array}
  \end{gather*}
  \begin{enumerate}
  \item Show that this operator does not have a bounded inverse, albeit
    its eigenvalues are positive.
  \item Show that the range of $A$ is not closed in $\ell_2(\R)$
  \end{enumerate}
\begin{solution}
  \begin{enumerate}
  \item For each $e_k$, the inverse is $A^{-1} e_k = k e_k$. In particular, $A$ is injective.
    On the other hand, it holds
    \begin{gather*}
      \lim_{k\to\infty}\frac{\norm{A^{-1}e_k}}{\norm{e_k}}=\lim_{k\to\infty}k=\infty
    \end{gather*}
      and the inverse cannot be bounded.
  \item We have to construct a convergent sequence in the range of $A$
    such that the pre-image of the sequence does not converge.
    \begin{enumerate}
    \item Choose
      \begin{gather*}
        v_n = \sum_{k=1}^n \frac1k e_k.
      \end{gather*}
      \item $v_n$ is a Cauchy-sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n \frac1k e_k}^2
          = \sum_{k=m}^n \frac1{k^2}\norm*{e_k}^2
          \le \frac1{m^2} \sum_{k=1}^\infty \frac1{k^2}
          = \frac{\pi^2}{6} \frac1{m^2}.
        \end{gather*}
      \item We conclude that $v=\lim_{n\to\infty}v_n$ exists in the
        closure of the range of $A$.
      \item There holds
        \begin{gather*}
          v_n = A \sum_{k=1}^n e_k =: A u_n.
        \end{gather*}
      \item Due to the injectivity of $A$ for $v$ to be in the range of $A$,
            $u_n$ has to converge in $\ell_2(\R)$.
      \item The sequence $u_n$ is not a Cauchy sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n e_k}^2
          = \sum_{k=m}^n \norm*{e_k}^2
          = n-m.
        \end{gather*}
    \end{enumerate}
  \end{enumerate}
\end{solution}
\end{Problem}

\begin{Problem}{lax-milgram-not-applicable}
  Find an invertible, symmetric matrix $A\in \R^{2\times 2}$ and a
  vector $v\in \R^2$ such that $v^T A v=0$ and thus the Lax-Milgram
  lemma is inconclusive.
\begin{solution}
  \begin{gather*}
    A =
    \begin{pmatrix}
      1 & 0 \\ 0 & -1
    \end{pmatrix}
  \end{gather*}
\end{solution}
\end{Problem}

The question of well-posedness in finite dimensions can be answered by:

\begin{Theorem}{la-invertible}
  A matrix $A\in\R^{n\times n}$ is invertible if and only if one of
  the following equivalent conditions holds:
  \begin{enumerate}
  \item all its (possibly complex) eigenvalues are nonzero,
  \item all its singular values are nonzero,
  \item for each nonzero $v\in\R^n$ holds $Av\neq 0$.
  \end{enumerate}
\end{Theorem}

\begin{intro}
  We focus on the second and third conditions, respectively, in
  Theorem~\ref{Theorem:la-invertible}.
  But, the problem above tells us that we
  will run into trouble, if we do not quantify this. Therefore, we
  start our attempt by requiring:
  \begin{gather*}
    \norm{Au}^2 \ge \ellipa \norm{u}^2 \qquad\forall u\in V.
  \end{gather*}
  But while this is a condition we can easily write down for matrices
  and operators, it does not work that well for bilinear forms. Thus,
  we first look at the singular value decomposition.
\end{intro}

\input{svd}

\begin{Definition}{ker-range-rn}
  \index{ker}
  Let $A: V\to W$ be a linear operator. Then, we define
  the \define{kernel} and the \define{range} of $A$ as
  \begin{align*}
    \ker A &= \bigl\{ v\in V \big| Av = 0\bigr\} \\
    \range A &= \bigl\{ w\in W \big| \;\exists\,v\in V: Av=w\bigr\}.
  \end{align*}
\end{Definition}

\begin{Definition}{orthogonal1}
  Let $V\subset\R^n$ be a subspace. We define the \define{orthogonal
    complement} of $V$ as
  \begin{gather}
    \label{eq:infsup:4}
    \ortho{V} = \bigl\{w\in \R^n \big| \;\forall\,v\in V \scal(w,v) = 0 \bigr\}.
  \end{gather}
\end{Definition}

\begin{Lemma}{ker-coker-rn}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, there holds
  \begin{gather}
    \label{eq:infsup:5}
    \begin{split}
      \ker A &= \ortho{\range{A^T}}\\
      \range A &= \ortho{\ker{A^T}}\\
      \ker{A^T} &= \ortho{\range A}\\
      \range{A^T} &= \ortho{\ker{A}}
    \end{split}
  \end{gather}
\end{Lemma}

\begin{proof}
  % First, we note that
  % \begin{gather*}
  %   \R^n = \ker A \oplus \ortho{\ker A},
  %   \qquad
  %   \R^m = \ker{A^T} \oplus \ortho{\ker{A^T}}.
  % \end{gather*}
  Let $A=U\Sigma V^T$ be the singular value decomposition of $A$ and
  $r$ be the number of nonzero singular values. Then, the first $r$
  vectors of $U$ span the range of $A$ and the last $n-r$ vectors of
  $V$ span its kernel. Furthermore,
  \begin{gather}
    A^T = \bigl(U\Sigma V^T\bigr)^T = V \Sigma U^T.
  \end{gather}
  Therefore, the first $r$ vectors of V span the range of $A^T$ and
  the last $n-r$ vectors of $U$ span its kernel.
\end{proof}

\begin{Corollary}{ker-coker-iso}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, the
  restrictions $A\colon \ortho{\ker A} \to \range A$ and $A^T\colon
  \ortho{\ker{A^T}} \to \range{A^T}$ are isomorphisms.
\end{Corollary}

\begin{proof}
  We note that $\dim \range A = \dim \range{A^T}$. Thus, by
  \slideref{Lemma}{ker-coker-rn} the dimensions of domain and range of
  each of the restricted operators are equal, say $\dim \range A =
  r$. The singular value decomposition of the operators is
  \begin{gather*}
    A = U\Sigma V^T \qquad A^T = V\Sigma U^T,
  \end{gather*}
  where all matrices are in $\R^{r\times r}$ and
  \begin{gather*}
    \Sigma = \diag(\sigma_1,\dots,\sigma_r),
  \end{gather*}
  and all singular values are positive. Thus, $A$ and $A^T$ are invertible.
\end{proof}

\begin{Corollary}{svd-infsup}
  Let $r=\dim \ortho{\ker A}$. Then, for the smallest nonzero singular
  value there holds
  \begin{gather}
    \label{eq:infsup:6}
    \sigma_r
    = \inf_{v\in \ortho{\ker A}} \sup_{w\in \R^m} \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{w\in \ortho{\ker{A^T}}} \sup_{v\in \R^n} \frac{w^T A v}{\abs{v}\abs{w}}.
  \end{gather}
\end{Corollary}

\begin{proof}
  Since the Cauchy-Schwarz inequality turns into an equation if and
  only if the two vectors are coaligned, there holds for any $v\in \R^n$:
  \begin{gather*}
    \sup_{w\in\R^m}\frac{w^T A v}{\abs{w}} = \frac{v^T A^T A v}{\abs{Av}}.
  \end{gather*}
  Therefore,
  \begin{gather*}
    \inf_{v\in \ortho{\ker A}} \sup_{w\in \R^m}
    \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{v\in \ortho{\ker A}} \frac{\abs{Av}}{\abs{v}}.
  \end{gather*}
  Now, let $v = \sum \alpha_i v_i$ where $v_i$ are the columns of $V$
  in the SVD of $A$. Then,
  \begin{gather*}
    \abs{Av}^2 = \abs*{A\sum_{i=1}^r \alpha_i v_i}^2
    = \abs*{\sum_{i=1}^r \sigma_i \alpha_i u_i}^2
    = \sum_{i=1}^r \sigma_i^2 \alpha_i^2.
  \end{gather*}
  The quotient
  \begin{gather*}
    \frac{\sum_{i=1}^r \sigma_i^2 \alpha_i^2}{\sum_{i=1}^r \alpha_i^2}
  \end{gather*}
  clearly has its minimum if $\alpha_1 = \dots=\alpha_{r-1} = 0$.
\end{proof}

\begin{Definition}{infsup1}
  A bilinear form $a(\cdot,\cdot)$ on $V\times W$ is said to admit the
  \define{inf-sup condition} or is called \define{inf-sup stable}, if
  there holds
  \begin{gather}
    \label{eq:infsup:1}
    \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W}
    \ge \ellipa > 0.
  \end{gather}
\end{Definition}

\begin{remark}
  In this finite dimensional exposition, is clear that $V$ and $W$
  must have the same dimension, and thus $V=W=\R^n$. This will be
  different, when we consider infinite dimensional spaces and indeed
  consider different spaces $V$ and $W$.
\end{remark}

\begin{Lemma}{infsup2}
  The following statements are equivalent to the inf-sup
  condition~\eqref{eq:infsup:1}:
  \begin{gather}
    \label{eq:infsup:2}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \ellipa \norm{u}_V\norm{w}_W
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \norm{w}_W &\le&\norm{u}_V\\
      a(u,w) &\ge& \ellipa \norm{u}_V^2
    \end{array}
    \right.
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3a}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \ellipa \norm{w}_W &\le&\norm{u}_V\\
      Aw &=& u
    \end{array}
    \right.
  \end{gather}
\end{Lemma}

\begin{Problem}{inf-sup-equivalence}
  Prove Lemma 2.1.16.
\begin{solution}
  We have to prove the following statements are equivalent to the inf-sup condition:
  \begin{align}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \ellipa \norm{u}_V\norm{w}_W     \tag{1}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \norm{w}_W \le\norm{u}_V\\
      a(u,w) \ge \ellipa \norm{u}_V^2
    \end{cases}
   \end{aligned}
   \tag{2}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \ellipa \norm{w}_W \le \norm{u}_V\\
      Aw = u
      \end{cases}
  \end{aligned}
      \tag{3}
  \end{align}
  The inf-sup condition reads
  \begin{align}
   \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W} \ge \ellipa \tag{IS}
  \end{align}

  $(IS)\Rightarrow(3)$\\
  The inf-sup condition is equivalent to $A: ker(A)^\perp\to V^*$ being an isomorphism.
  By the Riesz representation theorem there exists for a given $u\in V$ a $w\in W$ such that
  $Aw=J u$ where $J$ is the Riesz map. Hence, it holds
  \begin{align*}
a(u,w)=\langle A w, u\rangle = \langle J u, u\rangle = \norm{u}_V^2.
  \end{align*}
 Due to $w\in ker(A)^\perp$, $A^{-1}$ is bounded and
 \begin{align*}
 \norm{w}_W=\norm{A^{-1}u}_V\leq \frac{1}{\ellipa}\norm{u}_V.
  \end{align*}

  $(3)\Rightarrow(2)$ \\
  Define $\tilde{w}=\ellipa w$. Then,
  \begin{align*}
a(u,\tilde{w})=\ellipa a(u,w) = \ellipa \norm{u}_V^2
  \end{align*}
  and
  \begin{align*}
\norm{\tilde{w}}_W=\ellipa \norm{w}_W\leq\norm{u}_V
  \end{align*}

  $(2)\Rightarrow(1)$ \\
  \begin{align*}
  a(u,w) \ge \ellipa \norm{u}_V^2 \ge \ellipa \norm{u}_V \norm{w}_W
  \end{align*}

  $(1)\Rightarrow(IS)$ \\
  \begin{align*}
  &\forall u\in V \exists w\in W: a(u,w)\ge \ellipa \norm{u}_V\norm{w}_W\\
  &\Rightarrow \forall u\in V: \sup_{w\in W} \frac{a(u,w)}{\norm{w}_W}\ge \ellipa \norm{u}_V\\
  &\Rightarrow \inf_{u\in V}\sup_{w\in W} \frac{a(u,w)}{\norm{w}_W\norm{u}_V}\ge \ellipa
  \end{align*}
\end{solution}
\end{Problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Infinite dimensional Hilbert spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we derived quantitative conditions to
  ensure the invertibility of a matrix $A$ or its restriction to its
  cokernel $\ortho{\ker A}$. The arguments there have a natural
  extension to infinite dimensional Hilbert spaces, which we will
  derive in this section. We already saw in
  \slideref{Problem}{unbounded-inverse} that we may run into trouble
  if the range of $A$ is not closed. On the other hand, it turns out
  that most notions of linear algebra related to orthogonality can be
  maintained in Hilbert spaces if closed subspaces are considered.
  We begin by citing the most important results.
\end{intro}

\begin{Definition}{polar-orthogonal}
  Let $W\subset V$ be a subspace of a Hilbert space $V$. We define its
  \define{polar space} $\polar{W}\subset V^*$ and its
  \define{orthogonal complement} $\ortho{W}\subset V$ by
  \begin{gather}
    \label{eq:infsup:7}
    \begin{split}
    \polar{W} &= \bigl\{f\in V^* \big| \scal(f,w)_{V^*\times V} = 0
    \quad\forall\,w\in W\bigr\},
    \\
    \ortho{W} &= \bigl\{v\in V \big| \scal(v,w)_{V} = 0
    \quad\forall\,w\in W\bigr\}.
    \end{split}
  \end{gather}
  For a subspace $U\subset V^*$, we define its polar space
  \begin{gather}
    \polar{U} = \bigl\{v\in V \big| \scal(u,v)_{V^*\times V} = 0
    \quad\forall\,u\in U\bigr\}
  \end{gather}
\end{Definition}

\begin{Lemma}{orthogonal-closed}
  The polar space $\polar{W}$ and the orthogonal complement $\ortho{W}$ of a
  subspace $W\subset V$ are both closed. So is the polar space $\polar{U}$
  of a subspace $U\subset V^*$.
\end{Lemma}

\begin{proof}
  Consider the mapping
  \begin{align*}
    \Phi_w\colon V^* &\to \R,\\
    v&\mapsto \scal(v,w)_{V^*\times V}.
  \end{align*}
  For any $w$, the kernel of $\Phi$ is closed as
  the pre-image of a closed set. $\polar{W}$ is closed since it is the
  intersection of these kernels for all $w\in W$.

  The inner product is continuous on $V\times V$. Therefore, the
  mapping
  \begin{align*}
    \phi_w\colon V &\to \R,\\
    v&\mapsto \scal(v,w),
  \end{align*}
  is continuous. The argument continues as above. Similar for $\polar{U}$.
\end{proof}

\begin{Theorem}{orthogonal-complement}
  Let $W$ be a subspace of a Hilbert space $V$ and $\ortho{W}$ its
  orthogonal complement. Then, $\ortho{W} = \ortho{\overline{W}}$. Further,
  $V = W \oplus \ortho{W}$ if and only if $W$ is closed.
\end{Theorem}

\begin{proof}
  Clearly, $\ortho{\overline{W}} \subset \ortho{W}$ since
  $W\subset\overline{W}$. Let now $u\in \ortho{W}$. Then, $\phi =
  \scal(u,\cdot)$ is a continuous linear functional on $V$. Therefore,
  if a sequence $w_n \subset W$ converges to $w\in \overline{W}$, we
  have
  \begin{gather*}
    \scal(u,w) = \lim_{n\to\infty} \scal(u,w_n) = 0.
  \end{gather*}
  Hence, $u\in \ortho{\overline{W}}$ and $\ortho{W} = \ortho{\overline{W}}$.

  Now, the ``only if'' follows by the fact, that if $W$ is not
  closed, there is an element $w\in \overline{W}$ but not in $W$ such that
  $\scal(w,u)=0$ for all $u\in \ortho{W}$. Thus, $w\not\in \ortho{W}$ and
  consequently $w\not\in \ortho{W} \oplus W$.

  Let now $W$ be closed. We show that there is a unique decomposition
  \begin{gather}
    \label{eq:infsup:8}
    v = w + u,\qquad w\in W, \;u\in \ortho{W},
  \end{gather}
  which is equivalent to $V = W \oplus \ortho{W}$. Uniqueness follows,
  since
  \begin{gather*}
    v = w_1+u_1 = w_2+u_2
  \end{gather*}
  implies that for any $y\in V$
  \begin{gather*}
    0 = \scal(w_1-w_2+u_1-u_2,y) = \scal(w_1-w_2,y) + \scal(u_1-u_2,y).
  \end{gather*}
  Choosing $y=u_1-u_2$ and $w_1-w_2$ in turns, we see that one of the
  inner products vanishes for orthogonality and the other implies that
  the difference is zero.

  If $v\in W$, we choose $w=v$ and $u=0$. For $v\not\in W$, we prove
  existence by considering that due to the closedness of $W$ there holds
  \begin{gather*}
    d=\inf_{w\in W} \norm{v-w} >0.
  \end{gather*}
  Let $w_n$ be a minimizing sequence. Using the parallelogram identity
  \begin{gather*}
    \norm{a+b}^2+\norm{a-b}^2 = 2\norm{a}^2+2\norm{b}^2,
  \end{gather*}
  we prove that $\{w_n\}$ is a Cauchy sequence by
  \begin{align*}
    \norm{w_m-w_n}^2 &= \norm{(v-w_n)-(v-w_m)}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-\norm{2v-w_m-w_n}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4\norm*{v-\frac{w_m+w_n}2}^2\\
    &\le 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4d^2,
  \end{align*}
  since $(w_m+w_n)/2\in W$ and $d$ is the infimum. Now we use the
  minimizing property to obtain
  \begin{gather*}
    \lim_{m,n\to\infty}\norm{w_m-w_n}^2 = 2d^2-2d^2 -4d^2=0.
  \end{gather*}
  By completeness of $V$, $w=\lim w_n$ exists and by the closedness of
  $W$, we have $w\in W$. Let $u=v-w$. By continuity of the norm, we
  have $\norm{u}=d$. It remains to show that $u\in \ortho{W}$. To this
  end, we introduce the variation $w+\epsilon \tilde w$ with $\tilde
  w\in W$ to obtain
  \begin{align*}
    d^2 &\le \norm{v-w-\epsilon \tilde w}^2\\
    &= \norm{u}^2-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{align*}
  implying for any $\epsilon>0$
  \begin{gather*}
    0\le-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{gather*}
  which requires $\scal(u,\tilde w) = 0$.
\end{proof}

\begin{Definition}{orthogonal-projection}
  Let $V$ be a Hilbert space and $W\subset V$ be a closed
  subspace. For a vector $v\in V$, let $v=w+u$ be the unique
  decomposition with $w\in W$ and $u\in \ortho{W}$. Then we call $w$ and
  $u$ the \define{orthogonal projection}s of $v$ into $W$ and $\ortho{W}$,
  respectively. We write
  \begin{gather*}
    \Pi_W = w, \qquad \Pi_{\ortho{W}} = u.
  \end{gather*}
\end{Definition}

\begin{Lemma}{polar-orthogonal-hilbert}
  Let $V$ be a Hilbert space and $W\subset V$ be a closed
  subspace. Then, the polar space $\polar{W}\subset V^*$ and the orthogonal
  space $\ortho{W}$ can be isometrically identified by \putindex{Riesz
    representation}.
\end{Lemma}

\begin{proof}
  For every $f$ in the
  dual of $\ortho{W}$, define $g\in V^*$ by
  \begin{gather*}
    \scal(g,v)_{V^*\times V} =
    \scal(f,\Pi_{\ortho{V}}v)_{(\ortho{V})^*\times \ortho{V}}.
  \end{gather*}
  Clearly, $g(v)=0$ for $v\in W$, therefore $g\in \polar{W}$.
\end{proof}

% \begin{Corollary}

% \end{Corollary}


\begin{Theorem}{closed-range}
  Let $V,W$ be Hilbert spaces and $A\colon V\to W$ a continuous linear
  operator. Then, the following statements are equivalent:
  \begin{gather}
    \label{eq:infsup:9}
    \begin{split}
      \range A &\text{ is closed in } W,\\
      \range{A^T} &\text{ is closed in } V^*,\\
      \range A &= \polar{\ker{A^T}},\\
      \range{A^T} &= \polar{\ker A}.
    \end{split}
  \end{gather}
\end{Theorem}

\begin{remark}
  This is the famous \emph{\putindex{closed range theorem}} by Banach.
  It actually holds under weaker assumptions, for instance $V,W$ only
  Banach spaces. The proof can be found for instance
  in~\cite[p.~205--209]{Yosida80}.
\end{remark}

\begin{Theorem}{open-mapping}
  Let $A\colon V\to W$ be continuous and surjective. Then, the image
  $A(U)\subset W$ of any open set $U\subset V$ is open.
\end{Theorem}

\begin{remark}
  This is the \emph{\putindex{open mapping theorem}} by Banach. The
  proof can be found for instance in~\cite[p.75--76]{Yosida80}.
\end{remark}

\begin{Lemma}{closed-infsup}
  Let $A\colon V\to W$ be continuous. Then, $\range A$ is closed in
  $W$ if and only if there exists $\ellipa>0$ such that
  \begin{gather}
    \label{eq:infsup:10}
    \forall w\in \range A\;
    \exists v\in V\quad
    Av = w
    \;\wedge\;
    \ellipa \norm{v}_V \le \norm{w}_W.
  \end{gather}
\end{Lemma}

\begin{proof}
  We first show that the inf-sup condition~\eqref{eq:infsup:10}
  implies $\range A$ closed. To this end, let $\{w_n\}$ be a Cauchy
  sequence in $\range A$ converging to a point $w\in W$. Thus, there
  is a sequence $\{v_n\}$ in $V$ such that $Av_n = w_n$ and
  $\ellipa \norm{v_n} \le \norm{w_n}$. Hence,
  \begin{gather*}
    \norm{v_m-v_n}_V \le \frac1\ellipa \norm{w_m-w_n}_W,
  \end{gather*}
  and $\{v_n\}$ is a Cauchy sequence in $V$. Therefore, $v_n\to v\in
  V$ and due to continuity of $A$ we obtain $Av=w$ and thus $w\in
  \range A$.

  Conversely, let $\range A$ be closed in $W$. Thus, it is a Banach
  space and the \putindex{open mapping theorem} applies to $A\colon
  V\to\range A$. We map the open unit ball $B_1(0)\subset V$ and
  obtain that $A(B_1(0))$ is open in $\range A$, implying that there
  is an open ball $B_\delta(0) \subset A(B_1(0))$. This is sufficient
  to construct $v$:

  Let $w\in\range A$. Then,
  \begin{gather*}
    \tilde w\frac\delta2 \frac{w}{\norm{w}} \in B_\delta(0) \subset A(B_1(0)).
  \end{gather*}
  Hence, there is $v\in V$ with $\norm{v}<1$ such that $Av=\tilde w$,
  which proves the lemma.
\end{proof}

\begin{Theorem}{infsup-well-equivalence}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W,
  \end{gather}
  and $A\colon V\to W^*$ its associated operator.
  Then, the following statements are equivalent:
  \begin{enumerate}
  \item There exists $\ellipa>0$ such that
    \begin{gather}
      \label{eq:infsup:11}
      \inf_{w\in W}\sup_{v\in V}
      \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
      \ge \ellipa.
    \end{gather}
  \item The operator $A^T\colon W\to \polar{\ker A}$ is an isomorphism and
    \begin{gather}
      \label{eq:infsup:12}
      \norm{A^T w}_{V^*} \ge \ellipa \norm{w}_{W} \qquad\forall w\in W.
    \end{gather}
  \item The operator $A\colon \ortho{\ker A}\to W^*$ is an isomorphism
    and
    \begin{gather}
      \label{eq:infsup:13}
      \norm{Av}_{W^*} \ge \ellipa\norm{v}_V\qquad \forall v\in \ortho{\ker A}.
    \end{gather}
  \end{enumerate}
\end{Theorem}

\begin{proof}
  First, we show the equivalence of the first two statements. Let us
  rephrase the inf-sup condition to
  \begin{gather*}
    \norm{A^T w}_{V^*}
    = \sup_{v\in V}\frac{\scal(A^T w,v)}{\norm{v}_V}
    = \sup_{v\in V}\frac{a(v,w)}{\norm{v}_V}
    \ge \ellipa\norm{w} \qquad
    \forall w\in W.
  \end{gather*}
  Thus, equations~\eqref{eq:infsup:11} and~\eqref{eq:infsup:12} are
  equivalent and we have already proven that the second statement
  implies the first. It remains to show the $A^T$ is an isomorphism
  from $W$ onto $\polar{\ker A}$. Equation~\eqref{eq:infsup:12} implies that
  $A^T\colon W \to \range{A^T}$ is an isomorphism and its inverse is
  bounded by $1/\ellipa$ (multiply both sides by $A^{-1}$). Using
  \slideref{Lemma}{closed-infsup}, we obtain that $\range{A^T}$ is
  closed in $V^*$ and the \putindex{closed range theorem} settles the
  issue.

  In order to prove equivalence of the second and third statement, we
  use \slideref{Lemma}{polar-orthogonal-hilbert} to isometrically
  identify $(\ortho{\ker A})^*$ with $\polar{\ker A}$. Thus, $A$ is an
  isomorphism from $\ortho{\ker A}$ onto $W^*$ if and only if $A^T$ is an
  isomorphism from $W$ onto $(\ortho{\ker A})^* = \polar{\ker A}$. and
  \begin{gather*}
    \norm{A}_{W^*\to \ortho{\ker A}} = \norm{A^T}_{\polar{\ker A}\to W}.
  \end{gather*}
\end{proof}

\begin{Corollary}{infsup-well-posedness1}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W.
  \end{gather}
  Let the inf-sup-condition
  \begin{gather*}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    \ge \ellipa > 0
  \end{gather*}
  hold.  Then, the problem finding $w\in W$ such that
  \begin{gather*}
    a(v,w) = f(v) \qquad\forall v\in V,
  \end{gather*}
  has a unique solution for $f\in \polar{\ker A}$ and
  \begin{gather}
    \norm{w}_W \le \frac1\ellipa \norm{f}_{V^*}.
  \end{gather}
\end{Corollary}

\begin{remark}
  \slideref{Corollary}{infsup-well-posedness1} exhibits an asymmetry
  between the left and right argument. In particular, we obtain a
  unique solution only for the adjoint operator $A^T$, which is
  exactly what we need, when we compute say a pressure from the
  divergence of a velocity field. In general, we consider the
  restriction of $f$ to the polar set of the kernel in the above
  well-posedness result detrimental and would prefer a result that
  holds for all $f\in V^*$. This on the other hand requires
  $\ker A=\{0\}$, or $\overline{\range{A^T}} = W^*$. Then, on the
  other hand, we see that $\range{A^T}$ is closed since $\range{A}$ is
  closed and the closed range theorem holds. Therefore, we obtain the
  following theorem for the case that we require a unique solution for
  all right hand sides.
\end{remark}

\begin{Theorem}{infsup-well-posedness2}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W.
  \end{gather}
  Let for some $\ellipa>0$ the inf-sup-conditions
  \begin{align*}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \ellipa,\\
    \inf_{v\in V}\sup_{w\in W}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \ellipa
  \end{align*}
  hold.  Then, the problem finding $v\in V$ such that
  \begin{gather*}
    a(v,w) = f(w) \qquad\forall w\in W,
  \end{gather*}
  has a unique solution for $f\in W^*$ and
  \begin{gather}
    \norm{v}_V \le \frac1\ellipa \norm{f}_{W^*}.
  \end{gather}
\end{Theorem}

\begin{remark}
  If we compare \slideref{Theorem}{infsup-well-posedness2} with
  \slideref{Corollary}{infsup-well-posedness1}, we see that the only
  difference lies in the fact that the second inf-sup condition
  ensures surjectivity of $A$ by injectivity of $A^T$. In some cases
  it may be difficult to prove both inf-sup conditions. Then, it is
  sufficient to prove one inf-sup condition, say the first, and then
  only
  \begin{gather*}
     \inf_{v\in V}\sup_{w\in W}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W} > 0,
  \end{gather*}
  thus, injectivity of $A^T$. Although we verify less than the
  assumptions of \slideref{Theorem}{infsup-well-posedness2}, the
  closed range theorem saves us from the additional work. We further
  note that this notion is symmetric between $A$ and $A^T$, that is,
  it is sufficient to prove inf-sup for either operator and
  injectivity for the other.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The inf-sup condition for mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we have developed a framework for
  well-posedness of problems which are not $V$-elliptic. In principle,
  this theory can be applied to the bilinear form
  $\mathcal A((u,p),(v,q))$ as a whole. On the other hand, we can
  formally split the solution of a constrained minimization problem
  into the reduced problem and then computing the Lagrange multiplier,
  which more clearly exhibits the relation of the two spaces $V$ and
  $Q$ involved in the mixed formulation. Here are the resulting
  theorems.
\end{intro}

\begin{Theorem}{infsup-mixed1}
  Let $V$ and $Q$ be Hilbert spaces and let the mixed bilinear form
  \begin{gather*}
        \mathcal A\mixedform(u,p,v,q)
      = a(u,v) + b(v,p) + b(u,q)
  \end{gather*}
  be defined and bounded for any $u,v\in V$ and $p,q\in Q$.
  Then, the problem
  \begin{gather*}
    \mathcal A\mixedform(u,p,v,q) = \scal(f,v)+\scal(g,q)
    \quad\forall v\in V, q\in Q,
  \end{gather*}
  has a unique solution for any $f\in V^*$ and any $g\in Q^*$ if and
  only if there exists $\ellipa>0$ such that
  \begin{gather*}
    \forall
    \begin{bmatrix}
      u\in V\\p\in Q
    \end{bmatrix}
    \;
    \exists
    \begin{bmatrix}
      v\in V\\q\in Q
    \end{bmatrix}
    \colon
    \quad
    \mathcal A\mixedform(u,p,v,q) \ge \ellipa
    \norm{(u,p)}_{V\times Q}\norm{(v,q)}_{V\times Q},
  \end{gather*}
  and vice versa.
\end{Theorem}

\begin{proof}
  Straight application of \slideref{Theorem}{infsup-well-posedness2}.
\end{proof}

\begin{Theorem}{infsup-mixed2}
  Let $V$ and $Q$ be Hilbert spaces and let
  \begin{gather}
    \begin{split}
      \ker B &= \bigl\{v\in V \big| b(v,q) = 0 \;\forall q\in Q\bigr\}.
    \end{split}
  \end{gather}
  Then, the problem finding $(u,p)\in V\times Q$ such that
  \begin{gather}
    a(u,v) + b(v,p) + b(u,q) = f(v) \quad\forall v\in V, q\in Q,
  \end{gather}
  is well-posed if and only if the problem finding $u\in \ker B$ such that
    \begin{gather}
      a(u,v) = f(v) \quad\forall v\in \ker B
    \end{gather}
    is well-posed for any $f\in V^*$ and there is a positive constant
    $\beta$ such that
    \begin{gather}
      \inf_{q\in Q}\sup_{v\in V} \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} \ge \beta.
    \end{gather}
\end{Theorem}

\begin{proof}
  By requiring well-posedness of the reduced problem, $u\in V$ is
  well-determined and bounded by the data $f\in V^*$ without knowledge
  of the Lagrange multiplier. Hence, with $u\in V$ given and
  $b(u,q) = 0$, the problem of determining the Lagrange multiplier $p$
  reduces to
  \begin{gather}
    b(v,p) = f(v) - a(u,v), \qquad\forall v\in V.
  \end{gather}
  Applying \slideref{Corollary}{infsup-well-posedness1} to the
  bilinear form $b(.,.)$, we deduce that this equation has a unique
  solution $p\in Q$ if and only if $f(v)-a(u,v) \in \ker B^0$, which
  is the reduced problem.
\end{proof}

\begin{remark}
  Since $V$ is a Hilbert space, the decomposition
  $V = \ker B \oplus \ker B^\perp$ is uniquely determined and there is
  a corresponding decomposition $V^* = \ker B^0+(\ker B^\perp)^0$,
  such that $f = f^0+f^\perp$ above. The way we solve the reduced
  problem first and then compute the Lagrange multiplier implies that
  the solution $u$ only depends on $f^\perp$, while the Lagrange
  multiplier $p$ only depends on $f^0$.
\end{remark}

\begin{remark}
  We have imposed well-posedness of the reduced problem only in an
  abstract way. Depending on $a(.,.)$ we can formulate two conditions:
  ellipticity on $\ker B$ or inf-sup stability on $\ker B$. Indeed,
  most problems considered in this class will have symmetric bilinear
  forms $a(.,.)$, such that ellipticity serves as our usual
  assumption.  In these cases, note that $V$-ellipticity already
  implies the well-posedness on $\ker B$.
\end{remark}

\begin{Problem}{inhomogeneous-continuity}
  Show that \slideref{Theorem}{infsup-mixed2} can be extended to the
  case with right hand side $f(v)+g(q)$ with $g\in Q^*$.

\begin{solution}
  We want to solve the problem
  \begin{align*}
    a(u,v) + b(v,p) + b(u,q) = f(v)+g(q) \quad\forall v\in V, q\in Q,
  \end{align*}
  where $b(v,p)$ fulfills a inf-sup condition.

  \begin{enumerate}
  \item Due to \slideref{Theorem}{infsup-well-equivalence}, the
    operator $B: V\to Q^*$ is surjective and thus, there exists
    $u_g\in V$ such that
    \begin{gather*}
      b(u_g,q) = q(q) \quad\forall q\in Q.
    \end{gather*}
  \item Now consider the function $u_0 = u-u_g$. For $u$ to solve the
    original problem $u_0$ has to solve
    \begin{align*}
      a(u_0+u_g,v) + b(v,p) + b(u_0+u_g,q) = f(v)+g(q) \quad\forall v\in V, q\in Q\\
      \Leftrightarrow a(u_0,v) + b(v,p) + b(u_0,q) = f(v)-a(u_g,v) \quad\forall v\in V, q\in Q
    \end{align*}
  \item Due to $a(u_g,v) \leq \norm{a}\norm{u_g}_V\norm{v}_V$,
    the right-hand side $f(\cdot)-a(u_g,\cdot)$ is in $V^*$
    and we are in the setting of
    \slideref{Theorem}{infsup-mixed2}.
  \end{enumerate}
\end{solution}
\end{Problem}

\begin{intro}
  We summarize the result of this section in an assumption for
  well-posedness which will be the basis for further results in this
  course. We know from the discussion above that this assumption is
  only sufficient and weaker conditions may be imposed on
  $a(.,.)$. But indeed, it helps us through a lot of problems and is a
  good compromise between generality and ease of use.
\end{intro}

\begin{Assumption}{mixed-elliptic}
  Let $V$ and $Q$ be Hilbert spaces and let $a(.,.)$ and $b(.,.)$ be
  bounded bilinear forms on $V\times V$ and $V\times Q$,
  respectively. We define their norms as the smallest constants such
  that for all arguments there holds
  \begin{gather}
    a(u,v) \le \norm a \norm{u}_V \norm{v}_V,
    \qquad
    b(v,q) \le \norm b \norm{v}_V \norm{q}_Q.
  \end{gather}
  With these forms, we associate bounded operators $A$, $B$, and $B^T$
  according to \slideref{Definition}{saddle-point-operators}. With
  $b(.,.)$ we associate the spaces
  \begin{gather}
    \begin{split}
      \ker B &= \bigl\{v\in V \big| b(v,q) = 0 \;\forall q\in Q\bigr\},\\
      \ker B^T &= \bigl\{q\in Q \big| b(v,q) = 0 \;\forall v\in V\bigr\}.
    \end{split}
  \end{gather}
  Furthermore, we assume that $a(.,.)$ is positive semi-definite on
  $V$ and elliptic on $\ker B$,
  \begin{gather}
    a(u,u) \ge 0 \quad\forall u\in V,
    \qquad
    a(u,u) \ge \ellipa \norm{u}_V^2 \quad\forall u\in \ker B.
  \end{gather}
\end{Assumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Galerkin approximation of mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{../mixed/galerkin}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bringing back $c(p,q)$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  The key to the mixed analysis which is also underlying our
  quasi-best\-ap\-proxi\-mation result was a splitting of the solution
  process into the reduced problem for $u$ and then applying the
  inf-sup condition for $b(.,.)$ in order to estimate $p$. This way,
  we will be able to obtain estimates for the Stokes problem, but we
  have tacitly abandoned weakly compressible elasticity. Indeed, the
  mixed form of the Lamé-Navier equations is not a constrained
  minimization problem. In this section, we will fill the gap and
  derive estimates for the solution of this problem which are robust
  in $\lambda$.

  In the Lamé-Navier equations, we had
  \begin{gather*}
    c(p,q) = -\tfrac1\lambda \scal(q,p)_{L^2(\domain)},
  \end{gather*}
  which suggests assuming symmetric and $Q$-elliptic. But, we want
  estimates independent of $\lambda$! Therefore, we should only
  require semi-definite, which on the other hand turns out a bit too
  weak.
\end{intro}

\begin{Assumption}{mixed-elliptic-stabilized}
  In addition to \slideref{Assumption}{mixed-elliptic}, let $c(.,.)$
  be positive semi-definite and elliptic on $\ker{B^T}$,
  \begin{gather}
    c(q,q) \ge u\quad\forall q\in Q,
    \qquad
    c(q,q) \ge \ellipc \norm{q}_Q^2 \quad\forall q\in \ker{B^T}.
  \end{gather}
\end{Assumption}

\begin{remark}
  Again, this assumption is not necessary for the analysis, but it
  yields a convenient and useful theorem which goes far beyond weakly
  compressible elasticity and covers stabilized methods for spaces
  where the inf-sup condition for $b(.,.)$ does not hold for the whole
  space $Q$.
\end{remark}

\begin{Theorem}{mixed-stabilized-well-posed}
  Let \slideref{Assumption}{mixed-elliptic-stabilized} hold and
  let $a(.,.)$ and $c(.,)$ be symmetric. In addition, let there be
  $\beta>0$ such that
  \begin{gather}
    \begin{split}
      \inf_{q\in \ortho{\ker{B^T}}}  \sup_{v\in V}
      \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} &\ge \beta\\
      \inf_{v\in \ortho{\ker{B}}}  \sup_{q\in Q}
      \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} &\ge \beta
    \end{split}
  \end{gather}
  Then, the problem finding $(u,p)\in V\times Q$ such that
  \begin{multline}
    a(u,v) + b(v,p) + b(u,q) - c(p,q) = f(v)+g(q)
    \\
    \forall v\in V, q\in Q
  \end{multline}
  has a unique solution for all $f\in V^*$ and $g\in Q^*$ and there is
  a constant $C$ such that
  \begin{gather}
    \norm{u}_V+\norm{p}_Q
    \le C \bigl(\norm{f}_{V^*} + \norm{g}_{Q^*}\bigr).
  \end{gather}
\end{Theorem}

\begin{proof}
  First, note that the ellipticity assumptions as well as the inf-sup
  conditions are symmetric in $V$ and $Q$. Indeed, replacing the test
  functions and the form $b(.,.)$ by their negatives, we can transform
  the problem into one where $V$ and $Q$ have exchanged their
  roles. Thus, it is sufficient to show well-posedness for $f=0$. The
  same result then holds for $g=0$ and it holds for both nonzero by
  linearity.
  
  We note that by the inf-sup conditions both $\range B$ and
  $\range{B^T}$ are closed. Thus, we can decompose $u=u^0+u^\perp$
  with $u^0\in\ker B$ and $u^\perp$ in its orthogonal
  complement. Assuming $f=0$ and testing with $q=0$ we obtain the
  equation
  \begin{gather}
    \label{eq:infsup:14}
    a(u,v) = -b(v,p) = 0.
  \end{gather}
  In particular, testing with $v=u^0$ yields
  \begin{gather*}
    a(u,u^0) = -b(u^0,p) = 0.
  \end{gather*}
  Hence,
  \begin{gather*}
    \ellipa \norm{u^0}^2_V \le a(u^0,u^0)
    = -a(u^\perp,u^0)
    \le \norm a \norm{u^\perp}\norm{u^0},
  \end{gather*}
  which implies
  \begin{gather}
    \label{eq:infsup:15}
    \norm{u^0}_V \le \frac{\norm a}{\ellipa} \norm{u^\perp}_V.
  \end{gather}
  Testing with $v=u$ and $q=-p$ yields
  \begin{gather*}
    a(u,u)+c(p,p) \le g(p) = g^0(p^0) + g^\perp(p^\perp),
  \end{gather*}
  where $p^0\in \ker{B^T}$, $g^\perp\in \polar{\ker{B^T}}$ and the
  other two are defined by orthogonality in $Q$ and $Q^*$,
  respectively. Let first $g^0=0$. Then, by~\eqref{eq:infsup:14} and
  the inf-sup condition for $p$, there is $v\in V$ with $\norm{v}_V = 1$
  such that
  \begin{gather*}
    \beta \norm{p^\perp} \le
    \abs{b(v,p^\perp)} = \abs{b(v,p)} = \abs{a(u,v)}
    \le \sqrt{a(u,u)}\sqrt{a(v,v)},
  \end{gather*}
  by the Bunyakovsky-Cauchy-Schwarz inequality for symmetric bilinear
  forms.  Therefore, squaring and using the definition of the operator
  norm of $g^\perp$ yields
  \begin{gather*}
    \norm{p^\perp}_Q \le \frac{\norm a}{\beta^2}\norm{g^\perp}_{Q^*}.
  \end{gather*}
  Furthermore, we have
  \begin{gather*}
    c(p,p^0) = b(u,p^0) - g^\perp(p^0) = 0.
  \end{gather*}
  Hence,
  \begin{gather*}
    \ellipc \norm{p^0}_Q^2 \le c(p^0,p^0)
    = c(p^\perp,p^0) \le \norm c \norm{p^0}_Q \norm{p^\perp}_Q,
  \end{gather*}
  concluding
  \begin{gather*}
    \norm{p^0}_Q
    \le \frac{\norm a \norm c}{\ellipc\beta^2}
    \norm{g^\perp}_{Q^*}.
  \end{gather*}

  We continue our proof for $g^\perp=0$ and $g^0 \neq 0$. Testing with
  $q=p^0$, we obtain
  \begin{align*}
    c(p^0,p^0)
    &= c(p,p^0) - c(p^\perp,p^0) \\
    &= b(u,p^0) - g^0(p^0) - c(p^\perp,p^0) \\
    &\le \norm{g^0}_{Q^*} \norm{p^0}_Q
      + \norm c \norm{p^\perp}_Q \norm{p^0}_Q,
  \end{align*}
  yielding
  \begin{gather*}
    \norm{p^0}_Q \le \frac1{\ellipc}
    \left(\norm{g^0}_{Q^*} + \norm c \norm{p^\perp}_Q\right).
  \end{gather*}
  $p^\perp$ is estimated as before by
  \begin{align*}
    \norm{p^\perp}_Q^2
    &\le \frac{\norm a}{\beta^2}\norm{g^0}_{Q^*} \norm{p^0}_Q \\
    &\le \frac{\norm a}{\ellipc\beta^2} \norm{g^0}_{Q^*}^2
      + \frac{\norm a\norm c}{\ellipc\beta^2}
      \norm{g^0}_{Q^*}\norm{p^\perp}_Q\\
    &\le \frac{\norm a}{\ellipc\beta^2} \norm{g^0}_{Q^*}^2
      + \frac12 \norm{p^\perp}_Q^2
      + \frac{\norm a^2 \norm c^2}{2{\ellipc}^2\beta^4} \norm{g^0}_{Q^*}^2.
  \end{align*}
  We conclude that $p^\perp$ and $p^0$ are bounded by $g^0$. Summing
  up, we obtain for $f=0$ and $g\in Q^*$ the estimate
  \begin{gather*}
    \norm{p}_Q \le c \norm{g}_{Q^*}.
  \end{gather*}

  We estimate $u^0$ by $u^\perp$ using~\eqref{eq:infsup:15} and
  $u^\perp$ by the inf-sup condition, choosing $q\in Q$ with
  $\norm{q}_Q = 1$ such that
  \begin{gather*}
    \beta\norm{u^\perp} = b(u,q) = c(p,q) + g(q)
    \le \norm{c}\norm{p}_Q +  \norm{g}_{Q^*}.
  \end{gather*}
  Thus, we have estimated all components of the solution by the norm
  of $g$, assuming $f=0$. Now we conclude the proof by reverting the
  roles of $u$ and $p$, respectively $f$ and $g$.
\end{proof}

\begin{intro}
  The extension of \slideref{Theorem}{galerkin-mixed-p} to the
  saddle-point problem of \slideref{Definition}{saddle-point-abstract}
  with bilinear form $c(.,.)$ is even more cumbersome than this
  theorem. Nevertheless, the use of residual operators as a technique
  to structure the proof of convergence is instructive and may come
  handy at some point.
\end{intro}

\begin{Definition}{mixed-residual}
  For the saddle-point problem
  \begin{gather*}
    a(u,v) + b(v,p) + b(u,q) - c(p,q),
  \end{gather*}
  and functions $w_h\in V_h$ and $r_h\in Q_h$ we introduce the the
  residual operators $R_f \in V_h^*$ and $R_g\in Q_h^*$ as
  \begin{gather*}
    \begin{split}
      R_f(v_h) &= a(u-w_h, v_h) + b(v_h, p-r_h) \\
      R_g(q_h) &= b(u-w_h, q_h) - c(p-r_h, q_h).
    \end{split}
  \end{gather*}
\end{Definition}

\begin{Corollary}{mixed-residual-bounded}
  Under \slideref{Assumption}{mixed-elliptic-stabilized}, we have
  \begin{gather}
    \begin{split}
      \abs{R_f(v_h)}
      &\le \bigl(\norm a \norm{u-w_h}_V + \norm b \norm{p-r_h}\bigr)
      \norm{v_h}_V
      \\
      \abs{R_g(q_h)}
      &\le \bigl(\norm b \norm{u-w_h}_V + \norm c \norm{p-r_h}\bigr)
      \norm{q_h}_Q.
    \end{split}
  \end{gather}
\end{Corollary}

\begin{Lemma}{stabilized-mixed-approximation}
  Let the assumptions of
  \slideref{Theorem}{mixed-stabilized-well-posed} hold. Then, there
  are constants $c_1$ to $c_4$ independent of the solutions $u$, $p$,
  $u_h$, and $p_h$ and the discretization parameter $h$, such that for
  any $v_h\in V_h$ and $q_h\in Q_h$
  \begin{gather}
    \label{eq:infsup:16}
    \begin{split}
      \norm{u_h-v_h} &\le c_1 \norm{R_f}_{V_h^*} + c_2
      \norm{R_g}_{Q_h^*}
      \\
      \norm{p_h-q_h} &\le c_3 \norm{R_f}_{V_h^*} + c_4
      \norm{R_g}_{Q_h^*}.
    \end{split}
  \end{gather}
\end{Lemma}

\begin{proof}
  The proof is lengthy and follows the lines of the proof of
  well-posedness for
  \slideref{Theorem}{mixed-stabilized-well-posed}. It is obtained by
  considering the components $u_h^0-v_h^0$ and $u_h^\perp-v_h^\perp$
  as well as $p_h^0-q_h^0$ and $p_h^\perp-q_h^\perp$ separately.
\end{proof}

\begin{intro}
  In spite of the bad treatment the proof of the previous lemma
  received in these notes, it contains the main parts of the
  convergence proof, and whenever a saddle-point problem including
  $c(.,.)$ is solved, it has to be reproduced. It is just the fact
  that the proof is overwhelmingly technical that led to the decision
  to leave this experience to the first time the reader actually needs
  this result.
\end{intro}

\begin{Corollary}{stabilized-mixed-convergence}
    Let the assumptions of
  \slideref{Theorem}{mixed-stabilized-well-posed} hold. Then, there
  are constants $c_1$ to $c_4$ independent of the solutions $u$, $p$,
  $u_h$, and $p_h$ and the discretization parameter $h$, such that
  \begin{gather}
    \label{eq:infsup:17}
    \begin{split}
      \norm{u-u_h}
      &\le c_1 \inf_{v_h\in V_h}\norm{u-v_h}_V
      + c_2 \inf_{q_h\in Q_h} \norm{p-q_h}_Q
      \\
      \norm{p-p_h}
      &\le c_3 \inf_{v_h\in V_h}\norm{u-v_h}_V
      + c_4 \inf_{q_h\in Q_h} \norm{p-q_h}_Q.
    \end{split}
  \end{gather}
\end{Corollary}

\begin{proof}
  The proof begins with the standard approach with triangle inequality
  \begin{align*}
    \norm{u-u_h} &\le \norm{u-v_h} + \norm{v_h-u_h} \\
    \norm{p-p_h} &\le \norm{p-q_h} + \norm{q_h-p_h}.
  \end{align*}
  Then, we employ \slideref{Lemma}{stabilized-mixed-approximation} on
  the terms on the right and use the estimate of
  \slideref{Corollary}{mixed-residual-bounded}.
\end{proof}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End:
