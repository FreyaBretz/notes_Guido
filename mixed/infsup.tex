
\section{Finite-dimensional problems}
\begin{intro}
  So far, our power horse for well-posedness was the Lax-Milgram
  lemma, which can be applied under the conditions
  \begin{xalignat}2
    a(u,v) &\le M \norm{u}\norm{v} & \forall u,v&\in V\\
    \label{eq:infsup:elliptic}
    a(u,u) &\ge \gamma \norm{u}^2 & \forall u&\in V.
  \end{xalignat}
  The second condition can also be rewritten in terms of the
  \putindex{Rayleigh quotient} as
  \begin{gather*}
    0 < \gamma = \inf_{u\in V}\frac{a(u,u)}{\norm{u}^2}.
  \end{gather*}
  Restricting this to a finite dimensional space, the notation usually
  changes from
  \begin{gather}
    a(u,v) = f(v)
    \qquad\text{to}\qquad
    v^TA u = v^T f,
  \end{gather}
  where $A\in \R^{n\times n}$ is the matrix associated with the
  bilinear form. The bound for the Rayleigh quotient means nothing but
  that the real parts of all eigenvalues of $A$ are bounded from below
  by $\gamma$. Thus, a matrix $A$ for which we can apply the
  Lax-Milgram lemma is positive definite. And the statement of the
  lemma in finite dimension is, that a positive definite matrix is
  invertible. We know from linear algebra that this is true, but we
  also know that the condition is all but necessary.
\end{intro}

\begin{intro}
  Why did we replace this clear theorem by the weaker Lax-Milgram
  lemma, when we studied elliptic partial differential equations?  For
  the first condition, it should be noted that spectral properties of
  operators between spaces of infinite dimension are much harder to
  obtain. Further, we do not need information on the whole spectrum,
  but only on the eigenvalue closest to zero. Therefore, we used a
  simple estimate in order to avoid discussing the spectrum at
  all. But, there is an important difference between
  Theorem~\ref{Theorem:la-invertible} and the
  estimate~\eqref{eq:infsup:elliptic}: the assumption of the theorem
  is qualitative, $\lambda \neq 0$, while the assumption of Lax-Milgram
  is quantitative,
  \begin{gather*}
    \Re\lambda \ge \gamma> 0.
  \end{gather*}
  The following problem shows why such a change is necessary.
\end{intro}

\begin{Problem}{unbounded-inverse}
  On the space $\ell_2(\R)$ define the operator $A$ by its eigenvalue
  decomposition
  \begin{align*}
    A: \ell_2(\R) &\to \ell_2(\R)\\
    e_k & \mapsto \tfrac1k e_k.
  \end{align*}
  Here, $\{e_k\}$ is the orthogonal basis of unit vectors of the form
  \begin{gather*}
    \arraycolsep0.1em
    \begin{array}{cccccccc}
      e_k =(0&,\ldots,&0&,&1&,&0&,\ldots)^T.\\
      &&&&\uparrow\\
      &&&&k
    \end{array}
  \end{gather*}
  \begin{enumerate}
  \item Show that this operator does not have a bounded inverse, albeit
    its eigenvalues are positive.
  \item Show that the range of $A$ is not closed in $\ell_2(\R)$
  \end{enumerate}
\begin{solution}  
  \begin{enumerate}
  \item For each $e_k$, the inverse is $A^{-1} e_k = k e_k$. In particular, $A$ is injective.
    On the other hand, it holds
    \begin{gather*}
      \lim_{k\to\infty}\frac{\norm{A^{-1}e_k}}{\norm{e_k}}=\lim_{k\to\infty}k=\infty
    \end{gather*}
      and the inverse cannot be bounded.
  \item We have to construct a convergent sequence in the range of $A$
    such that the pre-image of the sequence does not converge.
    \begin{enumerate}
    \item Choose
      \begin{gather*}
        v_n = \sum_{k=1}^n \frac1k e_k.
      \end{gather*}
      \item $v_n$ is a Cauchy-sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n \frac1k e_k}^2
          = \sum_{k=m}^n \frac1{k^2}\norm*{e_k}^2
          \le \frac1{m^2} \sum_{k=1}^\infty \frac1{k^2}
          = \frac{\pi^2}{6} \frac1{m^2}.
        \end{gather*}
      \item We conclude that $v=\lim_{n\to\infty}v_n$ exists in the
        closure of the range of $A$.
      \item There holds
        \begin{gather*}
          v_n = A \sum_{k=1}^n e_k =: A u_n.
        \end{gather*}
      \item Due to the injectivity of $A$ for $v$ to be in the range of $A$,
            $u_n$ has to converge in $\ell_2(\R)$.
      \item The sequence $u_n$ is not a Cauchy sequence, since
        \begin{gather*}
          \norm{v_m-v_n}^2 = \norm*{\sum_{k=m}^n e_k}^2
          = \sum_{k=m}^n \norm*{e_k}^2
          = n-m.
        \end{gather*}
    \end{enumerate}
  \end{enumerate}
\end{solution}
\end{Problem}

\begin{Problem}{lax-milgram-not-applicable}
  Find an invertible, symmetric matrix $A\in \R^{2\times 2}$ and a
  vector $v\in \R^2$ such that $v^T A v=0$ and thus the Lax-Milgram
  lemma is inconclusive.
\begin{solution}
  \begin{gather*}
    A =
    \begin{pmatrix}
      1 & 0 \\ 0 & -1
    \end{pmatrix}
  \end{gather*}
\end{solution}
\end{Problem}

The question of well-posedness in finite dimensions can be answered by:

\begin{Theorem}{la-invertible}
  A matrix $A\in\R^{n\times n}$ is invertible if and only if one of
  the following equivalent conditions holds:
  \begin{enumerate}
  \item all its (possibly complex) eigenvalues are nonzero,
  \item all its singular values are nonzero,
  \item for each $v\in\R^n$ holds $Av\neq 0$.
  \end{enumerate}
\end{Theorem}

\begin{intro}
  We focus on the second and third conditions, respectively, in
  Theorem~\ref{Theorem:la-invertible}.
  But, the problem above tells us that we
  will run into trouble, if we do not quantify this. Therefore, we
  start our attempt by requiring:
  \begin{gather*}
    \norm{Au}^2 \ge \gamma \norm{u}^2 \qquad\forall u\in V.
  \end{gather*}
  But while this is a condition we can easily write down for matrices
  and operators, it does not work that well for bilinear forms. Thus,
  we first look at the singular value decomposition.
\end{intro}

\input{svd}

\begin{Definition}{ker-range-rn}
  \index{ker}
  Let $A: V\to W$ be a linear operator. Then, we define
  the \define{kernel} and the \define{range} of $A$ as
  \begin{align*}
    \ker A &= \bigl\{ v\in V \big| Av = 0\bigr\} \\
    \range A &= \bigl\{ w\in W \big| \;\exists\,v\in V: Av=w\bigr\}.
  \end{align*}
\end{Definition}

\begin{Definition}{orthogonal1}
  Let $V\subset\R^n$ be a subspace. We define the \define{orthogonal
    complement} of $V$ as
  \begin{gather}
    \label{eq:infsup:4}
    V^\perp = \bigl\{w\in \R^n \big| \;\forall\,v\in V \scal(w,v) = 0 \bigr\}.
  \end{gather}
\end{Definition}

\begin{Lemma}{ker-coker-rn}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, there holds
  \begin{gather}
    \label{eq:infsup:5}
    \begin{split}
      \ker A &= \range{A^T}^\perp\\
      \range A &= \ker{A^T}^\perp\\
      \ker{A^T} &= \range A^\perp\\
      \range{A^T} &= \ker{A}^\perp
    \end{split}
  \end{gather}
\end{Lemma}

\begin{proof}
  % First, we note that
  % \begin{gather*}
  %   \R^n = \ker A \oplus \ker A^\perp,
  %   \qquad
  %   \R^m = \ker{A^T} \oplus \ker{A^T}^\perp.
  % \end{gather*}
  Let $A=U\Sigma V^T$ be the singular value decomposition of $A$ and
  $r$ be the number of nonzero singular values. Then, the first $r$
  vectors of $U$ span the range of $A$ and the last $n-r$ vectors of
  $V$ span its kernel. Furthermore,
  \begin{gather}
    A^T = \bigl(U\Sigma V^T\bigr)^T = V \Sigma U^T.
  \end{gather}
  Therefore, the first $r$ vectors of V span the range of $A^T$ and
  the last $n-r$ vectors of $U$ span its kernel.
\end{proof}

\begin{Corollary}{ker-coker-iso}
  Let $A\in \R^{m\times n}$ and $A^T$ its transpose. Then, the
  restrictions $A\colon \ker A^\perp \to \range A$ and $A^T\colon
  \ker{A^T}^\perp \to \range{A^T}$ are isomorphisms.
\end{Corollary}

\begin{proof}
  We note that $\dim \range A = \dim \range{A^T}$. Thus, by
  \blockref{Lemma}{ker-coker-rn} the dimensions of domain and range of
  each of the restricted operators are equal, say $\dim \range A =
  r$. The singular value decomposition of the operators is
  \begin{gather*}
    A = U\Sigma V^T \qquad A^T = V\Sigma U^T,
  \end{gather*}
  where all matrices are in $\R^{r\times r}$ and
  \begin{gather*}
    \Sigma = \diag(\sigma_1,\dots,\sigma_r),
  \end{gather*}
  and all singular values are positive. Thus, $A$ and $A^T$ are invertible.
\end{proof}

\begin{Corollary}{svd-infsup}
  Let $r=\dim \ker A^\perp$. Then, for the smallest nonzero singular
  value there holds
  \begin{gather}
    \label{eq:infsup:6}
    \sigma_r
    = \inf_{v\in \ker{A}^\perp} \sup_{w\in \R^m} \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{w\in \ker{A^T}^\perp} \sup_{v\in \R^n} \frac{w^T A v}{\abs{v}\abs{w}}.
  \end{gather}
\end{Corollary}

\begin{proof}
  Since the Cauchy-Schwarz inequality turns into an equation if and
  only if the two vectors are coaligned, there holds for any $v\in \R^n$:
  \begin{gather*}
    \sup_{w\in\R^m}\frac{w^T A v}{\abs{w}} = \frac{v^T A^T A v}{\abs{Av}}.
  \end{gather*}
  Therefore,
  \begin{gather*}
    \inf_{v\in \ker{A}^\perp} \sup_{w\in \R^m}
    \frac{w^T A v}{\abs{v}\abs{w}}
    = \inf_{v\in \ker{A}^\perp} \frac{\abs{Av}}{\abs{v}}.
  \end{gather*}
  Now, let $v = \sum \alpha_i v_i$ where $v_i$ are the columns of $V$
  in the SVD of $A$. Then,
  \begin{gather*}
    \abs{Av}^2 = \abs*{A\sum_{i=1}^r \alpha_i v_i}^2
    = \abs*{\sum_{i=1}^r \sigma_i \alpha_i u_i}^2
    = \sum_{i=1}^r \sigma_i^2 \alpha_i^2.
  \end{gather*}
  The quotient
  \begin{gather*}
    \frac{\sum_{i=1}^r \sigma_i^2 \alpha_i^2}{\sum_{i=1}^r \alpha_i^2}
  \end{gather*}
  clearly has its minimum if $\alpha_1 = \dots=\alpha_{r-1} = 0$.
\end{proof}

\begin{Definition}{infsup1}
  A bilinear form $a(\cdot,\cdot)$ on $V\times W$ is said to admit the
  \define{inf-sup condition} or is called \define{inf-sup stable}, if
  there holds
  \begin{gather}
    \label{eq:infsup:1}
    \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W}
    \ge \gamma > 0.
  \end{gather}
\end{Definition}

\begin{remark}
  In this finite dimensional exposition, is clear that $V$ and $W$
  must have the same dimension, and thus $V=W=\R^n$. This will be
  different, when we consider infinite dimensional spaces and indeed
  consider different spaces $V$ and $W$.
\end{remark}

\begin{Lemma}{infsup2}
  The following statements are equivalent to the inf-sup
  condition~\eqref{eq:infsup:1}:
  \begin{gather}
    \label{eq:infsup:2}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \gamma \norm{u}_V\norm{w}_W
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \norm{w}_W &\le&\norm{u}_V\\
      a(u,w) &\ge& \gamma \norm{u}_V^2
    \end{array}
    \right.
  \end{gather}
  \begin{gather}
    \label{eq:infsup:3a}
    \forall u\in V
    \;\exists w\in W \;:\;
    \left\{
    \arraycolsep0.3ex
    \begin{array}{rcl}
      \gamma \norm{w}_W &\le&\norm{u}_V\\
      Aw &=& u
    \end{array}
    \right.
  \end{gather}
\end{Lemma}

\begin{Problem}{inf-sup-equivalence}
  Prove Lemma 2.1.16.
\begin{solution}
  We have to prove the following statements are equivalent to the inf-sup condition:
  \begin{align}
    \forall u\in V \;\exists w\in W \;:\; a(u,w) \ge \gamma \norm{u}_V\norm{w}_W     \tag{1}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \norm{w}_W \le\norm{u}_V\\
      a(u,w) \ge \gamma \norm{u}_V^2
    \end{cases}
   \end{aligned}
   \tag{2}
  \end{align}
  \begin{align}
   \begin{aligned}
    \forall u\in V \;\exists w\in W \;:\;
    \begin{cases}
      \gamma \norm{w}_W \le \norm{u}_V\\
      Aw = u
      \end{cases}
  \end{aligned}
      \tag{3}
  \end{align}
  The inf-sup condition reads
  \begin{align}
   \inf_{u\in V} \sup_{w\in W} \frac{a(u,w)}{\norm{u}_V\norm{w}_W} \ge \gamma \tag{IS}
  \end{align}

  $(IS)\Rightarrow(3)$\\
  The inf-sup condition is equivalent to $A: ker(A)^\perp\to V^*$ being an isomorphism.
  By the Riesz representation theorem there exists for a given $u\in V$ a $w\in W$ such that
  $Aw=Ju$ where $J$ is the Riesz map. Hence, it holds
  \begin{align*}
a(u,w)=\langle A w, u\rangle = \langle J u, u\rangle = \norm{u}_V^2.
  \end{align*}
 Due to $w\in ker(A)^\perp$, $A^{-1}$ is bounded and
 \begin{align*}
 \norm{w}_W=\norm{A^{-1}u}_V\leq \frac{1}{\gamma}\norm{u}_V.
  \end{align*}

  $(3)\Rightarrow(2)$ \\
  Define $\tilde{w}=\gamma w$. Then,
  \begin{align*}
a(u,\tilde{w})=\gamma a(u,w) = \gamma \norm{u}_V^2
  \end{align*}
  and
  \begin{align*}
\norm{\tilde{w}}_W=\gamma \norm{w}_W\leq\norm{u}_V
  \end{align*}

  $(2)\Rightarrow(1)$ \\
  \begin{align*}
  a(u,w) \ge \gamma \norm{u}_V^2 \ge \gamma \norm{u}_V \norm{w}_W
  \end{align*}

  $(1)\Rightarrow(IS)$ \\
  \begin{align*}
  &\forall u\in V \exists w\in W: a(u,w)\ge \gamma \norm{u}_V\norm{w}_W\\
  &\Rightarrow \forall u\in V: \sup_{w\in W} \frac{a(u,w)}{\norm{w}_W}\ge \gamma \norm{u}_V\\
  &\Rightarrow \inf_{u\in V}\sup_{w\in W} \frac{a(u,w)}{\norm{w}_W\norm{u}_V}\ge \gamma
  \end{align*}
\end{solution}
\end{Problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Infinite dimensional Hilbert spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we derived quantitative conditions to
  ensure the invertibility of a matrix $A$ or its restriction to its
  cokernel $\ker A^\perp$. The arguments there have a natural
  extension to infinite dimensional Hilbert spaces, which we will
  derive in this section. We already saw in
  \blockref{Problem}{unbounded-inverse} that we may run into trouble
  if the range of $A$ is not closed. On the other hand, it turns out
  that most notions of linear algebra related to orthogonality can be
  maintained in Hilbert spaces if closed subspaces are considered.
  We begin by citing the most important results.
\end{intro}

\begin{Definition}{polar-orthogonal}
  Let $W\subset V$ be a subspace of a Hilbert space $V$. We define its
  \define{polar space} $W^0\subset V^*$ and its
  \define{orthogonal complement} $W^\perp\subset V$ by
  \begin{gather}
    \label{eq:infsup:7}
    \begin{split}
    W^0 &= \bigl\{f\in V^* \big| \scal(f,w)_{V^*\times V} = 0
    \quad\forall\,w\in W\bigr\},
    \\
    W^\perp &= \bigl\{v\in V \big| \scal(v,w)_{V} = 0
    \quad\forall\,w\in W\bigr\}.
    \end{split}
  \end{gather}
  For a subspace $U\subset V^*$, we define its polar space
  \begin{gather}
    U^0 = \bigl\{v\in V \big| \scal(u,v)_{V^*\times V} = 0
    \quad\forall\,u\in U\bigr\}
  \end{gather}
\end{Definition}

\begin{Lemma}{orthogonal-closed}
  The polar space $W^0$ and the orthogonal complement $W^\perp$ of a
  subspace $W\subset V$ are both closed. So is the polar space $U^0$
  of a subspace $U\subset V^*$.
\end{Lemma}

\begin{proof}
  Consider the mapping
  \begin{align*}
    \Phi_w\colon V^* &\to \R,\\
    v&\mapsto \scal(v,w)_{V^*\times V}.
  \end{align*}
  For any $w$, the kernel of $\phi$ is closed as
  the pre-image of a closed set. $W^0$ is closed since it is the
  intersection of these kernels for all $w\in W$.

  The inner product is continuous on $V\times V$. Therefore, the
  mapping
  \begin{align*}
    \phi_w\colon V &\to \R,\\
    v&\mapsto \scal(v,w),
  \end{align*}
  is continuous. The argument continues as above. Similar for $U^0$.
\end{proof}

\begin{Theorem}{orthogonal-complement}
  Let $W$ be a subspace of a Hilbert space $V$ and $W^\perp$ its
  orthogonal complement. Then, $W^\perp = \overline{W}^\perp$. Further,
  $V = W \oplus W^\perp$ if and only if $W$ is closed.
\end{Theorem}

\begin{proof}
  Clearly, $\overline{W}^\perp \subset W^\perp$ since
  $W\subset\overline{W}$. Let now $u\in W^\perp$. Then, $\phi =
  \scal(u,\cdot)$ is a continuous linear functional on $V$. Therefore,
  if a sequence $w_n \subset W$ converges to $w\in \overline{W}$, we
  have
  \begin{gather*}
    \scal(u,w) = \lim_{n\to\infty} \scal(u,w_n) = 0.
  \end{gather*}
  Hence, $u\in \overline{W}^\perp$ and $W^\perp = \overline{W}^\perp$.

  Now, the ``only if'' follows by the fact, that if $W$ is not
  closed, there is an element $w\in \overline{W}$ but not in $W$ such that
  $\scal(w,u)=0$ for all $u\in W^\perp$. Thus, $w\not\in W^\perp$ and
  consequently $w\not\in W^\perp \oplus W$.

  Let now $W$ be closed. We show that there is a unique decomposition
  \begin{gather}
    \label{eq:infsup:8}
    v = w + u,\qquad w\in W, \;u\in W^\perp,
  \end{gather}
  which is equivalent to $V = W \oplus W^\perp$. Uniqueness follows,
  since
  \begin{gather*}
    v = w_1+u_1 = w_2+u_2
  \end{gather*}
  implies that for any $y\in V$
  \begin{gather*}
    0 = \scal(w_1-w_2+u_1-u_2,y) = \scal(w_1-w_2,y) + \scal(u_1-u_2,y).
  \end{gather*}
  Choosing $y=u_1-u_2$ and $w_1-w_2$ in turns, we see that one of the
  inner products vanishes for orthogonality and the other implies that
  the difference is zero.

  If $v\in W$, we choose $w=v$ and $u=0$. For $v\not\in W$, we prove
  existence by considering that due to the closedness of $W$ there holds
  \begin{gather*}
    d=\inf_{w\in W} \norm{v-w} >0.
  \end{gather*}
  Let $w_n$ be a minimizing sequence. Using the parallelogram identity
  \begin{gather*}
    \norm{a+b}^2+\norm{a-b}^2 = 2\norm{a}^2+2\norm{b}^2,
  \end{gather*}
  we prove that $\{w_n\}$ is a Cauchy sequence by
  \begin{align*}
    \norm{w_m-w_n}^2 &= \norm{(v-w_n)-(v-w_m)}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-\norm{2v-w_m-w_n}^2\\
    &= 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4\norm*{v-\frac{w_m+w_n}2}^2\\
    &\le 2\norm{v-w_n}^2+2\norm{v-w_m}^2-4d^2,
  \end{align*}
  since $(w_m+w_n)/2\in W$ and $d$ is the infimum. Now we use the
  minimizing property to obtain
  \begin{gather*}
    \lim_{m,n\to\infty}\norm{w_m-w_n}^2 = 2d^2-2d^2 -4d^2=0.
  \end{gather*}
  By completeness of $V$, $w=\lim w_n$ exists and by the closedness of
  $W$, we have $w\in W$. Let $u=v-w$. By continuity of the norm, we
  have $\norm{u}=d$. It remains to show that $u\in W^\perp$. To this
  end, we introduce the variation $w+\epsilon \tilde w$ with $\tilde
  w\in W$ to obtain
  \begin{align*}
    d^2 &\le \norm{v-w-\epsilon \tilde w}^2\\
    &= \norm{u}^2-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{align*}
  implying for any $\epsilon>0$
  \begin{gather*}
    0\le-2\epsilon\scal(u,\tilde w)+\epsilon^2 \norm{\tilde w},
  \end{gather*}
  which requires $\scal(u,\tilde w) = 0$.
\end{proof}

\begin{Definition}{orthogonal-projection}
  Let $V$ be a Hilbert space and $W\subset W$ be a closed
  subspace. For a vector $v\in V$, let $v=w+u$ be the unique
  decomposition with $w\in W$ and $u\in W^\perp$. Then we call $w$ and
  $u$ the \define{orthogonal projection}s of $v$ into $W$ and $W^\perp$,
  respectively. We write
  \begin{gather*}
    \Pi_W = w, \qquad \Pi_{W^\perp} = u.
  \end{gather*}
\end{Definition}

\begin{Lemma}{polar-orthogonal-hilbert}
  Let $V$ be a Hilbert space and $W\subset W$ be a closed
  subspace. Then, the polar space $W^0\subset V^*$ and the orthogonal
  space $W^\perp$ can be isometrically identified by \putindex{Riesz
    representation}.
\end{Lemma}

\begin{proof}
  For every $f$ in the
  dual of $W^\perp$, define $g\in V^*$ by
  \begin{gather*}
    \scal(g,v)_{V^*\times V} =
    \scal(f,\Pi_{V^\perp}v)_{(V^\perp)^*\times V^\perp}.
  \end{gather*}
  Clearly, $g(v)=0$ for $v\in W$, therefore $g\in W^0$.
\end{proof}

% \begin{Corollary}
  
% \end{Corollary}


\begin{Theorem}{closed-range}
  Let $V,W$ be Hilbert spaces and $A\colon V\to W$ a continuous linear
  operator. Then, the following statements are equivalent:
  \begin{gather}
    \label{eq:infsup:9}
    \begin{split}
      \range A &\text{ is closed in } W,\\
      \range{A^T} &\text{ is closed in } V^*,\\
      \range A &= \ker{A^T}^0,\\
      \range{A^T} &= \ker{A}^0.
    \end{split}
  \end{gather}
\end{Theorem}

\begin{remark}
  This is the famous \emph{\putindex{closed range theorem}} by Banach.
  It actually holds under weaker assumptions, for instance $V,W$ only
  Banach spaces. The proof can be found for instance
  in~\cite[p.~205--209]{Yosida80}.
\end{remark}

\begin{Theorem}{open-mapping}
  Let $A\colon V\to W$ be continuous and surjective. Then, the image
  $A(U)\subset W$ of any open set $U\subset V$ is open.
\end{Theorem}

\begin{remark}
  This is the \emph{\putindex{open mapping theorem}} by Banach. The
  proof can be found for instance in~\cite[p.75--76]{Yosida80}.
\end{remark}

\begin{Lemma}{closed-infsup}
  Let $A\colon V\to W$ be continuous. Then, $\range A$ is closed in
  $W$ if and only if there exists $\gamma>0$ such that
  \begin{gather}
    \label{eq:infsup:10}
    \forall \,w\in W\;
    \exists \,v\in V\quad
    Av = w
    \;\wedge\;
    \gamma \norm{v}_V \le \norm{w}_W.
  \end{gather}
\end{Lemma}

\begin{proof}
  We first show that the inf-sup condition~\eqref{eq:infsup:10}
  implies $\range A$ closed. To this end, let $\{w_n\}$ be a Cauchy
  sequence in $\range A$ converging to a point $w\in W$. Thus, there
  is a sequence $\{v_n\}$ in $V$ such that $Av_n = w_n$ and
  $\gamma \norm{v_n} \le \norm{w_n}$. Hence,
  \begin{gather*}
    \norm{v_m-v_n}_V \le \frac1\gamma \norm{w_m-w_n}_W,
  \end{gather*}
  and $\{v_n\}$ is a Cauchy sequence in $V$. Therefore, $v_n\to v\in
  V$ and due to continuity of $A$ we obtain $Av=w$ and thus $w\in
  \range A$.

  Conversely, let $\range A$ be closed in $W$. Thus, it is a Banach
  space and the \putindex{open mapping theorem} applies to $A\colon
  V\to\range A$. We map the open unit ball $B_1(0)\subset V$ and
  obtain that $A(B_1(0))$ is open in $\range A$, implying that there
  is an open ball $B_\delta(0) \subset A(B_1(0))$. This is sufficient
  to construct $v$:

  Let $w\in\range A$. Then,
  \begin{gather*}
    \tilde w\frac\delta2 \frac{w}{\norm{w}} \in B_\delta(0) \subset A(B_1(0)).
  \end{gather*}
  Hence, there is $v\in V$ with $\norm{v}<1$ such that $Av=\tilde w$,
  which proves the lemma.
\end{proof}

\begin{Theorem}{infsup-well-equivalence}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W,
  \end{gather}
  and $A\colon V\to W^*$ its associated operator.
  Then, the following statements are equivalent:
  \begin{enumerate}
  \item There exists $\gamma>0$ such that
    \begin{gather}
      \label{eq:infsup:11}
      \inf_{w\in W}\sup_{v\in V}
      \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
      \ge \gamma.
    \end{gather}
  \item The operator $A^T\colon W\to \ker A^0$ is an isomorphism and
    \begin{gather}
      \label{eq:infsup:12}
      \norm{A^Tw}_{V^*} \ge \norm{w}_{W} \qquad\forall w\in W.
    \end{gather}
  \item The operator $A\colon \ker A^\perp\to W^*$ is an isomorphism
    and
    \begin{gather}
      \label{eq:infsup:13}
      \norm{Av}_{W^*} \ge \gamma\norm{v}_V\qquad \forall v\in \ker A^\perp.
    \end{gather}
  \end{enumerate}
\end{Theorem}

\begin{proof}
  First, we show the equivalence of the first two statements. Let us
  rephrase the inf-sup condition to
  \begin{gather*}
    \norm{A^T w}_{V^*}
    = \sup_{v\in V}\frac{\scal(A^T w,v)}{\norm{v}_V}
    = \sup_{v\in V}\frac{a(v,w)}{\norm{v}_V}
    \ge \gamma\norm{w} \qquad
    \forall w\in W.
  \end{gather*}
  Thus, equations~\eqref{eq:infsup:11} and~\eqref{eq:infsup:12} are
  equivalent and we have already proven that the second statement
  implies the first. It remains to show the $A^T$ is an isomorphism
  from $W$ onto $\ker A^0$. Equation~\eqref{eq:infsup:12} implies that
  $A^T\colon W \to \range{A^T}$ is an isomorphism and its inverse is
  bounded by $1/\gamma$ (multiply both sides by $A^{-1}$). Using
  \blockref{Lemma}{closed-infsup}, we obtain that $\range{A^T}$ is
  closed in $V^*$ and the \putindex{closed range theorem} settles the
  issue.

  In order to prove equivalence of the second and third statement, we
  use \blockref{Lemma}{polar-orthogonal-hilbert} to isometrically
  identify $(\ker A^\perp)^*$ with $\ker A^0$. Thus, $A$ is an
  isomorphism from $\ker A^\perp$ onto $W$ if and only if $A^T$ is an
  isomorphism from $W$ onto $(\ker A^\perp)^* = \ker A^0$. and
  \begin{gather*}
    \norm{A}_{W^*\to \ker A^\perp} = \norm{A^T}_{\ker A^0\to W}.
  \end{gather*}
\end{proof}

\begin{Corollary}{infsup-well-posedness1}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W.
  \end{gather}
  Let the inf-sup-condition
  \begin{gather*}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    \ge \gamma > 0    
  \end{gather*}
  hold.  Then, the problem finding $w\in W$ such that
  \begin{gather*}
    a(v,w) = f(v) \qquad\forall v\in V,
  \end{gather*}
  has a unique solution for $f\in \ker A^0$ and
  \begin{gather}
    \norm{w}_W \le \frac1\gamma \norm{f}_{V^*}.
  \end{gather}
\end{Corollary}

\begin{remark}
  \blockref{Theorem}{infsup-well-equivalence} exhibits an asymmetry
  between the left and right argument. In particular, we obtain a
  unique solution only for the adjoint operator $A^T$, which is
  exactly what we need, when we compute say a pressure from the
  divergence of a velocity field. In general, we consider the
  restriction of $f$ to the polar set of the kernal in the above
  well-posedness result detrimental and would prefer a result that
  holds for all $f\in V^*$. This on the other hand requires
  $\ker A=\{0\}$, or $\overline{\range{A^T}} = W^*$. Then, on the
  other hand, we see that $\range{A^T}$ is closed since $\range{A}$ is
  closed and the closed range theorem holds. Therefore, we obtain the
  following theorem for the case that we require a unique solution for
  all right hand sides.
\end{remark}

\begin{Theorem}{infsup-well-posedness2}
  Let $a(\cdot,\cdot)$ on $V\times W$ be a bounded bilinear form such that
  \begin{gather}
    a(v,w) \le M \norm{v}_V \norm{w}_W.
  \end{gather}
  Let for some $\gamma>0$ the inf-sup-conditions
  \begin{align*}
    \inf_{w\in W}\sup_{v\in V}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \gamma,\\
    \inf_{v\in V}\sup_{w\in W}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W}
    &\ge \gamma
  \end{align*}
  hold.  Then, the problem finding $v\in V$ such that
  \begin{gather*}
    a(v,w) = f(w) \qquad\forall w\in W,
  \end{gather*}
  has a unique solution for $f\in W^*$ and
  \begin{gather}
    \norm{v}_V \le \frac1\gamma \norm{f}_{W^*}.
  \end{gather}  
\end{Theorem}

\begin{remark}
  If we compare \blockref{Theorem}{infsup-well-posedness2} with
  \blockref{Corollary}{infsup-well-posedness1}, we see that the only
  difference lies in the fact that the second inf-sup condition
  ensures surjectivity of $A$ by injectivity of $A^T$. In some cases
  it may be difficult to prove both inf-sup conditions. Then, it is
  sufficient to prove one inf-sup condition, say the first, and then
  only
  \begin{gather*}
     \inf_{v\in V}\sup_{w\in W}
    \frac{a(v,w)}{\norm{v}_V\norm{w}_W} > 0,
  \end{gather*}
  thus, injectivity of $A^T$. Although we verify less than the
  assumptions of \blockref{Theorem}{infsup-well-posedness2}, the
  closed range theorem saves us from the additional work. We further
  note that this notion is symmetric between $A$ and $A^T$, that is,
  it is sufficient to prove inf-sup for either operator and
  injectivity for the other.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The inf-sup condition for mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In the previous section, we have developed a framework for
  well-posedness of problems which are not $V$-elliptic. In principle,
  this theory can be applied to the bilinear form
  $\mathcal A((u,p),(v,q))$ as a whole. On the other hand, we can
  formally split the solution of a constrained minimization problem
  into the reduced problem and then computing the Lagrange multiplier,
  which more clearly exhibits the relation of the two spaces $V$ and
  $Q$ involved in the mixed formulation. Here are the resulting
  theorems.
\end{intro}

\begin{Theorem}{infsup-mixed1}
  Let $V$ and $Q$ be Hilbert spaces and let the mixed bilinear form
  \begin{gather*}
        \mathcal A\mixedform(u,p,v,q)
      = a(u,v) + b(v,p) + b(u,q)
  \end{gather*}
  be defined and bounded for any $u,v\in V$ and $p,q\in Q$.
  Then, the problem
  \begin{gather*}
    \mathcal A\mixedform(u,p,v,q) = \scal(f,v)+\scal(g,q)
    \quad\forall v\in V, q\in Q,
  \end{gather*}
  has a unique solution for any $f\in V^*$ and any $g\in Q^*$ if and
  only if there exists $\gamma>0$ such that
  \begin{gather*}
    \forall
    \begin{bmatrix}
      u\in V\\p\in Q
    \end{bmatrix}
    \;
    \exists
    \begin{bmatrix}
      v\in V\\q\in Q
    \end{bmatrix}
    \colon
    \quad
    \mathcal A\mixedform(u,p,v,q) \ge \gamma
    \norm{(u,p)}_{V\times Q}\norm{(v,q)}_{V\times Q},
  \end{gather*}
  and vice versa.
\end{Theorem}

\begin{proof}
  Straight application of \blockref{Theorem}{infsup-well-posedness2}.
\end{proof}

\begin{Theorem}{infsup-mixed2}
  Let $V$ and $Q$ be Hilbert spaces and let
  \begin{gather}
    \begin{split}
      \ker B &= \bigl\{v\in V \big| b(v,q) = 0 \;\forall q\in Q\bigr\}.
    \end{split}
  \end{gather}
  Then, the problem finding $(u,p)\in V\times Q$ such that
  \begin{gather}
    a(u,v) + b(v,p) + b(u,q) = f(v)+g(q) \quad\forall v\in V, q\in Q,
  \end{gather}
  is well-posed if and only if the problem finding $u\in \ker B$ such that
    \begin{gather}
      a(u,v) = f(v) \quad\forall v\in \ker B
    \end{gather}
    is well-posed and there is a positive constant $\beta$ such that
    \begin{gather}
      \inf_{q\in Q}\sup_{v\in V} \frac{b(v,q)}{\norm{v}_V\norm{q}_Q} \ge \beta.
    \end{gather}
\end{Theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Galerkin approximation of mixed problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
