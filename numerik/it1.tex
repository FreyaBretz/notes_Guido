\subsection{Fixpunktiterationen}
\begin{Definition}{iteration}
  Ein \define{Iterationsverfahren} berechnet schrittweise
  Approximationen an die Lösung $x$ einer Aufgabe aus einem Startwert
  $x^{(0)}$ mit der Verfahrensvorschrift der Form
  \begin{gather}
    x^{(k+1)} = f(x^{(k)}), \qquad k=0,1,2,\dots.
  \end{gather}
  Wir nennen $f$ die \define{Iterationsfunktion} und $\{x^{(k)}\}$ die
  \define{Iterationsfolge} zu $f$ mit Startwert $x^{(0)}$.
  
  Das Verfahren heißt \define{konvergent}, wenn gilt $x^{(k)} \to x$
  für $k\to\infty$.
\end{Definition}

\begin{Definition}{iteration-ordnung}
  Ein Iterationsverfahren
  \index{Konvergenzordnung!Iterationsverfahren} ist konvergent
  mindestens von Ordnung $p>1$ zum Grenzwert $x$, wenn es eine
  Konstante $c>0$ gibt, so dass
  \begin{gather}
    \norm*{x^{(k+1)} - x} \le  c \norm*{x^{(k)} - x}^p
  \end{gather}
  gilt. Es ist linear konvergent, wenn
  \begin{gather}
    \norm*{x^{(k+1)} - x} \le  c \norm*{x^{(k)} - x}
  \end{gather}
  mit einer Konstanten $c<1$. Wir sprechen von superlinearer Konvergenz, wenn
  \begin{gather}
    \norm*{x^{(k+1)} - x} = \smallo\left(\norm*{x^{(k)} - x}\right)
  \end{gather}
\end{Definition}

\begin{Definition}{kontraktion}
  Sei $M\subset\R^n$. Eine Abbildung $f\colon M\to M$ ist eine \define{Kontraktion} auf $M$, wenn es eine Konstante $\rho < 1$ gibt, so dass
  \begin{gather}
    \norm{f(x) - f(y)} \le \rho \norm{x-y} \qquad\forall x,y\in M.
  \end{gather}
\end{Definition}

\begin{Satz*}{bfs}{Banachscher Fixpunktsatz}
  Sei $f$ eine Kontraktion auf der abgeschlossenen Menge
  $M\subset\R^n$. Dann gibt es genau einen \define{Fixpunkt} $x^*\in M$,
  also
  \begin{gather}
    x^* = f(x^*).
  \end{gather}
  Für jeden Startwert $x^{(0)}\in M$ konvergiert die Folge definiert durch
  \begin{gather}
    x^{(k+1)} = f\left(x^{(k)}\right)
  \end{gather}
  gegen diesen Fixpunkt. Es gelten die Fehlerabschätzungen
  \begin{gather}
    \norm{x^{(k)}-x^*}
    \le \frac{\rho}{1-\rho}\norm{x^{(k)}-x^{(k-1)}}
    \le \frac{\rho^k}{1-\rho}\norm{x^{(1)}-x^{(0)}}.
  \end{gather}
\end{Satz*}

\begin{proof}
  Zunächst zeigen wir die Eindeutigkeit nach der üblichen Methode:
  seien $x,y\in M$ Fixpunkte der Kontraktion $f(\cdot)$. Dann gilt
  \begin{gather}
    \norm{x-y} = \norm{f(x)-f(y)} \le \rho \norm{x-y}.
  \end{gather}
  Da $\rho<1$ kann dies nur gelten, wenn die Differenz verschwindet.

  Als nächstes zeigen wir, dass die Iterationsvorschrift eine
  Cauchy-Folge erzeugt, woraus die Konvergenz gegen einen Grenzwert
  folgt. Dazu beobachten wir zunächst, dass mit $x^{(0)}\in M$ auch
  alle weiteren Folgenglieder in $M$ liegen. Ferner gilt:
  \begin{gather}
    \norm{x^{(k+1)}-x^{(k)}} \le \rho \norm{x^{(k)}-x^{(k-1)}}
    \le \rho^k\norm{x^{(1)}-x^{(0)}}.
  \end{gather}
  Daher gilt für $m\ge 1$
  \begin{align}
    \label{eq:it1:1}
    \norm{x^{(k+m)}-x^{(k)}}
    &\le \sum_{i=1}^m \norm{x^{(k+i)}-x^{(k+i-1)}}\\
    & \le \sum_{i=1}^m \rho^{i} \norm{x^{(k)}-x^{(k-1)}}\\
    & \le \sum_{i=1}^m \rho^{k+i-1} \norm{x^{(1)}-x^{(0)}}\\
    & \le \rho^k \sum_{i=0}^\infty \rho^i \norm{x^{(1)}-x^{(0)}}\\
    & \le \frac{\rho^k}{1-\rho} \norm{x^{(1)}-x^{(0)}}. 
  \end{align}
  Daher gilt das Cauchy-Kriterium: für alle $\epsilon>0$ gibt es ein
  $k_0\ge 0$, so dass für alle $k\ge k_0$ und alle $m\ge 1$ gilt, dass
  $\norm{x^{(k+m)}-x^{(k)}} < \epsilon$. Damit existiert also der
  Grenzwert $x^*$ und liegt wegen der Abgeschlossenheit in $M$. aus
  der Konvergenz der Folge folgt auch
  \begin{gather}
    \norm{f\left(x^{(k)}\right)-x^{(k)}} = \norm{x^{(k+1)}-x^{(k)}}
    \to 0,
  \end{gather}
  und damit im Limes $x^* = f(x^*)$. Für die Fehlerabschätzung
  beobachten wir, dass die Norm links in~\eqref{eq:it1:1} für
  $m\to\infty$ gegen $\norm{x^*-x^{(k)}}$ konvergiert, so dass mit
  demselben Argument wie dort die Abschätzungen gelten.
\end{proof}

\begin{Satz}{lokale-konvergenz}
  Sei $f\colon \R^n\to \R^n$ eine Selbstabbildung mit einem Fixpunkt
  $x^*$, so dass für eine Kugel vom Radius $R$ um $x^*$ mit $p>1$ gilt,
  \begin{gather}
    \norm*{f\left(x^{(k)}\right) - x^*}
    \le c \norm*{x^{(k)}-x^*}^p.
  \end{gather}
  Dann gibt es einen Radius $r\le R$, so dass die zugehörige
  Iterationsfolge für alle Startwerte $x^{(0)}\in B_r(x^*)$
  konvergiert.
\end{Satz}

\begin{proof}
  Sei $r$ so gewählt, dass
  \begin{gather}
    \rho = c \norm{x-x^*}^{p-1} < 1 \qquad\forall x\in B_r(x^*).
  \end{gather}
  Dann gilt $f\colon  B_r(x^*) \to  B_r(x^*)$ und
  \begin{gather}
    \norm{f(x)-x^*} \le \rho \norm{x-x^*} \qquad\forall x\in B_r(x^*).
  \end{gather}
  Für die Iterationsfolge gilt also
  \begin{gather}
    \norm{x^{(k)}-x^*}
    \le \rho^k \norm{x^{(0)}-x^*}
    \qquad\forall x\in B_r(x^*).
  \end{gather} 
\end{proof}

\begin{Satz}{optimize-solve}
  Sei $g\colon \R^n\to \R$ stetig differenzierbar. Dann gilt für eine
  Minimalstelle $x^*$ von $g$, also
  \begin{gather}
    x^* = \operatorname*{argmin}_{x\in\R^n} g(x),
  \end{gather}
  notwendig
  \begin{gather}
    \nabla g(x^*) = 0.
  \end{gather}
  Das Minimierungsproblem lässt sich also auf das Finden einer
  Nullstelle von $f\colon \R^n\to \R^n$ mit $f(x) = \nabla g(x)$
  reduzieren. Umgekehrt lässt sich die Aufgabe, eine Nullstelle der
  Funktion $f\colon \R^n \to \R^n$ zu finden, durch die Minimierung
  der Norm $g(x) = \norm{f(x)}$ darstellen.
\end{Satz}

\begin{proof}
  Die erste Tatsache ist aus der Analysis bekannt. Die zweite Aussage
  ergibt sich daraus, dass $g(x)$ nirgendwo negativ ist, eine
  Nullstelle also ein Minimum sein muss.
\end{proof}

\begin{remark}
  Beide Aussagen des vorigen Satzes lassen keine Umkehr zu: weder ist
  jede Nullstelle des Gradienten ein Minimum, noch ist jedes lokale
  Minimum von $g$ eine Nullstelle.
\end{remark}

\subsection{Vektor- und Matrixnormen}
\begin{Definition}{norm}
  Eine \define{Norm} $\norm{\cdot}$ auf dem Vektorraum $V$ ist eine Abbildung
  \begin{gather}
    \begin{split}
      \norm{\cdot}\colon V &\to \R\\
      x&\mapsto \norm{x}
    \end{split}
  \end{gather}
  mit den Eigenschaften
    \begin{xalignat}3
    &\text{Homogenität:}
    &\norm{\alpha x} &= \abs{\alpha}\norm{x}
    &\forall \alpha&\in\R,x\in V
    \\
    &\text{Dreiecksungleichung:}
    &\norm{x+y} &\le \norm{x}+\norm{y}
    &\forall x,y&\in V
    \\
    &\text{Definitheit:}
    &\norm{x} & \ge 0
    &\forall x&\in V
    \\
    &&\norm{x} &\neq 0
    &\forall x&\neq0
    \end{xalignat}
  Verzichtet man auf die zweite Definitheitsbedingung, so erhält man eine \define{Seminorm}.
\end{Definition}

\begin{Definition}{norm-aequivalenz}
  Sei $V$ ein reeller oder komplexer Vektorraum. Zwei Normen
  $\norm{\cdot}_X$ und $\norm{\cdot}_Y$ auf $V$ heißen äquivalent,
  wenn es Konstanten $c>0$ und $C>0$ gibt, so dass
  \begin{gather}
    c \norm{v}_X \le \norm{v}_Y \le C \norm{v}_X
    \qquad\forall v\in V.
  \end{gather}
\end{Definition}

\begin{Definition}{rn-konvergenz}
  Eine Folge $\{x^{(k)}\}\subset \R^n$ für $k=1,2,\dots$ heißt
  \textbf{komponentenweise konvergent} gegen $x\in \R^n$, wenn gilt
  \begin{gather}
    \forall \epsilon>0\;
    \exists k_0\in \mathbb N\;
    \forall k\ge k_0, i=1,\dots,n
    : \abs{x^{(k)}_i - x_i} < \epsilon.
  \end{gather}
  Die Folge heißt konvergent unter der Norm $\norm{\cdot}$ oder
  \define{normkonvergent} wenn gilt
  \begin{gather}
    \forall \epsilon>0\;
    \exists k_0\in \mathbb N\;
    \forall k\ge k_0
    : \norm{x^{(k)} - x} < \epsilon.
  \end{gather}
\end{Definition}

\begin{Lemma}{norm-aequivalenz}
  Sei $\norm{\cdot}$ eine beliebige Norm auf $\R^n$. Dann ist die Abbildung
  \begin{gather}
    f\colon x \mapsto \norm{x}
  \end{gather}
  stetig bezüglich der komponentenweisen Konvergenz. Ferner ist die
  Norm $\norm{\cdot}$ äquivalent zur Maximumsnorm.
\end{Lemma}

\begin{proof}
  Für den ersten Teil ist zu zeigen, dass zu einer komponentenweise
  konvergenten Folge von Vektoren auch deren Norm konvergiert. Sei
  $\{x^{(k)}\}$ eine solche Folge und dazu $k_0$ so gewählt, dass
  \begin{gather}
    \max_{i=1,\dots,n}\left\lvert\left(x_i^{(k)}-x_i\right) \norm{e_i}\right\rvert
      < \frac\epsilon n
    \qquad \forall k\ge k_0.
  \end{gather}
  Hier ist $e_i$ der $i$-te Einheitsvektor. Dann folgt
  \begin{align}
    \norm{x^{(k)}-x}
    &= \left\lVert\sum_{i=1}^n \left(x_i^{(k)}-x_i\right) e_i\right\rVert
    \\
    &\le \sum_{i=1}^n \left\lVert\left(x_i^{(k)}-x_i\right) e_i\right\rVert
    \\
    & < n \frac\epsilon n = \epsilon.
  \end{align}
  Hiermit haben wir bereits bewiesen, dass komponentenweise Konvergenz
  auch Normkonvergenz impliziert.

  Die \glqq{}Einheitssphäre\grqq{}
   \begin{gather}
     S = \bigl\{ x\in \R^n \big| \norm{x}_\infty = 1 \bigr\}
   \end{gather}
   ist beschränkt und bezüglich der komponentenweisen Konvergenz
   abgeschlossen. Die Norm $\norm.$ nimmt dort als stetige Funktion
   ihr Minimum $c$ und ihr Maximum $C$ an. Insbesondere gilt aber
   wegen der Definitheit $c > 0$. Für einen beliebigen Vektor
   $x\in \R^n$ ist $x/\norm{x}_\infty \in S$, so dass gilt
   \begin{gather}
     c \norm{x}_\infty \le \norm{x} \le C \norm{x}_\infty.
   \end{gather}
\end{proof}

\begin{Satz}{norm-aequivalenz}
  Auf $\R^n$ sind zwei beliebige Normen $\norm._X$ und $\norm._Y$ äquivalent.
\end{Satz}

\begin{proof}
  Nach dem vorherigen Lemma sind beide Normen äquivalent zur Maximumsnorm. Es gibt also Konstanten $c_X, c_Y, C_X, C_y>0$ mit
  \begin{gather}
    \begin{aligned}
     c_X \norm{x}_\infty &\le &\norm{x}_X \;&\le C_X \norm{x}_\infty\\
     c_Y \norm{x}_\infty &\le &\norm{x}_Y \;&\le C_Y \norm{x}_\infty.
    \end{aligned}
  \end{gather}
  Daher gilt
  \begin{gather}
    \begin{split}
      \norm{x}_Y \le C_Y\norm{x}_\infty &\le \frac{C_Y}{c_X} \norm{x}_X\\
      \norm{x}_X \le C_X\norm{x}_\infty &\le \frac{C_X}{c_y} \norm{x}_Y
    \end{split}
  \end{gather}
\end{proof}

\begin{Lemma}{norm-aequivalent-konvergenz}
  Sind zwei Normen $\norm{\cdot}_X$ und $\norm{\cdot}_Y$ äquivalent,
  so konvergiert die Folge $x^{(k)}$ in $\R^n$ bezüglich der Norm
  $\norm{\cdot}_X$ genau dann, wenn sie bezüglich der Norm
  $\norm{\cdot}_Y$ konvergiert.
\end{Lemma}

\begin{remark}
  Da die Definition der komponentenweisen Konvergenz der Konvergenz
  bezüglich der Maximumsnorm $\norm{\cdot}_\infty$ entspricht,
  konvergiert damit eine Folge $x^{(k)}$ in $\R^n$ genau dann
  komponentensweise, wenn sie in einer beliebigen Norm konvergiert.

  Diese Aussage überträgt sich \emph{nicht} auf die
  Kontraktionseigenschaft! Eine lineare Abbildung $x\mapsto Ax$ kann
  sehr wohl eine Kontraktion bezüglich einer Norm sein, aber nicht
  einer anderen.
\end{remark}

\begin{Aufgabe}{konvergenz-aequivalenz}
  Zeigen Sie \slideref{Lemma}{norm-aequivalent-konvergenz}. Zeigen Sie
  die Aussage über Kontraktionen in der vorigen Bemerkung durch ein
  Gegenbeispiel.
\end{Aufgabe}

\begin{Definition}{matrix-norm}
  Auf dem Vektorraum der Matrizen $\R^{m\times n}$ ist durch
  \slideref{Definition}{norm} eine Norm definiert. Gilt zusätzlich
  \begin{gather}
    \norm{Ax} \le \norm{A}\norm{x}
    \qquad\forall A\in \R^{m\times n}, x\in \R^n,
  \end{gather}
  so heißt die Norm $\norm.$ der Matrix \define{verträglich} mit der
  Vektornorm $\norm.$. Wir sprechen von einer \define{Matrixnorm},
  wenn sie zusätzlich \define{submultiplikativ} ist, dass heißt,
  für alle Matrizen $A,B$ passender Dimensionen gilt
  \begin{gather}
    \norm{AB} \le \norm{A}\norm{B}.
  \end{gather}
  Ferner definieren wir die \define{Operatornorm} oder \define{natürliche Norm}
  \begin{gather}
    \norm{A} = \sup_{\substack{x\in\R^n\\x\neq0}} \frac{\norm{Ax}}{\norm{x}}
    = \sup_{\norm{x}=1}\norm{Ax}.
  \end{gather}
\end{Definition}

\begin{remark}
  In der Definition der Operatornorm wird das Supremum über alle von
  null verschiedenen Vektoren gebildet. Andererseits ist der Quotient
  aber für $x=0$ offensichtlich sinnlos. Um die Notation einfach zu halten sei daher im folgenden vereinbart, dass im Ausdruck
  \begin{gather}
    \sup_{x\in\R^n}\frac{\norm{Ax}}{\norm{x}}
  \end{gather}
  der Wert $x=0$ implizit ausgenommen sei.
\end{remark}

\begin{Lemma}{operator-norm}
  Die Operatornorm ist verträglich zu ihrer Vektornorm und submultiplikativ.
\end{Lemma}

\begin{proof}
  Sei $\norm{x}$ die gewählte Norm in $\R^n$ und $\norm{A}$ die
  zugehörige Operatornorm.  Die Verträglichkeit folgt aus
  \begin{gather}
    \norm{Ax} = \frac{\norm{Ax}}{\norm{x}}\norm{x}
    \le \sup_{x\in\R^n}\frac{\norm{Ax}}{\norm{x}}\norm{x}
    = \norm{A}\norm{x}.
  \end{gather}

  Für die Submultiplikativität folgt aus der Verträglichkeit
  \begin{gather}
    \norm{AB} = \sup_{x\in\R^n}\frac{\norm{ABx}}{\norm{x}}
    \le \sup_{x\in\R^n}\frac{\norm{A}\norm{Bx}}{\norm{x}}
    = \norm{A}\sup_{x\in\R^n}\frac{\norm{Bx}}{\norm{x}}
    = \norm{A}\norm{B}
  \end{gather}
\end{proof}

\begin{Beispiel}{Zeilensummen}
  Die Operatornormen zu den Vektornormen $\norm._1$ und
  $\norm._\infty$ sind die \define{Spaltensummennorm} und die
  \define{Zeilensummennorm}
  \begin{align}
    \norm{A}_1 &= \max_{i=1,\dots,n} \sum_{j=1}^n \abs{a_{ji}}\\
    \norm{A}_\infty &= \max_{i=1,\dots,n} \sum_{j=1}^n \abs{a_{ij}}
  \end{align}
\end{Beispiel}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
