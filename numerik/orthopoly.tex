\section{Polynomräume}

% \begin{Satz}{nullstellen}
%   Ein reelles Polynom vom Grad $n$ hat höchstens $n$ Nullstellen oder es ist das Nullpolynom.
% \end{Satz}

% \begin{proof}
%   Für $n=1$ handelt es sich um ein lineares Polynom und die Aussage
%   des Satzes ist unmittelbar klar. Sei nun $p$ ein Polynom strikt vom
%   Grad $n>1$ mit Nullstelle $x_0$. Dann gibt es nach dem euklidischen
%   Algorithmus zur Division mit Rest ein Polynom $q$ vom Grad $n-1$ und
%   eine Konstante $c$, so dass
%   \begin{gather}
%     p(x) = (x-x_0)q(x)+c.
%   \end{gather}
%   Daraus folgt $p(x_0) = c$, so dass folgt $c=0$. Wir können dieses
%   Verfahren für alle weiteren Nullstellen $x_1,\dots,x_m$ wiederholen
%   und erhalten
%   \begin{gather}
%     p(x) =  r(x) \prod_{k=0}^m (x-x_i),
%   \end{gather}
%   wobei $r(x)$ ein Polynom vom Grad $n-m$ sein muss, da $p$ vom Grad
%   $n$ ist. Insbesondere muss gelten $m\le n$.
% \end{proof}

% \begin{Korollar}{polynome-identisch}
%   Zwei reelle Polynome vom Grad $n$ sind identisch, wenn sie in
%   mindestens $n+1$ Punkten übereinstimmen. 
% \end{Korollar}


\begin{Lemma}{monome-linear-unabhaengig}
  Die Menge der Monome $\{x^0, x^1,\dots,x^n\}$ ist linear unabhängig.
\end{Lemma}

\begin{proof}
  Sei $p$ ein Polynom vom Grad $n$, also
  \begin{gather}
     p(x) = a_nx^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0
   \end{gather}
   $p$ ist also gerade eine Linearkombination der Monome.  Zu zeigen
   ist, dass aus der Eigenschaft $p \equiv 0$ folgt, dass alle
   Koeffizienten verschwinden, also
  \begin{gather}
    p(x) \equiv 0
    \quad\Rightarrow\quad a_n = \dots = a_0 = 0.
  \end{gather}
  Zu diesem Zweck berechnen wir die $n$-te Ableitung von $p$ und
  erhalten, da mit $p$ auch alle seine Ableitungen identisch
  verschwinden,
  \begin{gather}
    n! a_n = 0.
  \end{gather}
  Daraus schließen wir $a_n = 0$. Nun gilt für die $(n-1)$-te Ableitung
  \begin{gather}
    n! a_n x + (n-1)! a_{n-1} = (n-1)! a_{n-1} = 0.
  \end{gather}
  Auf diese Weise schließen wir rekursiv bis $a_0$, dass alle Koeffizienten verschwinden. Damit ist das Lemma bewiesen.
\end{proof}

\begin{Satz}{polynomraum}
  Die Polynome vom maximalen Grad $n$ bilden einen Vektorraum der
  Dimension $n+1$.  Wir bezeichnen ihn mit $\P_n$.
\end{Satz}

\begin{proof}
  Es ist leicht nachzurechnen, dass sowohl die Summe, als auch reelle
  Vielfache von Polynomen wieder Polynome sind. Insbesondere erhöhen
  beide Operationen den Grad nicht. Damit ist $\P_n$ ein
  Vektorraum. Er wird per definitionem von den Monomen vom Grad bis zu
  $n$ erzeugt. Da diese nach
  \slideref{Lemma}{monome-linear-unabhaengig} linear unabhängig sind,
  bilden sie eine Basis und die Dimension von $\P_n$ ist $n+1$.
\end{proof}

\begin{Quiz}{Polynomräume}
  Gegeben beliebige Werte $x_j\in\R$ mit $j=1,\dots,n$. Die Menge der
  Polynome $p_i$ definiert durch
  \begin{align*}
    p_0(x) &= 1\\
    p_i(x) &= \prod_{j=1}^i (x-x_j),\qquad i=1,\dots,n
  \end{align*}
  \begin{enumerate}[A]
  \item ist linear unabhängig
  \item ist linear abhängig
  \item ist ein Erzeugendensystem für $\P_n$
  \item ist eine Basis von $\P_n$
  \end{enumerate}
\end{Quiz}
\section{Skalarprodukt und Orthogonalität}
\begin{Definition}{skalarprodukt}
  Sei $V$ ein reeller Vektorraum. Eine Abbildung
  $a\colon V \times V \to \R$ heißt \define{Bilinearform}, wenn für
  $u,v,w\in V$ und $\lambda,\mu\in \R$ gilt
  \begin{align}
    a(\lambda u + \mu v,w) &= \lambda a(u,w) + \mu a(v,w)\\
    a(w,\lambda u + \mu v) &= \lambda (w,u) + \mu a(w,v).
  \end{align}
  Eine Bilinearform heißt \define{symmetrisch}, wenn für $u,v\in V$ gilt
  \begin{gather}
    a(u,v) = a(v,u).
  \end{gather}
  Sie heißt \define{positiv semi-definit}, wenn $a(u,u) \ge 0$ für alle
  $u\in V$ und \define{positiv definit}, wenn zusätzlich
  \begin{gather}
    a(u,u) = 0 \quad \Longrightarrow \quad u=0.
  \end{gather}
  Eine symmetrische, positiv definite Bilinearform heißt
  \define{Skalarprodukt}, in der Regel notiert als $\scal(\cdot,\cdot)$.
\end{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{Lemma*}{bcs}{Bunjakowski-Cauchy-Schwarzsche Ungleichung}
  Sei $\scal(\cdot,\cdot)$ ein Skalarprodukt auf $V$.  Für zwei beliebige Elemente $u,v\in V$ gilt
  \begin{gather}
    \abs{\scal(u,v)} \le \sqrt{\scal(u,u)} \, \sqrt{\scal(v,v)}.
  \end{gather}
  Gleichheit gilt genau dann, wenn $u$ und $v$ kollinear sind, also
  $v=\alpha u$ mit einem skalaren Faktor $\alpha$.
\end{Lemma*}

\begin{proof}
  Zunächst zeigen wir nur die Ungleichung: Für $v=0\in V$ ist sie
  offensichtlich.
  
  Seien nun $v,u \in V$ keine Nullvektoren. Für beliebige $\mu, \lambda \in \R$
  gilt wegen der Bilinearität 
  \begin{gather}
   0 \le \scal(\lambda u + \mu v,\lambda u +  \mu v)
    = \lambda^{2} \scal(u,u)+2 \mu \lambda \scal(u,v) +\mu^{2} \scal(v,v)
  \end{gather}
  Setze $\lambda := \scal(v,v) \neq 0$
  \begin{gather}
   0 \le \scal(v,v)^{2} \scal(u,u) + 2\mu \scal(v,v)\scal(u,v) +\mu^{2}\scal(v,v)
  \end{gather}
  Dividiere durch$\scal(v,v)$
  \begin{gather}
   0 \le \scal(v,v) \scal(u,u) + 2\mu \scal(u,v) +\mu^{2}
  \end{gather}
  Setze nun $\mu := -\scal(u,v)$
  \begin{gather}
    0 \le \scal(v,v) \scal(u,u) -2\scal(u,v)^{2}+\scal(u,v)^{2}
  \end{gather}
  Daraus folgt
  \begin{gather}
    \scal(u,v)^{2} \le \scal(u,u) \scal(v,v)
  \end{gather}
  und mit der Monotonie der Quadratfunktion die Ungleichung.

  Nun bleibt die Äquivalenz für die Gleichheit zu zeigen.
  Für $v=0$ ist dies wieder trivial erfüllt. Seien zunächst $u,v$ linear abhängig, also zum Beispiel $u=av$.
  Dann gilt mit der Abkürzung $f(v) = \sqrt{\scal(v,v)}$
  \begin{gather}
    \abs{\scal(u,v)} = \abs{\scal(av,v)}
    = \abs{a} \cdot f(v) \cdot f(v)
    = f(av) \cdot f(v) =f(u) \cdot f(v).
  \end{gather}

  Gelte nun umgekehrt $\scal(u,v) = \sqrt{\scal(u,u)}\sqrt{\scal(v,v)}$.
  Es folgt
  \begin{gather}
     \scal(v,v) \scal(u,u) -2\scal(u,v)^{2}+\scal(u,v)^{2} = 0.
  \end{gather}
  Setze $\mu = \scal(u,v)\neq 0 $ und
  $\lambda = \scal(v,v)\neq 0$. Dann erhält man
  \begin{gather}
    \lambda \scal(u,u) - 2 \mu \scal(u,v) + \mu^2 = 0.
  \end{gather}
  Multipliplikation mit $\scal(v,v)$ ergibt
  \begin{gather}
   \lambda^2 \scal(u,u)+2\mu \scal(u,v)\scal(v,v) +\mu^{2}\scal(v,v) = 0 = \scal(\lambda u-\mu v,\lambda u-\mu v).
  \end{gather}
  
  Wegen der Definitheit folgt nun
  $\lambda u + \mu v = 0$ und da $\mu$ und $\lambda$ ungleich Null sind gilt,
  dass $ u,v$ linear abhängig sind
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{Lemma}{hilbertnorm}
  Sei $V$ ein reeller Vektorraum mit Skalarprodukt
  $\scal(\cdot,\cdot)$. Dann ist durch
  \begin{gather}
    \norm{u} = \sqrt{\scal(u,u)}
  \end{gather}
  auf $V$ eine Norm definiert. Ein reeller Vektorraum $V$ mit
  Skalarprodukt und zugehöriger Norm heißt \define{euklidischer
    Vektorraum}.
\end{Lemma}

\begin{proof}
  Das Skalarprodukt ist nicht negativ, daher ist die Abbildung $\norm{\cdot}\colon V \to \R$ wohldefiniert.
  Wir müssen nun die Normeigenschaften nachrechnen. Sei dazu $u \in V$. Es gilt
  \begin{enumerate}
  \item Nichtnegativität und Definitheit folgen sofort aus den entsprechenden Eigenschaften des Skalarprodukts.
  \item Homogenität
  \begin{gather}
    \norm{\lambda u} = \sqrt{\scal(\lambda u,\lambda u)}
    =\sqrt{\lambda^{2}\scal(u,u)}
    = \abs{\lambda}\sqrt{\scal(u,u)}
    =\abs{\lambda}\norm{u}
  \end{gather}
  \item Deiecksungleichung
  \begin{gather}
    \begin{aligned}
      \norm{u+v}^{2}
      &= \scal(u+v,u+v)\\
      &= \scal(u,u)+ 2 \scal(u,v) + \scal(v,v)\\
      \label{eq:orthopoly:1}
      &\le \scal(u,u)+ 2 \norm{u} \, \norm{v} + \scal(v,v)\\
      &=\norm{u}^{2}+ 2 \norm{u} \, \norm{v}+ \norm{v}^{2}\\
      & =(\norm{u}+\norm{v})^{2}\\
    \end{aligned}
  \end{gather}
  Daraus folgt durch Wurzelziehen auf beiden Seiten $\norm{u+v} \le \norm{u}+\norm{v}$.
  Für die Abschätzung in Zeile~\eqref{eq:orthopoly:1} haben wir die
  Bunyakovsky-Cauchy-Schwarz-Ungleichung aus \slideref{Lemma}{bcs} verwendet.
  \end{enumerate}
\end{proof}

\begin{remark}
  Ein reeller Vektorraum $V$ mit
  Skalarprodukt und zugehöriger Norm heißt \define{euklidischer
    Vektorraum}.
\end{remark}

\begin{Lemma*}{l2-norm}{$L^2$-Skalarprodukt}
  Auf dem Raum $V=\P_n$ der reellen Polynome vom Grad bis zu $n$ ist durch
  \begin{gather}
    \scal(p,q) = \int_{-1}^1 p(x)q(x)\dx
  \end{gather}
  ein Skalarprodukt definiert.
\end{Lemma*}

\begin{proof}
  Hier gilt es zu prüfen, ob die Abbildung auch die vier Eigenschaften eines
  Skalarprodukts erfüllt.\\
  Seien  $p,q,g \in \P_n$ in diesem Beweis.\\
  Da wir schon von einem Skalarprodukt ausgehen, empfiehlt es sich
  zuerst die Symmetrie zu zeigen.
  \begin{gather}
    \scal(p,q) =  \int_{-1}^1 p(x)q(x)\dx = \int_{-1}^1 q(x)p(x)\dx
    =\scal(q,p)
  \end{gather}
  Wenn wir nun zeigen, dass es eine Bilinearform ist müssen wir nur noch eine
  Identität zeigen, da wir schon wissen, dass die Symmetrieeigenschaft
  erfüllt ist.
  \begin{gather}
    \begin{aligned}
    \scal(\lambda p + \mu q, g)
    &= \int_{-1}^1 (\lambda p(x)+ \mu q(x))g(x)\dx\\
   & = \int_{-1}^1 \lambda p(x)g(x)+ \mu q(x)g(x)\dx \\
   &= \int_{-1}^1 \lambda  p(x)g(x)\dx + \int_{-1}^1 \mu q(x)g(x)\dx \\
   &= \lambda \int_{-1}^1 p(x)g(x)\dx + \mu  \int_{-1}^1 q(x)g(x)\dx \\
   &= \lambda \scal(p,g) + \mu \scal(q,g)
    \end{aligned}
  \end{gather}
  Da wir die Symmetrie vorher gezeigt haben, gilt Linearität auch
  im zweiten Argument.\\
  
  Als letztes zeigen wir, dass die Abbildung positiv definit ist.
  \begin{gather}
    0 = \scal(p,p) = \int_{-1}^1 p(x)p(x)\dx =\int_{-1}^1 p(x)^{2}\dx
  \end{gather}
  Aus den Integraleigenschaften folgt
  \begin {gather}
    0 = p(x)^{2} \quad \forall x
  \end{gather}
  Dies kann nur der Fall sein, wenn $p \equiv 0$ ist.\\
  Somit haben wir nachgerechnet, dass es sich um Skalarprodukt handelt.
  \end{proof}
    
  
\begin{Definition}{orthogonal}
  Zwei Vektoren $u,v\in V$ heißen \define{orthogonal}, wenn
  \begin{gather}
    \scal(u,v) = 0.
  \end{gather}
  Ein Vektor $u\in V$ ist orthogonal zum Untervektorraum $W\subset V$, wenn
  \begin{gather}
    \scal(u,v) = 0\quad\forall v\in W.
  \end{gather}
\end{Definition}

\begin{Notation}{euklidischer-vr}
  Von nun an bezeichnet $V$ immer einen endlichdimensionalen, reellen,
  euklidischen Vektorraum.
\end{Notation}

\begin{Lemma*}{pythagoras}{Pythagoras}
  Seien zwei Vektoren $u\in V$ und $v\in V$ orthogonal zueinander. Dann gilt
  \begin{gather}
    \norm{u+v}^{2} = \norm{u}^{2} + \norm{v}^{2}
  \end{gather}
\end{Lemma*}

\begin{proof}
  Seien $u,v \in V$. Es gilt $ 0 = \scal(u,v)$
   \begin{gather}
    \norm{u+v}^{2} = \scal(u+v,u+v)
    %=\scal(u+v,u)+\scal(u+v,v)
    %=\scal(u,u)+\scal(v,u)+\scal(u,v)+\scal(v,v)
    =\norm{u}^{2} + \norm{v}^{2} +2\scal(u,v) = \norm{u}^{2} + \norm{v}^{2}
  \end{gather}
\end{proof}

\section{Bestapproximation und orthogonale Projektion}
\begin{Definition}{bestapproximation}
  Sei $A\subset V$ ein affiner Unterraum eines euklidischen
  Vektorraums. Dann ist die Bestapproximation $u_b\in A$ eines Vektors
  $u\in V$ in $A$ definiert durch die Beziehung
  \begin{gather}
    \norm{u-u_b} = \min_{v\in A} \norm{u-v}.
  \end{gather}
\end{Definition}

\begin{Satz}{bestapproximation}
  Sei $w \in V$ und $W \subset V$.
  Sei $A=w+W$ ein nichtleerer, affiner Unterraum von $V$. Dann
  existiert die Bestapproximation nach
  \slideref{Definition}{bestapproximation} und ist eindeutig
  bestimmt. Es gilt die notwendige und hinreichende Bedingung
  \begin{gather}
    \scal(u-u_b,v) = 0 \quad \forall v\in W.
  \end{gather}
  Das heißt $ u_b$ ist Bestapproximation genau dann wenn $u-u_b$
  orthogonal zu $W$ bzgl. des Skalarprodukts $\scal(\cdot,\cdot)$ ist.
\end{Satz}

\begin{proof}
  Der Beweis gliedert sich in drei Teile. Zuert wird die Äquivalenz
  gezeigt danach zeigen wir die Eindeutigkeit und zum Schluss
  erst die Existenz.\\ \\
 $\glqq \Rightarrow \glqq$
  Sei $ u_b \in A$ die Bestapproximation des Vektors $ u \in V$\\
  Wir defnieren nun eine Funktion:
  \begin{gather}
    F_v(x):= \norm{u-u_b-xv}^{2}, x \in \R,  v\in A
  \end{gather}

  Diese Funktion besitzt ein Minimum bei x=0. Folglich gilt
  \begin{gather}
    \left. \frac{d}{dx} F(x) \right|_{x=0}
    =\left. \frac{d}{dx}\norm{u-u_b-xv}^{2} \right|_{x=0}=0
  \end{gather}
    
  Dies kann weiter umgeformt werden zu
  $\scal(u-u_b-xv,v)|_{x=0}=0 \ \forall v\in A$ und folglich zu
  \begin{gather}
   \scal(u-u_b,v)=0 \ \forall v\in A
  \end{gather}

  $\grqq \Leftarrow \glqq$
  Nun erfüllt $u_b\in A$ die Bedingung.\\
  Dann gilt mit einem beliebigen $v\in A$:
  \begin{gather}
   \norm{u-u_b}^{2}=\scal(u-u_b,u-u_b)\\
   = \scal(u-u_b,u-v)+\scal(u-u_b,v-u_b)\\
   \le \norm{u-u_b}\cdot\norm{u-v}\\
  \end{gather}
  Daraus folgt $\norm{u-u_b} \le \inf_{v\in A}{\norm{u-v}}$\\
  Damit erfüllt $u_b$ eben die Definiton der Bestapproximation\\ \\
  Nun zur Eindeutigkeit:\\
  Seien $u_b$ und $u_d \in$ A zwei Bestapproximationen.
  Dann gilt notwendigerweise
  \begin{gather}
   \scal(u-u_b,v) = 0 = \scal(u-u_d,v) \quad \forall v\in A
  \end{gather}
  Dies wird umgeformt zu
  \begin{gather}
  \scal(u-u_d,v)-\scal(u-u_b,v)=0 \quad  \forall v\in A \\
  \scal(u_b-u_d,v) = 0 \quad \forall v \in A
  \end{gather}
  Wähle nun $v:=u_b-u_d \in A$. Dies ergibt
  $\norm{u_b-u_d}^{2} =0$ und somit folgt $u_b = u_d$\\
  Die Existenz:\\
  Der endliche dimensionale Teilraum A$\subseteq$V besitzt eine Basis
  $(b_1,\dots, b_n)$ mit $n:=dim V$. Die gesuchte Approximation
  $u_b\in A$ lässt sich
  durch die Basis in folgender Form darstellen
  \begin{gather}
   u_b = \sum_{k=1}^n a_k b_k
  \end{gather}
  Dies wird in die notwendige Orthogonalitätsbedingung
  \slideref{Satz}{bestapproximation} eingesetzt.
  \begin{gather}
   \scal(u-\sum_{k=1}^n a_k b_k,v)=\scal(u,v)-\sum_{k=1}^n a_k\scal(b_k,v)=0
   \quad \forall v\in A
   \end{gather}
 Dies ist bei der Wahl von $v:=b_i \quad i=1,\dots,n$ äquivalent zu dem
 linearen $n$x$n$ Gleichungssystem.
 \begin{gather}
   \sum_{k=1}^n\scal(b_k,b_i) a_k= \scal(u,b_i) \quad i=1,\dots,n
 \end{gather}
 Definiere nun $A,x,b$ wie folgt
 \begin{gather}
  A:=(\scal(b_k,b_i))_{i,k=1}^n \quad x:=(a_k)_{k=1}^n\quad b:=(\scal(u,b_i))_{i=1}^n
 \end{gather}
 Dadurch lässt sich das LGS in der Form $Ax=b$ schreiben.
 Betrachte nun folgendes
 \begin{gather}
  x^{T}Ax =\sum_{i,k=1}^n a_i a_k\scal(b_k,b_i)=\scal(u_b,u_b)\ge 0
 \end{gather}
 $A$ ist folglich positiv definit. Das Gleichungssystem $Ax=b$ ist also für
 jede rechte Seite $b$, das heißt für jedes $u \in V$ eindeutig lösbar.
 Folglich bestimmt die Orthogonalitätsbedingung eindeutig ein Element
 $u_b \in A$, welches dann die Bestapproximation von $u$ ist.
 
\end{proof}

\begin{Definition}{komplement-projektion}
  Sei $W \subset V$ ein Untervektorraum. Dann gilt
  $V = W \oplus W^\perp$, wobei das \define{orthogonale Komplement}
  $W^\perp$ eindeutig definiert ist durch
  \begin{gather}
    W^\perp = \bigl\{ v\in V \big| \scal(v,w) = 0 \quad\forall w\in W\bigr\}.
  \end{gather}
  Die Lösung der Bestapproximationsaufgabe bezeichnen wir mit
  \begin{gather}
    \Pi_W u = u_b\in W
  \end{gather}
  und nennen es die \define{orthogonale Projektion} von $u\in V$ auf $W$.
\end{Definition}

\begin{Lemma}{komp-projekt-wohldefiniert}
  Das orthogonale Komplement und die orthogonale Projektion sind wohldefiniert.
\end{Lemma}

\begin{proof}
  \slideref{Satz}{bestapproximation}.
\end{proof}

\begin{Beispiel}{polynom-bestapproximation}
  Die Aufgabe der Gaußschen Ausgleichsrechnung lautet: finde zu einer
  gegebenen Funktion $f$ das Polynom vom Grad höchstens $n$, das auf
  dem Intervall $[-1,1]$ den mittleren quadratischen Abstand
  minimiert, also $p\in \P_n$ mit
  \begin{gather}
    \int_{-1}^1 \bigl(f(x)-p(x)\bigr)^2 \dx
    = \min_{q\in \P_n} \int_{-1}^1 \bigl(f(x)-q(x)\bigr)^2 \dx.
  \end{gather}
  Die Lösung erfüllt
  \begin{gather}
    \int_{-1}^1 p(x)q(x) \dx = \int_{-1}^1 f(x)q(x) \dx
    \qquad\forall q\in \P_n.
  \end{gather}
\end{Beispiel}

\section{Orthogonale Basen}

\begin{Lemma}{gram-system}
  Wählt man eine Basis $\{\phi_i\}$ für $W$, so transformiert wird die
  Orthogonalitätsbedingung in \slideref{Satz}{bestapproximation} zum
  linearen Gleichungssystem
  \begin{gather}
    \matg\vx = \vb.
  \end{gather}
  Hier sind $\vx$ der Koeffizientenvektor der Lösung $u_b$, $\matg$ die
  \define{Gramsche Matrix} und $\vb$ die rechte Seite gegeben durch
\begin{gather}
  g_{ij} = \scal(\phi_i,\phi_j), \qquad
  b_i = \scal(u,\phi_i).
\end{gather}
\end{Lemma}

\begin{remark}
  Das Gleichungssystem hängt nur von der Wahl einer Basis in $W$ ab,
  nicht in $V$.
\end{remark}

\begin{Definition}{ortho-system}
  Eine Menge von Vektoren $\{\phi_1,\dots,\phi_n\}\subset V$ bildet
  ein \define{Orthogonalsystem}, wenn
  \begin{gather*}
    \scal(\phi_i,\phi_j) = 0
    \qquad \forall 1\le i < j \le n.
  \end{gather*}
  Sie ist ein \define{Orthonormalsystem}, wenn zusätzlich
  $\norm{\phi_i} = 1$ für alle Elemente gilt. Ein Orthonormalsystem, das eine Basis bildet, heißt \define{Orthonormalbasis} (\define{ONB}).
\end{Definition}

\begin{Lemma}{ortho-lu}
  Jedes Orthogonalsystem ist linear unabhängig.
\end{Lemma}

\begin{Lemma*}{parseval}{Parsevalsche Gleichung}
  Sei $\{\phi_i\}$ für $i=1,\dots,n$ eine ONB von $V$. dann gilt für
  jedes $v\in V$ mit der Basisdarstellung
  \begin{gather}
    v = \sum_{i=1}^n x_i \phi_i
  \end{gather}
  die Identität
  \begin{gather}
    \norm{v}^2 = \sum_{i=1}^n x_i^2.
  \end{gather}
\end{Lemma*}
\begin{Lemma}{least-squares-orthogonal}
  Bezüglich einer ONB ist die Gramsche Matrix die
  Einheitsmatrix. Damit berechnen sich die Einträge des
  Koeffizientenvektors $\vx$ in \slideref{Lemma}{gram-system} durch
  die einfache Formel
  \begin{gather}
    x_i = b_i = \scal(u,\phi_i).
  \end{gather}
\end{Lemma}

\begin{Theorem*}{gram-schmidt}{Gram-Schmidt-Verfahren}
  Jede linear unabhängige Menge von Vektoren
  $\{v_1,\dots,v_n\}\subset V$ wird mit dem folgenden Verfahren in ein
  Orthonormalsystem $\{\phi_1,\dots,\phi_n\}\subset V$ umgeformt:
  \begin{gather}
    \begin{aligned}
      \phi_1 &= \tfrac1{\norm{v_1}} \,v_1\\
      w_j &= v_j - \sum_{i=1}^{j-1} \scal(v_j,\phi_i)\,\phi_i
      & \quad \phi_j &= \tfrac1{\norm{w_j}}\, w_j
      &\quad j=2,\dots,n
    \end{aligned}
  \end{gather}
  Für alle $1\le k \le n$ gilt
  \begin{gather}
    \operatorname{span}\{\phi_1,\dots,\phi_k\}
    =
    \operatorname{span}\{v_1,\dots,v_k\}
  \end{gather}
\end{Theorem*}

\begin{proof}
  Per Induktion über $n$ zeigen wir Orthogonalität und Normierung.\\

  $Indukionsanfang$ Sei $n=1$.\\
  Wird nur ein Vektor aus dem Raum gewählt, so erfüllt dieser
  die Orthogonalitätsbedingung, da er der einzige Vektor im System ist.
  Wird dieser Vektor zusätzlich normiert erhält man ein Orthonormalsystem.\\
  
  $Induktionsschritt$ Das Verfahren gelte
  für $\{v_1,\dots,v_{n-1}\}$ Vektoren aus V. \\
  $n-1 \rightarrow n$\\
  Sei $(\phi_1,\dots,\phi_{n-1})$ ein Orthonormalsystem\\
  Annahme $w_n$ nicht wohldefiniert. Dann gilt
  \begin{gather}
    w_n = v_n -\sum_{i=1}^{n-1}\scal(v_n,\phi_i)\,\phi_i = 0
  \end{gather}
  In diesem Fall sind $(v_1,\dots,v_n)$ linear abhängig.
  Das ist ein Widerspruch zur Voraussetzung,
  dass $(v_1,\dots,v_n)$ linear unabhängig sind.\\
  $w_n$ wird nun normiert über $\frac{1}{\norm{w_n}} \cdot w_n =\phi_n $.
  Nun zur Orthogonalität:
  \begin{gather}
    \scal(\phi_n,\phi_j)=\scal(v_n,v_j)-
    \sum_{i=1}^{n-1}\scal(v_n,\phi_i)
    \,\underbrace{\scal(\phi_i,\phi_j)}_{=\delta_{ij}}  = 0
    \quad j=1,\dots,n-1
  \end{gather}
\end{proof}

\begin{Algorithmus*}{gram-schmidt}{Gram-Schmidt}
  \lstinputlisting{code/gram-schmidt.py}
\end{Algorithmus*}

\begin{Beispiel}{gram-schmidt}
  Wir wählen für Polynome das $L^2$-Skalarprodukt aus
  \slideref{Lemma}{l2-norm} und die Basis $\{1,x,\dots,x^{n-1}\}$
  für $\P_{n-1}$. Wir verwenden die Iplementation in
  \slideref{Algorithmus}{gram-schmidt} und messen den Erfolg nach der
  Größe der Nebendiagonaleinträge der Gramschen Matrix.
  \begin{center}
    \begin{tabular}{c|c}
      $n$ & $\max_{i\neq j} \abs{g_{ij}}$ \\
      \hline
      5 & $8.9\cdot 10^{-16}$ \\
      10 & $9.1\cdot 10^{-12}$ \\
      15 & $1.2\cdot 10^{-7}$ \\
      20 & $0.23$
    \end{tabular}
  \end{center}
\end{Beispiel}

\begin{Algorithmus*}{mgs}{Modifizierter Gram-Schmidt}
  \lstinputlisting{code/modified-gram-schmidt.py}  
\end{Algorithmus*}

\begin{Beispiel}{gs-mgs}
  In dieser Tabelle wiederholen wir die Zahlen
  $\max_{i\neq j} \abs{g_{ij}}$ aus \slideref{Beispiel}{gram-schmidt}
  und stellen sie den entsprechenden Ergebnissen des modifizierten
  Verfahrens in \slideref{Algorithmus}{mgs} gegenüber.
  \begin{center}
    \begin{tabular}{c|cc}
      $n$ &  Gram-Schmidt & modifiziert\\
      \hline
      5 & $8.9\cdot 10^{-16}$ & $1.3\cdot 10^{-16}$ \\
      10 & $9.1\cdot 10^{-12}$ & $2.9\cdot 10^{-12}$ \\
      15 & $1.2\cdot 10^{-7}$ & $2.7\cdot 10^{-9}$ \\
      20 & $0.23$ & $3.9\cdot 10^{-5}$
    \end{tabular}
  \end{center}
\end{Beispiel}

\begin{remark}
  Wir sehen, dass die Wahl der Implementation eines Rechenverfahrens
  bei mathematischer Äquivalenz durchaus erheblichen Einfluss auf das
  Ergebnis haben kann. Dieses Phänomen werden wir in
  \Cref{sec:stability} näher untersuchen. Zunächst diskutieren wir
  aber eine weitere Variante der Erzeugung orthogonaler Basen in
  Polynomräumen.
\end{remark}

\section{Drei-Term-Rekursion}

\begin{Satz*}{dreiterm}{Dreiterm-Rekursion}
  Zu jedem Skalarprodukt $\scal(\cdot,\cdot)$ auf dem Raum der
  stetigen Funktionen gibt es genau eine Folge von orthogonalen
  Polynomen $p_k\in \P_k$ mit führendem Koeffizienten eins. Sie
  genügen der Dreiterm-Rekursionsformel
  \begin{gather}
    p_k(x) = (x-a_k)p_{k-1}(x) - b_k p_{k-2}(x),
    \qquad k=1,2,\ldots
  \end{gather}
  mit Startwerten $p_{-1} \equiv 0$ und $p_0 \equiv 1$. Die
  Koeffizienten sind
  \begin{gather}
    a_k = \frac{\scal(x p_{k-1},p_{k-1})}{\scal(p_{k-1},p_{k-1})}
    \qquad\text{und}\qquad
    b_k = \frac{\scal(p_{k-1},p_{k-1})}{\scal(p_{k-2},p_{k-2})}.
  \end{gather}
\end{Satz*}

\begin{proof}
  Siehe \cite[Satz 6.2]{DeuflhardHohmann08}
\end{proof}

\begin{Bemerkung}{dreiterm-normierung}
  Der Beweis ergibt, eigentlich die ``Eindeutigkeit einer Orthogonalfolge bis auf Normierung''. Tatsächlich werden in der Literatur immer wieder veschiedene Normierungen benutzt. Beispiele sind:
  \begin{enumerate}
  \item Führender Koeffizient eins, $p_k = x^k + \dots$
  \item $\norm{p_k} = 1$
  \item $p_k(1) = 1$
  \end{enumerate}
\end{Bemerkung}

\begin{Definition}{legendre-polynome}
  Die \define{Legendre-Polynome} $L_k$ sind definiert durch
  die Dreiterm-Rekursion
  \begin{gather}
    L_{k} = \tfrac{2k-1}{k}x L_{k-1}(x) - \tfrac{k-1}{k} L_{k-2}(x).
  \end{gather}
  Sie sind orthogonal bezüglich des $L^2$-Skalarprodukts in
  \slideref{Lemma}{l2-norm}.
\end{Definition}

\begin{Beispiel}{least-squares-legendre}
  Das Problem der Gaußschen Ausgleichsrechnung war: zu einer gegebenen
  Funktion $f$ finde $p\in \P_n$, so dass
  \begin{gather}
    \int_{-1}^1 (f-p)^2 \dx
    = \min_{q\in\P_n} \int_{-1}^1 (f-q)^2 \dx.
  \end{gather}
  Mit Hilfe der Legendre-Polynome können wir nun die Lösung explizit angeben als
  \begin{gather}
    p(x) = \sum_{i=0}^n \alpha_i L_i(x)
    \qquad\text{mit}\qquad
    \alpha_i = \frac1{\norm{L_i}^2}\int_{-1}^1 f L_i(x)\dx.
  \end{gather}
\end{Beispiel}

\begin{Definition}{chebyshev-polynome}
  Die \define{Tschebyscheff-Polynome} $T_k$ sind definiert durch
  die Dreiterm-Rekursion
  \begin{gather}
    T_{k} = 2x T_{k-1}(x) - T_{k-2}(x).
  \end{gather}
  Sie sind orthogonal bezüglich des Skalarprodukts
  \begin{gather}
    \scal(p,q) = \int_{-1}^1 \tfrac1{\sqrt{1-x^2}} \,p(x)q(x)\dx.
  \end{gather}
\end{Definition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
