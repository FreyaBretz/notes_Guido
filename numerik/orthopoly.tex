\section{Polynomräume}

% \begin{Satz}{nullstellen}
%   Ein reelles Polynom vom Grad $n$ hat höchstens $n$ Nullstellen oder es ist das Nullpolynom.
% \end{Satz}

% \begin{proof}
%   Für $n=1$ handelt es sich um ein lineares Polynom und die Aussage
%   des Satzes ist unmittelbar klar. Sei nun $p$ ein Polynom strikt vom
%   Grad $n>1$ mit Nullstelle $x_0$. Dann gibt es nach dem euklidischen
%   Algorithmus zur Division mit Rest ein Polynom $q$ vom Grad $n-1$ und
%   eine Konstante $c$, so dass
%   \begin{gather}
%     p(x) = (x-x_0)q(x)+c.
%   \end{gather}
%   Daraus folgt $p(x_0) = c$, so dass folgt $c=0$. Wir können dieses
%   Verfahren für alle weiteren Nullstellen $x_1,\dots,x_m$ wiederholen
%   und erhalten
%   \begin{gather}
%     p(x) =  r(x) \prod_{k=0}^m (x-x_i),
%   \end{gather}
%   wobei $r(x)$ ein Polynom vom Grad $n-m$ sein muss, da $p$ vom Grad
%   $n$ ist. Insbesondere muss gelten $m\le n$.
% \end{proof}

% \begin{Korollar}{polynome-identisch}
%   Zwei reelle Polynome vom Grad $n$ sind identisch, wenn sie in
%   mindestens $n+1$ Punkten übereinstimmen. 
% \end{Korollar}


\begin{Lemma}{monome-linear-unabhaengig}
  Die Menge der Monome $\{x^0, x^1,\dots,x^n\}$ ist linear unabhängig.
\end{Lemma}

\begin{proof}
  Sei $p$ ein Polynom vom Grad $n$, also
  \begin{gather}
     p(x) = a_nx^n+a_{n-1}x^{n-1}+\dots+a_1x+a_0
   \end{gather}
   $p$ ist also gerade eine Linearkombination der Monome.  Zu zeigen
   ist, dass aus der Eigenschaft $p \equiv 0$ folgt, dass alle
   Koeffizienten verschwinden, also
  \begin{gather}
    p(x) \equiv 0
    \quad\Rightarrow\quad a_n = \dots = a_0 = 0.
  \end{gather}
  Zu diesem Zweck berechnen wir die $n$-te Ableitung von $p$ und
  erhalten, da mit $p$ auch alle seine Ableitungen identisch
  verschwinden,
  \begin{gather}
    n! a_n = 0.
  \end{gather}
  Daraus schließen wir $a_n = 0$. Nun gilt für die $(n-1)$-te Ableitung
  \begin{gather}
    n! a_n x + (n-1)! a_{n-1} = (n-1)! a_{n-1} = 0.
  \end{gather}
  Auf diese Weise schließen wir rekursiv bis $a_0$, dass alle Koeffizienten verschwinden. Damit ist das Lemma bewiesen.
\end{proof}

\begin{Satz}{polynomraum}
  Die Polynome vom maximalen Grad $n$ bilden einen Vektorraum der
  Dimension $n+1$.  Wir bezeichnen ihn mit $\P_n$.
\end{Satz}

\begin{proof}
  Es ist leicht nachzurechnen, dass sowohl die Summe, als auch reelle
  Vielfache von Polynomen wieder Polynome sind. Insbesondere erhöhen
  beide Operationen den Grad nicht. Damit ist $\P_n$ ein
  Vektorraum. Er wird per definitionem von den Monomen vom Grad bis zu
  $n$ erzeugt. Da diese nach
  \slideref{Lemma}{monome-linear-unabhaengig} linear unabhängig sind,
  bilden sie eine Basis und die Dimension von $\P_n$ ist $n+1$.
\end{proof}

\begin{Quiz}{Polynomraeume}
  Gegeben beliebige Werte $x_j\in\R$ mit $j=1,\dots,n$. Die Menge der
  Polynome $p_i$ definiert durch
  \begin{align*}
    p_0(x) &= 1\\
    p_i(x) &= \prod_{j=1}^i (x-x_j),\qquad i=1,\dots,n
  \end{align*}
  \begin{enumerate}[A]
  \item ist linear unabhängig
  \item ist linear abhängig
  \item ist ein Erzeugendensystem für $\P_n$
  \item ist eine Basis von $\P_n$
  \end{enumerate}
\end{Quiz}
\section{Skalarprodukt und Orthogonalität}
\begin{Definition}{skalarprodukt}
  Sei $V$ ein reeller Vektorraum. Eine Abbildung
  $a\colon V \times V \to \R$ heißt \define{Bilinearform}, wenn für
  $u,v,w\in V$ und $\lambda,\mu\in \R$ gilt
  \begin{align}
    a(\lambda u + \mu v,w) &= \lambda a(u,w) + \mu a(v,w)\\
    a(w,\lambda u + \mu v) &= \lambda a(w,u) + \mu a(w,v).
  \end{align}
  Eine Bilinearform heißt \define{symmetrisch}, wenn für $u,v\in V$ gilt
  \begin{gather}
    a(u,v) = a(v,u).
  \end{gather}
  Sie heißt \define{positiv semi-definit}, wenn $a(u,u) \ge 0$ für alle
  $u\in V$ und \define{positiv definit}, wenn zusätzlich
  \begin{gather}
    a(u,u) = 0 \quad \Longrightarrow \quad u=0.
  \end{gather}
  Eine symmetrische, positiv definite Bilinearform heißt
  \define{Skalarprodukt}, in der Regel notiert als $\scal(\cdot,\cdot)$.
\end{Definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{Lemma*}{bcs}{Bunjakowski-Cauchy-Schwarzsche Ungleichung}
  Sei $\scal(\cdot,\cdot)$ ein Skalarprodukt auf $V$.  Für zwei beliebige Elemente $u,v\in V$ gilt
  \begin{gather}
    \abs{\scal(u,v)} \le \sqrt{\scal(u,u)} \, \sqrt{\scal(v,v)}.
  \end{gather}
  Gleichheit gilt genau dann, wenn $u$ und $v$ kollinear sind, also
  $v=\alpha u$ mit einem skalaren Faktor $\alpha$.
\end{Lemma*}

\begin{proof}
  Zunächst zeigen wir nur die Ungleichung: Für $v=0\in V$ ist sie
  offensichtlich.
  
  Seien nun $v,u \in V$ keine Nullvektoren. Für beliebige $\mu, \lambda \in \R$
  gilt wegen der Bilinearität 
  \begin{gather}
   0 \le \scal(\lambda u + \mu v,\lambda u +  \mu v)
    = \lambda^{2} \scal(u,u)+2 \mu \lambda \scal(u,v) +\mu^{2} \scal(v,v)
  \end{gather}
  Setze $\lambda := \scal(v,v) \neq 0$
  \begin{gather}
   0 \le \scal(v,v)^{2} \scal(u,u) + 2\mu \scal(v,v)\scal(u,v) +\mu^{2}\scal(v,v)
  \end{gather}
  Dividiere durch$\scal(v,v)$
  \begin{gather}
   0 \le \scal(v,v) \scal(u,u) + 2\mu \scal(u,v) +\mu^{2}
  \end{gather}
  Setze nun $\mu := -\scal(u,v)$
  \begin{gather}
    0 \le \scal(v,v) \scal(u,u) -2\scal(u,v)^{2}+\scal(u,v)^{2}
  \end{gather}
  Daraus folgt
  \begin{gather}
    \scal(u,v)^{2} \le \scal(u,u) \scal(v,v)
  \end{gather}
  und mit der Monotonie der Quadratfunktion die Ungleichung.

  Nun bleibt die Äquivalenz für die Gleichheit zu zeigen.
  Für $v=0$ ist dies wieder trivial erfüllt. Seien zunächst $u,v$ linear abhängig, also zum Beispiel $u=av$.
  Dann gilt mit der Abkürzung $f(v) = \sqrt{\scal(v,v)}$
  \begin{gather}
    \abs{\scal(u,v)} = \abs{\scal(av,v)}
    = \abs{a} \cdot f(v) \cdot f(v)
    = f(av) \cdot f(v) =f(u) \cdot f(v).
  \end{gather}

  Gelte nun umgekehrt $\scal(u,v) = \sqrt{\scal(u,u)}\sqrt{\scal(v,v)}$.
  Es folgt
  \begin{gather}
     \scal(v,v) \scal(u,u) -2\scal(u,v)^{2}+\scal(u,v)^{2} = 0.
  \end{gather}
  Setze $\mu = \scal(u,v)\neq 0 $ und
  $\lambda = \scal(v,v)\neq 0$. Dann erhält man
  \begin{gather}
    \lambda \scal(u,u) - 2 \mu \scal(u,v) + \mu^2 = 0.
  \end{gather}
  Multipliplikation mit $\scal(v,v)$ ergibt
  \begin{gather}
   0=\lambda^2 \scal(u,u)+2\mu \scal(u,v)\scal(v,v) +\mu^{2}\scal(v,v) = \scal(\lambda u-\mu v,\lambda u-\mu v).
  \end{gather}
  
  Wegen der Definitheit folgt nun
  $\lambda u - \mu v = 0$ und da $\mu$ und $\lambda$ ungleich Null sind gilt,
  dass $ u,v$ linear abhängig sind
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{Lemma}{hilbertnorm}
  Sei $V$ ein reeller Vektorraum mit Skalarprodukt
  $\scal(\cdot,\cdot)$. Dann ist durch
  \begin{gather}
    \norm{u} = \sqrt{\scal(u,u)}
  \end{gather}
  auf $V$ eine Norm definiert. Ein endlichdimensionaler, reeller Vektorraum $V$ mit
  Skalarprodukt und zugehöriger Norm heißt \define{euklidischer
    Vektorraum}.
\end{Lemma}

\begin{proof}
  Das Skalarprodukt ist nicht negativ, daher ist die Abbildung $\norm{\cdot}\colon V \to \R$ wohldefiniert.
  Wir müssen nun die Normeigenschaften nachrechnen. Sei dazu $u \in V$. Es gilt
  \begin{enumerate}
  \item Nichtnegativität und Definitheit folgen sofort aus den entsprechenden Eigenschaften des Skalarprodukts.
  \item Homogenität
  \begin{gather}
    \norm{\lambda u} = \sqrt{\scal(\lambda u,\lambda u)}
    =\sqrt{\lambda^{2}\scal(u,u)}
    = \abs{\lambda}\sqrt{\scal(u,u)}
    =\abs{\lambda}\norm{u}
  \end{gather}
  \item Deiecksungleichung
  \begin{gather}
    \begin{aligned}
      \norm{u+v}^{2}
      &= \scal(u+v,u+v)\\
      &= \scal(u,u)+ 2 \scal(u,v) + \scal(v,v)\\
      \label{eq:orthopoly:1}
      &\le \scal(u,u)+ 2 \norm{u} \, \norm{v} + \scal(v,v)\\
      &=\norm{u}^{2}+ 2 \norm{u} \, \norm{v}+ \norm{v}^{2}\\
      & =(\norm{u}+\norm{v})^{2}\\
    \end{aligned}
  \end{gather}
  Daraus folgt durch Wurzelziehen auf beiden Seiten $\norm{u+v} \le \norm{u}+\norm{v}$.
  Für die Abschätzung in Zeile~\eqref{eq:orthopoly:1} haben wir die
  Bunyakovsky-Cauchy-Schwarz-Ungleichung aus \slideref{Lemma}{bcs} verwendet.
  \end{enumerate}
\end{proof}

\begin{Lemma*}{l2-norm}{$L^2$-Skalarprodukt}
  Auf dem Raum $V=\P_n$ der reellen Polynome vom Grad bis zu $n$ ist durch
  \begin{gather}
    \scal(p,q) = \int_{-1}^1 p(x)q(x)\dx
  \end{gather}
  ein Skalarprodukt definiert. Dieses wird auch $L^2$-Skalarprodukt genannt.
\end{Lemma*}

\begin{proof}
  Hier gilt es zu prüfen, ob die Abbildung auch die vier Eigenschaften eines
  Skalarprodukts erfüllt. Seien dazu im Folgenden $p,q,g \in \P_n$.
  Zunächst zeigen wir die Symmetrie.
  \begin{gather}
    \scal(p,q) =  \int_{-1}^1 p(x)q(x)\dx = \int_{-1}^1 q(x)p(x)\dx
    =\scal(q,p)
  \end{gather}
  Wenn wir nun Bilinearität zeigen, genügt es wegen der
  Symmetrieeigenschaft, das erste Argument zu untersuchen.
  \begin{gather}
    \begin{aligned}
    \scal(\lambda p + \mu q, g)
    &= \int_{-1}^1 (\lambda p(x)+ \mu q(x))g(x)\dx\\
   & = \int_{-1}^1 \lambda p(x)g(x)+ \mu q(x)g(x)\dx \\
   &= \int_{-1}^1 \lambda  p(x)g(x)\dx + \int_{-1}^1 \mu q(x)g(x)\dx \\
   &= \lambda \int_{-1}^1 p(x)g(x)\dx + \mu  \int_{-1}^1 q(x)g(x)\dx \\
   &= \lambda \scal(p,g) + \mu \scal(q,g)
    \end{aligned}
  \end{gather}

  Als letztes zeigen wir, dass die Abbildung positiv definit ist.
  \begin{gather}
    0 = \scal(p,p) = \int_{-1}^1 p(x)p(x)\dx =\int_{-1}^1 p(x)^{2}\dx
  \end{gather}
  Aus den Integraleigenschaften folgt
  \begin {gather}
    0 = p(x)^{2} \quad \forall x
  \end{gather}
  Dies kann nur der Fall sein, wenn $p \equiv 0$ ist.\\
  Somit haben wir nachgerechnet, dass es sich um Skalarprodukt handelt.
  \end{proof}

\begin{Definition}{l2-norm}
  Nach dem \slideref{Lemma}{hilbertnorm} können wir mit diesem Skalarprodukt eine Norm auf $\P_n$
  definierten. Diese Norm wird als die $L^2$-Norm bezeichnet.
  \begin{gather}
    \norm{f}_{L^2} = \sqrt{\scal(f,f)_{L^2}} = \sqrt{\int_{-1}^1 f(x)^2 dx}
  \end{gather}
\end{Definition}

\begin{Definition}{orthogonal}
  Zwei Vektoren $u,v\in V$ heißen \define{orthogonal}, wenn
  \begin{gather}
    \scal(u,v) = 0.
  \end{gather}
  Ein Vektor $u\in V$ ist orthogonal zum Untervektorraum $W\subset V$, wenn
  \begin{gather}
    \scal(u,v) = 0\quad\forall v\in W.
  \end{gather}
\end{Definition}

\begin{Notation}{euklidischer-vr}
  Von nun an bezeichnet $V$ immer einen endlichdimensionalen, reellen,
  euklidischen Vektorraum.
\end{Notation}

\begin{Lemma*}{pythagoras}{Pythagoras}
  Seien zwei Vektoren $u\in V$ und $v\in V$ orthogonal zueinander. Dann gilt
  \begin{gather}
    \norm{u+v}^{2} = \norm{u}^{2} + \norm{v}^{2}
  \end{gather}
\end{Lemma*}

\begin{proof}
  Seien $u,v \in V$. Es gilt $ 0 = \scal(u,v)$
   \begin{gather}
    \norm{u+v}^{2} = \scal(u+v,u+v)
    %=\scal(u+v,u)+\scal(u+v,v)
    %=\scal(u,u)+\scal(v,u)+\scal(u,v)+\scal(v,v)
    =\norm{u}^{2} + \norm{v}^{2} +2\scal(u,v) = \norm{u}^{2} + \norm{v}^{2}
  \end{gather}
\end{proof}

\section{Bestapproximation und orthogonale Projektion}
\begin{Definition}{bestapproximation}
  Sei $A\subset V$ ein affiner Unterraum eines euklidischen
  Vektorraums. Dann ist die Bestapproximation $u_b\in A$ eines Vektors
  $u\in V$ in $A$ definiert durch die Beziehung
  \begin{gather}
    \norm{u-u_b} = \min_{v\in A} \norm{u-v}.
  \end{gather}
\end{Definition}

\begin{Satz}{bestapproximation}
  Sei $w \in V$ und $W \subset V$.
  Sei $A=w+W$ ein nichtleerer, affiner Unterraum von $V$. Dann
  existiert die Bestapproximation nach
  \slideref{Definition}{bestapproximation} und ist eindeutig
  bestimmt. Es gilt die notwendige und hinreichende Bedingung
  \begin{gather}
    \scal(u-u_b,v) = 0 \quad \forall v\in W.
  \end{gather}
  Das heißt $ u_b$ ist Bestapproximation genau dann wenn $u-u_b$
  orthogonal zu $W$ bzgl. des Skalarprodukts $\scal(\cdot,\cdot)$ ist.
\end{Satz}

\begin{proof}
  Der Beweis gliedert sich in drei Teile. Zuert wird die Äquivalenz
  gezeigt danach zeigen wir die Eindeutigkeit und zum Schluss
  erst die Existenz.
  
 \glqq $\Rightarrow$\grqq{}:
  Sei $ u_b \in A$ die Bestapproximation des Vektors $ u \in V$.
  Wir definieren nun für beliebiges $v\in W$ die Funktion:
  \begin{gather}
    \begin{split}
      F_v\colon \R&\to \R,\\
      F_v(x)&= \norm{u-u_b+ xv}^{2}
    \end{split}
  \end{gather}
  Da $u_b\in A$ liegt auch $u_b+xv$ in $A$ und folglich besitzt 
  diese Funktion nach Voraussetzung ein Minimum bei $x=0$. Wir untersuchen daher die Ableitung
  \begin{align}
    \frac{d}{dx} F(x)
    &= \frac{d}{dx}\norm{u-u_b+xv}^{2} \\
    &= \frac{d}{dx} \Bigl(\norm{u-u_b}^2 + 2x \scal(u-u_b,v)
      + x^2\norm{v}^2\Bigr)\\
     &= 2 \scal(u-u_b,v) + 2x \norm{v}^2
  \end{align}
  und es gilt
  \begin{gather}
    0 = \left. \frac{d}{dx} F(x) \right|_{x=0} =  2 \scal(u-u_b,v).
  \end{gather}

  \grqq$\Leftarrow$\grqq{}:
  Erfülle nun $u_b\in A$ die Bedingung $\scal(u-u_b,v) = 0$ für alle $v\in W$.
  \begin{align}
    \norm{u-u_b}^{2}
    &=\scal(u-u_b,u-u_b)\\
    &= \scal(u-u_b,u-u_b-v)\\
    &\le \norm{u-u_b}\cdot\norm{u-v}.
  \end{align}
  Da nun $v\in W$ beliebig und $u_b\in A$, so wird der gesamte affine
  Unterraum $A$ durch die Terme der Form $u_b+v$ aufgespannt.
  Daraus folgt
  \begin{gather}
   \norm{u-u_b} \le \inf_{v\in A}{\norm{u-v}}
  \end{gather}
  und $u_b$ ist die Bestapproximation.
  
  Nun zur Eindeutigkeit:
  Seien $u_b$ und $u_d \in$ A zwei Bestapproximationen.
  Dann gilt notwendigerweise
  \begin{gather}
   \scal(u-u_b,v) = 0 = \scal(u-u_d,v) \quad \forall v\in W
  \end{gather}
  Dies wird umgeformt zu
  \begin{xalignat}2
  \scal(u-u_d,v)-\scal(u-u_b,v)&=0 &  \forall v&\in W \\
  \Leftrightarrow\qquad\qquad\qquad\scal(u_b-u_d,v) &= 0 & \forall v &\in W
  \end{xalignat}
  Da $u_b,u_d\in A$ folgt $u_b-u_d \in W$. Daher dürfen wir oben
  $v=u_b-u_d$ einsetzen. Dies ergibt $\norm{u_b-u_d}^{2} =0$ und somit
  folgt $u_b = u_d$.
  
  Schließlich die Existenz:
  Der endliche dimensionale Teilraum $W\subseteq V$ besitzt eine Basis
  $(\phi_1,\dots, \phi_n)$ mit $n=\dim W$. Die gesuchte Approximation
  $u_b\in A$ lässt sich
  durch die Basis in folgender Form darstellen
  \begin{gather}
   u_b = w + \sum_{k=1}^n x_k \phi_k
  \end{gather}
  Dies wird in die notwendige Orthogonalitätsbedingung
  \slideref{Satz}{bestapproximation} eingesetzt.
  \begin{gather}
   \scal(u-w-\sum_{k=1}^n x_k \phi_k,v)=\scal(u-w,v)-\sum_{k=1}^n x_k\scal(\phi_k,v)=0
   \quad \forall v\in W.
   \end{gather}
 Dies ist durch Einsetzen von $v:=\phi_i$ für  $i=1,\dots,n$ äquivalent zu einem
 linearen Gleichungssystem $\matg\vx=\vb$ mit Matrix und rechter Seite
 \begin{gather}
  \matg=\bigl(\scal(\phi_k,\phi_i)\bigr)_{i,k=1}^n \qquad \vb:=\bigl(\scal(u-w,\phi_i)\bigr)_{i=1}^n
 \end{gather}
 Da die Matrix $A$ quadratisch ist, folgt aus der Eindeutigkeit die Existenz.
\end{proof}

\begin{Definition}{komplement-projektion}
  Sei $W \subset V$ ein Untervektorraum. Dann gilt
  $V = W \oplus W^\perp$, wobei das \define{orthogonale Komplement}
  $W^\perp$ eindeutig definiert ist durch
  \begin{gather}
    W^\perp = \bigl\{ v\in V \big| \scal(v,w) = 0 \quad\forall w\in W\bigr\}.
  \end{gather}
  Die Lösung der Bestapproximationsaufgabe bezeichnen wir mit
  \begin{gather}
    \Pi_W u = u_b\in W
  \end{gather}
  und nennen es die \define{orthogonale Projektion} von $u\in V$ auf $W$.
\end{Definition}

\begin{Lemma}{komp-projekt-wohldefiniert}
  Das orthogonale Komplement und die orthogonale Projektion sind wohldefiniert.
\end{Lemma}

\begin{proof}
  \slideref{Satz}{bestapproximation}.
\end{proof}

\begin{Beispiel}{polynom-bestapproximation}
  Die Aufgabe der Gaußschen Ausgleichsrechnung (in $L^2$) lautet: finde zu einer
  gegebenen Funktion $f$ das Polynom vom Grad höchstens $n$, das auf
  dem Intervall $[-1,1]$ den mittleren quadratischen Abstand
  minimiert, also $p\in \P_n$ mit
  \begin{gather}
    \int_{-1}^1 \bigl(f(x)-p(x)\bigr)^2 \dx
    = \min_{q\in \P_n} \int_{-1}^1 \bigl(f(x)-q(x)\bigr)^2 \dx.
  \end{gather}
  Die Lösung erfüllt
  \begin{gather}
    \int_{-1}^1 p(x)q(x) \dx = \int_{-1}^1 f(x)q(x) \dx
    \qquad\forall q\in \P_n.
  \end{gather}
\end{Beispiel}

\begin{remark}
  Nach \slideref{Satz}{bestapproximation} ist die Aufgabe der
  Gaußschen Ausgleichsrechnung äquivalent zur Minimierungsaufgabe ein
  $p\in\P_n$ zu finden, so dass
  \begin{gather}
    \norm{f-p}_{L^2}^2 = \min_{q\in \P_n} \norm{f-q}_{L^2}^2.
  \end{gather}
\end{remark}

\begin{remark}
  Die Formulierung in~\slideref{Beispiel}{polynom-bestapproximation}
  ist die mathematische Überspitzung einer häufigen Aufgabe der
  Wissenschaft: gegeben $N$ Messwerte $(x_i, f_i)$, wie kann ich eine
  Funktion $f(x)$ finden, die die Messwerte im quadratischen Mittel am
  besten approximiert. Schränken wir die Suche nach der Funktion $f$
  auf den Polynomraum $\P_n$ mit $n<N$ ein, so erhalten wir die
  diskrete Variante der Gaußschen Ausgleichsrechnung: finde
  $p\in \P_n$, so dass
  \begin{gather}
    \sum_{i=1}^N \bigl(f_i-p(x_i)\bigr)^2 =  \min_{q\in \P_n}  \bigl(f_i-q(x_i)\bigr)^2.
  \end{gather}
  Ein Spezialfall ist $n = N-1$ und wir werden im Kapitel zur
  Interpolation sehen, dass dort das Minimum zu null wird. Dort werden wir auch sehen, dass
  \begin{gather}
    \scal(p,q) = \sum_{i=1}^N p(x_i)q(x_i)
  \end{gather}
  ein Skalarprodukt auf $\P_n$ mit $n<N$ darstellt.
\end{remark}

\section{Orthogonale Basen}

\begin{Lemma}{gram-system}
  Wählt man eine Basis $\{\phi_i\}$ für $W$, so transformiert sich die
  Orthogonalitätsbedingung in \slideref{Satz}{bestapproximation} zum
  linearen Gleichungssystem
  \begin{gather}
    \matg\vx = \vb.
  \end{gather}
  Hier sind $\vx$ der Koeffizientenvektor der Lösung $u_b$, $\matg$ die
  \define{Gramsche Matrix} und $\vb$ die rechte Seite gegeben durch
\begin{gather}
  g_{ij} = \scal(\phi_i,\phi_j), \qquad
  b_i = \scal(u,\phi_i).
\end{gather}
Wir haben hier o.B.d.A. den affinen Unterraum $A$ durch den Untervektorraum $W$ ersetzt.
\end{Lemma}

\begin{remark}
  Das Gleichungssystem hängt nur von der Wahl einer Basis in $W$ ab,
  nicht in $V$.
\end{remark}

\begin{Definition}{ortho-system}
  Eine Menge von Vektoren $\{\phi_1,\dots,\phi_n\}\subset V$ bildet
  ein \define{Orthogonalsystem}, wenn
  \begin{gather*}
    \scal(\phi_i,\phi_j) = 0
    \qquad \forall 1\le i < j \le n.
  \end{gather*}
  Sie ist ein \define{Orthonormalsystem}, wenn zusätzlich
  $\norm{\phi_i} = 1$ für alle Elemente gilt. Ein Orthonormalsystem, das eine Basis bildet, heißt \define{Orthonormalbasis} (\define{ONB}).
\end{Definition}

\begin{Lemma}{ortho-lu}
  Jedes Orthogonalsystem ist linear unabhängig.
\end{Lemma}


\begin{Lemma*}{parseval}{Parsevalsche Gleichung}
  Sei $\{\phi_i\}$ für $i=1,\dots,n$ eine ONB von $V$. dann gilt für
  jedes $v\in V$ mit der Basisdarstellung
  \begin{gather}
    v = \sum_{i=1}^n x_i \phi_i
  \end{gather}
  die Identität
  \begin{gather}
    \norm{v}^2 = \sum_{i=1}^n x_i^2.
  \end{gather}
\end{Lemma*}

\begin{Aufgabe}{ortho-lu}
  Zeigen Sie \slideref{Lemma}{ortho-lu} und die Parsevalsche Gleichung.
\end{Aufgabe}

\begin{Bemerkung}{least-squares-orthogonal}
  Bezüglich einer ONB ist die Gramsche Matrix die
  Einheitsmatrix. Damit berechnen sich die Einträge des
  Koeffizientenvektors $\vx$ in \slideref{Lemma}{gram-system} durch
  die einfache Formel
  \begin{gather}
    x_i = b_i = \scal(u,\phi_i).
  \end{gather}
\end{Bemerkung}


\begin{Theorem*}{gram-schmidt}{Gram-Schmidt-Verfahren}
  Jede linear unabhängige Menge von Vektoren
  $\{v_1,\dots,v_n\}\subset V$ wird mit dem folgenden Verfahren in ein
  Orthonormalsystem $\{\phi_1,\dots,\phi_n\}\subset V$ umgeformt:
  \begin{gather}
    \begin{aligned}
      \phi_1 &= \tfrac1{\norm{v_1}} \,v_1\\
      w_j &= v_j - \sum_{i=1}^{j-1} \scal(v_j,\phi_i)\,\phi_i
      & \quad \phi_j &= \tfrac1{\norm{w_j}}\, w_j
      &\quad j=2,\dots,n
    \end{aligned}
  \end{gather}
  Für alle $1\le k \le n$ gilt
  \begin{gather}
    \operatorname{span}\{\phi_1,\dots,\phi_k\}
    =
    \operatorname{span}\{v_1,\dots,v_k\}
  \end{gather}
\end{Theorem*}

\begin{proof}
  Per Induktion über $n$ zeigen wir Orthogonalität und Normierung.

  \textbf{Indukionsanfang:} Sei $n=1$.
  Wird nur ein Vektor aus dem Raum gewählt, so erfüllt dieser
  die Orthogonalitätsbedingung, da er der einzige Vektor im System ist.
  Wird dieser Vektor zusätzlich normiert erhält man ein Orthonormalsystem.
  
  \textbf{Induktionsschritt:} Die Aussage gelte
  für $\{v_1,\dots,v_{n-1}\}$ und das Orthonormalsystem $(\phi_1,\dots,\phi_{n-1})$.
  Zunächst zeigen wir: $\phi_n$ ist wohldefiniert, also $w_n\neq 0$. Wäre dies nicht so, so gälte
  \begin{gather}
    w_n = v_n -\sum_{i=1}^{n-1}\scal(v_n,\phi_i)\,\phi_i = 0
  \end{gather}
  Es ist also $v_n \in \operatorname{span}\{\phi_1,\dots,\phi_{n-1}\}$
  und daher ist die Menge $(v_1,\dots,v_n)$ linear abhängig.  Das ist
  ein Widerspruch zur Voraussetzung.
  
  $w_n$ wird nun normiert zu $\phi_n = \frac{1}{\norm{w_n}} w_n$.

  Schließlich gilt
  \begin{gather}
    \scal(\phi_n,\phi_j)=\scal(v_n,\phi_j)-
    \sum_{i=1}^{n-1}\scal(v_n,\phi_i)
    \,\underbrace{\scal(\phi_i,\phi_j)}_{=\delta_{ij}}  = 0
    \qquad j=1,\dots,n-1
  \end{gather}
  und damit die Orthogonalität.
\end{proof}

\begin{Algorithmus*}{gram-schmidt}{Gram-Schmidt}
  \lstinputlisting{code/gram-schmidt.py}
\end{Algorithmus*}

\begin{intro}
  Da dies der erste Algorithmus in dieser Vorlesung ist, erläutern wir
  das Programm im Detail und gehen etwas auf die Syntax von Python
  ein. Ebenso sollte der Code mit einem einfachen Beispiel
  probiert werden, um die Parallelen zum Verfahren besser zu erkennen.
  \begin{enumerate}
  \item Es wird eine Funktion mit dem Namen
    \lstinline!gram_schmidt!. Dieser Funktion wird eine Matrix $v$
    übergeben deren Spalten die Vektoren $v_1$ bis $v_n$ sind, die wir
    orthogonalisieren wollen.
  \item Es wird $n$ die Länge der Zeilen (Anzahl der Vektoren)
    zugewiesen und $m$ wird die Länge der Spalten (Anzahl der Einträge
    im Vektor) zugewiesen.
  \item Beginn des Gram-Schmidt-Verfahrens und der Schleife über die
    Vektoren von 1 bis $n$.
   \item Initialsieren eines Vektors $\delta$ der Länge $m$ mit Nullen als Einträge
   \item Es wird eine weitere Schleife begonnen in der ein Index i
     über alle bisher orthogonalisierten Vektoren läuft. Dies
     entspricht der Summe aus dem Verfahren. Beachten Sie, dass diese
     Schleife für $j=0$ nicht ausgeführt wird.
   \item $r$ ist das Skalarprodukt aus dem Vektor $v_j$ und einem
     bereits orthogonalisierten Vektor $v_i$.  Die Vektoren befinden sich in
     der Matrix $v$ und über diesen Befehl wird darauf zugegriffen.
   \item Addiere zu $\delta$ die Projektion von $V_j$ auf $v_i$.
   \item Hier endet die zweite for-Schleife durch reduktion der
     Einrückung.  Die Summe wird vom Vektor $v_j$ abgezogen, somit
     $v_j$ orthogonalisiert und wieder in der Matrix $v$ an der
     richtigen Stelle zugewiesen.
   \item  Die Norm von $v_j$ wird berechnet.  
   \item Wie im Verfahren wird in  dieser Zeile $v_j$ normiert und in der Matrix $v$ an der Stelle des
       früheren $v_j$ zugewiesen.
  \end{enumerate}
\end{intro}

\begin{Beispiel}{gram-schmidt}
  Wir wählen für Polynome das $L^2$-Skalarprodukt aus
  \slideref{Lemma}{l2-norm} und die Basis $\{1,x,\dots,x^{n-1}\}$
  für $\P_{n-1}$. Wir verwenden die Iplementation in
  \slideref{Algorithmus}{gram-schmidt} und messen den Erfolg nach der
  Größe der Nebendiagonaleinträge der Gramschen Matrix.
  \begin{center}
    \begin{tabular}{c|c}
      $n$ & $\max_{i\neq j} \abs{g_{ij}}$ \\
      \hline
      5 & $8.9\cdot 10^{-16}$ \\
      10 & $9.1\cdot 10^{-12}$ \\
      15 & $1.2\cdot 10^{-7}$ \\
      20 & $0.23$
    \end{tabular}
  \end{center}
\end{Beispiel}

\begin{Algorithmus*}{mgs}{Modifizierter Gram-Schmidt}
  \lstinputlisting{code/modified-gram-schmidt.py}  
\end{Algorithmus*}

\begin{remark}
  In diesem Programm wurde der Zwischenschritt über den Vektor
$\delta$ ausgelassen.
\end{remark}

\begin{Beispiel}{gs-mgs}
  In dieser Tabelle wiederholen wir die Zahlen
  $\max_{i\neq j} \abs{g_{ij}}$ aus \slideref{Beispiel}{gram-schmidt}
  und stellen sie den entsprechenden Ergebnissen des modifizierten
  Verfahrens in \slideref{Algorithmus}{mgs} gegenüber.
  \begin{center}
    \begin{tabular}{c|cc}
      $n$ &  Gram-Schmidt & modifiziert\\
      \hline
      5 & $8.9\cdot 10^{-16}$ & $1.3\cdot 10^{-16}$ \\
      10 & $9.1\cdot 10^{-12}$ & $2.9\cdot 10^{-12}$ \\
      15 & $1.2\cdot 10^{-7}$ & $2.7\cdot 10^{-9}$ \\
      20 & $0.23$ & $3.9\cdot 10^{-5}$
    \end{tabular}
  \end{center}
\end{Beispiel}

\begin{remark}
  Wir sehen, dass die Wahl der Implementation eines Rechenverfahrens
  bei mathematischer Äquivalenz durchaus erheblichen Einfluss auf das
  Ergebnis haben kann. Dieses Phänomen werden wir in
  \Cref{sec:stability} näher untersuchen. Zunächst diskutieren wir
  aber eine weitere Variante der Erzeugung orthogonaler Basen in
  Polynomräumen.
\end{remark}

\section{Drei-Term-Rekursion}

\begin{Satz*}{dreiterm}{Dreiterm-Rekursion}
  Zu jedem Skalarprodukt $\scal(\cdot,\cdot)$ auf dem Raum der
  stetigen Funktionen mit der Eigenschaft, dass für alle Polynome $p,q$
  \begin{gather}
    \scal(xp,q) = \scal(p,xq)
  \end{gather}
  gilt,
  gibt es genau eine Folge von orthogonalen
  Polynomen $\{p_k\}_{k=0,\dots}$ wobei $p_k\in \P_k$ mit führendem Koeffizienten eins. Sie
  genügen der Dreiterm-Rekursionsformel
  \begin{gather}
    p_k(x) = (x-a_k)p_{k-1}(x) - b_k p_{k-2}(x),
    \qquad k=1,2,\ldots
  \end{gather}
  mit Startwerten $p_{-1} \equiv 0$ und $p_0 \equiv 1$. Die
  Koeffizienten sind
  \begin{gather}
    a_k = \frac{\scal(x p_{k-1},p_{k-1})}{\scal(p_{k-1},p_{k-1})}
    \qquad\text{und}\qquad
    b_k = \frac{\scal(p_{k-1},p_{k-1})}{\scal(p_{k-2},p_{k-2})}.
  \end{gather}
\end{Satz*}

\begin{proof}
  Siehe auch \cite[Satz 6.2]{DeuflhardHohmann08} und \cite[Satz 2.17]{Rannacher17}.

  Wir beginnen mit der Eindeutigkeit: wenn $p_k$ orthogonal zu allen
  vorherigen Folgengliedern ist, so ist es auch orthogonal zu allen
  ihren Linearkombinationen und daher gilt $p_k \perp \P_{k-1}$. Nach
  \slideref{Lemma}{komp-projekt-wohldefiniert} ist das orthogonale
  Komplement von $\P_{k-1}$ in $\P_k$ wohldefiniert und nach der
  Dimensionsformel ist seine Dimension eins. Daher unterscheiden sich
  alle Polynome dort nur um einen skalaren Faktor und durch die
  Normierung wird $p_k$ eindeutig.
  
  Wir zeigen nun per inductionem, dass die erzeugte Folge eine
  Orthogonalfolge ist. Da alle Vektoren zum Nullvektor orthogonal ist,
  gilt die Aussage für die beiden Startpolynome. Sei sie nun bis
  $p_{k-1}$ bewiesen. Dann gilt zunächst mit der Wahl von $a_k$:
  \begin{align}
    \scal(p_k,p_{k-1}) &= \scal(x p_{k-1},p_{k-1}) - a_k\scal(p_{k-1},p_{k-1})
                         - b_k \scal(p_{k-2},p_{k-1})\\
                       &=  \scal(x p_{k-1},p_{k-1}) -  \scal(x p_{k-1},p_{k-1}) - 0 \\
                       &= 0.
  \end{align}
  Nach der Wahl von $b_k$ erhalten wir:
  \begin{align}
    \scal(p_k,p_{k-2}) &= \scal(x p_{k-1},p_{k-2}) - a_k\scal(p_{k-1},p_{k-2})
                         - b_k \scal(p_{k-2},p_{k-2})\\
                       &=  \scal(x p_{k-1},p_{k-2}) - 0 - \scal(p_{k-1},p_{k-1})\\
                       &= \scal(p_{k-1},xp_{k-2}-p_{k-1}).
  \end{align}
  Da die führenden Koeffizienten beider Polynome zur rechten eins
  sind, ist die Differenz in $\P_{k-2}$ und damit nach
  Induktionsannahme orthogonal zu $p_{k-1}$. Schließlich gilt für $j<k-2$:
  \begin{align}
    \scal(p_k,p_j) &= \scal(x p_{k-1},p_{j}) - a_k\scal(p_{k-1},p_{j})
                     - b_k \scal(p_{k-2},p_{j})\\
                   &= \scal(p_{k-1}, xp_j) - 0 - 0.
  \end{align}
  Dieser Term verschwindet, da $xp_j\in \P_{k-2}$ und damit orthogonal zu $p_{k-1}$.
\end{proof}

\begin{Bemerkung}{dreiterm-normierung}
  Der Beweis ergibt eigentlich die ``Eindeutigkeit einer
  Orthogonalfolge bis auf Normierung''. Tatsächlich werden in der
  Literatur immer wieder veschiedene Normierungen benutzt. Beispiele
  sind:
  \begin{enumerate}
  \item $p_k(1) = 1$
  \item $\norm{p_k} = 1$
  \item Führender Koeffizient eins, $p_k = x^k + \dots$
  \end{enumerate}
\end{Bemerkung}

\begin{Definition}{legendre-polynome}
  Die \define{Legendre-Polynome} $L_k$ sind definiert durch
  die Dreiterm-Rekursion
  \begin{gather}
    L_{k}(x) = \tfrac{2k-1}{k}x L_{k-1}(x) - \tfrac{k-1}{k} L_{k-2}(x)
  \end{gather}
  mit den Startbedingungen $L_0(x) = 1$ und $L_1(x) = x$.
  Sie sind orthogonal bezüglich des $L^2$-Skalarprodukts in
  \slideref{Lemma}{l2-norm}.
\end{Definition}

\begin{Beispiel}{least-squares-legendre}
  Das Problem der Gaußschen Ausgleichsrechnung war: Zu einer gegebenen
  Funktion $f$ finde $p\in \P_n$, so dass
  \begin{gather}
    \norm{f-q}_{L^2}^2
    = \min_{q\in\P_n} \norm{f-q}_{L^2}^2.
  \end{gather}
  Mit Hilfe der Legendre-Polynome können wir nun die Lösung explizit angeben als
  \begin{gather}
    p(x) = \sum_{i=0}^n \alpha_i L_i(x)
    \qquad\text{mit}\qquad
    \alpha_i = \frac1{\norm{L_i}^2}\int_{-1}^1 f L_i(x)\dx.
  \end{gather}
\end{Beispiel}

\begin{Aufgabe}{shifted-Legendre}
  Oft benötigt man Legendre-Polynome nicht auf dem Intervall $[-1,1]$,
  sondern auf einem anderen Intervall $[a,b]$. Man spricht dann auch
  von \emph{shifted Legendre polynomials}. Während die abstrakte
  Formel in \slideref{Satz}{dreiterm} natürlich weiter gültig bleibt,
  ändern sich dabei die Skalarprodukte und damit die Koeffizienten in
  \slideref{Definition}{legendre-polynome}

  Berechnen Sie die ersten drei Polynome für das Intervall $[0,1]$ mit
  der Normierung $L_k(1) = 1$.
\end{Aufgabe}

\begin{Definition}{chebyshev-polynome}
  Die \define{Tschebyscheff-Polynome} $T_k$ sind definiert durch
  die Dreiterm-Rekursion
  \begin{gather}
    T_{k} = 2x T_{k-1}(x) - T_{k-2}(x).
  \end{gather}
  Sie sind orthogonal bezüglich des Skalarprodukts
  \begin{gather}
    \scal(p,q) = \int_{-1}^1 \tfrac1{\sqrt{1-x^2}} \,p(x)q(x)\dx.
  \end{gather}
\end{Definition}

\begin{Aufgabe}{Tschebyscheff}
  Zeigen Sie durch vollständige Induktion, dass die Teschebyscheff-Polynome der Darstellung
  \begin{gather}
    T_k(x) = \cos(k \operatorname{arccos} x)
  \end{gather}
  genügen.
\end{Aufgabe}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
