\begin{intro}
  Betrachten wir die Konstruktion der LR-Zerlegung als Folge von
  Operationen auf Matrizen, so ergibt sich das Bild
  \begin{gather}
    \begin{split}
      A^{(1)} & \mapsto L_{1} A^{(2)} \\
      A^{(2)} & \mapsto L_{2} A^{(3)} \\
      A^{(n-1)} & \mapsto L_{n-1} R.
    \end{split}
  \end{gather}
  Es findet also in jedem Schritt eine Umformung der Matrix des
  aktuellen Schritts mit einer Frobeniusmatrix statt.

  In der Fehlerabschätzung steht der Faktor
  \begin{gather}
    \rho = \frac{\alpha_{\max}}{\max \abs{a_{ij}}}
    = \frac{\max \abs{a^{(k)}_{ij}}}{\max \abs{a_{ij}}},
  \end{gather}
  wobei das Wachstum der Zähler von den Eigenschaften der
  Frobeniusmatrizen abhängt. Aus \cite{DeuflhardHohmann08} zitieren
  wir dazu folgende Werte abhängig von der Struktur der Matrix
  \begin{center}
    \begin{tabular}{l|c}
      Matrix & $\rho$\\\hline
      invertierbar & $2^{n-1}$ \\
      diagonaldominant & 2 \\
      s.p.d. & 1
    \end{tabular}
  \end{center}
  Hierbei wird für allgemein invertierbare Matrizen Spaltenpivotierung
  angewandt, für die anderen nicht. Für allgemeine Matrizen kann
  dieser Faktor also sehr schnell anwachsen und der Algorithmus
  instabil werden.

  Dieser Abschnitt beschäftigt sich nun mit alternativen
  Transformationen, die immer zu einer stabilen Zerlegung
  führen. Gesucht werden dazu Matrizen so dass
  \begin{gather}
    \norm{A^{(n-1)}} = \dots = \norm{A^{(k)}} = \dots = \norm{A^{(1)}}
  \end{gather}
  für eine geeignete Norm gilt.
\end{intro}

\subsection{Orthogonale Matrizen}

\begin{Definition}{ortho-matrix}
  Eine \define{orthogonale Matrix} ist eine quadratische Matrix, deren
  Spaltenvektoren bzw. deren Zeilenvektoren eine Orthonormalbasis des
  $\R^n$ bilden.
\end{Definition}

\begin{Satz}{ortho-matrix}
  Für eine orthogonale Matrix $Q$ gilt
  \begin{gather}
    \label{eq:qr:1}
    Q^{-1} = Q^T.
  \end{gather}
  Umgekehrt folgt aus dieser Beziehung die Orthogonalität der
  Zeilenvektoren und Spaltenvektoren.
\end{Satz}

\begin{proof}
  Nehmen wir an, die Spaltenvektoren $q^{(1)},\dots,q^{(n)}$ von $Q$
  seinen eine ONB. Dann gilt für die Matrix $A = Q^TQ$:
  \begin{gather}
    a_{ij} = \sum_{k=1}^n q_{ki}q_{kj}
    = \sum_{k=1}^n q^{(i)}_k q^{(j)}_k
    = \bigl(q^{(i)}\bigr)^T q^{(j)} = \delta_{ij}.
  \end{gather}
  Daher gilt $Q^TQ=I$.  Multiplizieren wir diese Gleichung von rechts
  mit $Q^{-1}$, so erhalten wir~\eqref{eq:qr:1}. Setzen wir umgekehrt
  $Q^TQ=I$, so ergibt obige Rechnung die Orthogonalität der
  Spaltenvektoren.

  Aus $Q^T = Q^{-1}$ folgt aber durch Transponieren
  \begin{gather}
    Q = Q^{-T},
  \end{gather}
  wobei $Q^{-T}$ die Inverse von $Q^T$ ist. Multiplizieren wir die
  letzte Gleichung von rechts mit $Q^T$, so erhalten wir $QQ^T = I$,
  was äquivalent zur Orthonormalität der Zeilenvektoren ist.

  Wir hätten diesen Beweis auch mit den Zeilenvektoren beginnen können
  und $QQ^T=I$ folgern. Der Rest verläuft dann analog.
\end{proof}

\begin{example}
  Die Rotationsmatrix
  \begin{gather}
    Q = \begin{bmatrix}
      \cos \phi & \sin \phi\\-\sin\phi&\cos\phi
    \end{bmatrix}
  \end{gather}
  ist orthogonal. Dasselbe gilt für die Reflexionsmatrix an einem
  Vektor $w\in \R^n$,
  \begin{gather}
    Q = I-2\frac{ww^T}{w^Tw}
  \end{gather}
\end{example}

\begin{Lemma}{qr-norm}
  Für jede orthogonale Matrix $Q\in \R^{n\times n}$, jeden Vektor
  $x\in\R^n$ und jede beliebige Matrix $A\in \R^{n\times n}$ gilt
  \begin{gather}
    \norm{Qx}_2 = \norm{x}_2,
    \qquad \norm{QA}_2 = \norm{A}_2.
  \end{gather}
\end{Lemma}

\subsection{Existenz und Konstruktion}

\begin{Definition}{qr}
  Bei der \define{QR-Zerlegung} wird die Matrix $A\in \R^{n\times n}$
  in das Produkt
  \begin{gather}
    A = QR
  \end{gather}
  aus einer orthogonalen Matrix $Q$ und einer oberen Dreiecksmatrix
  $R$ zerlegt.
\end{Definition}

\begin{Lemma}{qr-spalten}
  Seien $q^{(1)},\dots,q^{(n)}$ die Spaltenvektoren von $Q$ und
  $a^{(1)},\dots,a^{(n)}$ die Spaltenvektoren von $A$.
  Dann gilt
  \begin{gather}
    \label{eq:qr:2}
    a^{(k)} = \sum_{i=1}^k r_{ik} q^{(i)}.
  \end{gather}
  Gilt $r_{ii}\neq 0$ für $i=1,\dots,k$ so ist die Beziehung eindeutig
  umkehrbar.  Insbesondere besteht dann die Folge der Spaltenvektoren von
  $Q$ aus den orthogonalisierten Spaltenvektoren von $A$ mit
  \begin{align}
    \operatorname{span}\bigl\{q^{(1)},\dots,q^{(k)}\bigr\}
    = \operatorname{span}\bigl\{a^{(1)},\dots,a^{(k)}\bigr\}
    \qquad k=1,\dots,n.
  \end{align}
\end{Lemma}

\begin{Satz}{qr}
  Zu jeder invertierbaren quadratischen Matrix $A\in \R^{n\times n}$
  gibt es eine QR-Zerlegung. Unter der Zusatzbedingung $r_{ii}>0$ ist
  diese eindeutig.
\end{Satz}

\begin{proof}
  Nach dem vorigen Lemma können die Spaltenvektoren $q^{(i)}$ der
  Matrix $Q$ mit dem Gram-Schmidt-Verfahren aus den Spaltenvektoren
  $a^{(i)}$ von $A$ gewonnen werden. Da die Spalten von $A$ linear
  unabhängig sind, bricht das Verfahren nicht ab.

  Daraus folgt insbesondere
  Gleichung~\eqref{eq:qr:2} mit $r_{ij} = \scal(a^{(j)},q^{(i)})$.

  Zur Eindeutigkeit nehmen wir an, es gelte $A = Q_1 R_1 = Q_2
  R_2$. Es gilt dann für die Hilfsmatrix $P = Q_2^T Q_1$
  \begin{gather}
    P = Q_2^{-1}Q_1 = R_2A^{-1}A R_1^{-1}
  \end{gather}
  und ebenso $P^T = R_1R_2^{-1}$. Beide Produkte auf der rechten Seite
  sind opere Dreiecksmatrizen, woraus folgt, dass $P$ diagonal sein
  muss. Wegen der Orthogonalität gilt $\abs{p_{ii}} = 1$ für
  $i=1,\dots,n$. Schließlich benutzen wir
  \begin{gather}
    PR_1 = Q_2^T Q_1 R_1 = Q_2^{T}A = Q_2^{T}Q_2R_2,
  \end{gather}
  woraus folgt $p_{ii} r_{1;ii} = r_{2;ii}$. Wegen der Positivität der
  Diagonalelemente von $R_1$ und $R_2$ ist damit $p_{ii}=1$ und
  $P=\id$. Aus $R_2R_1^{-1} = \id$ folgt dann $R_1 = R_2$ und
  \begin{gather}
    Q_1 = AR_1^{-1} = AR_2^{-1} = Q_2.
  \end{gather}
\end{proof}

\begin{Definition}{givens}
  Die \define{Givens-Rotation} $\Omega_{jk}$ mit $j<k$ zum Winkel $\theta$ bildet ab
  \begin{gather}
    \label{eq:qr:givens-1}
    \begin{aligned}
      \Omega_{jk}\colon \R^n &\to \R^n\\
      x & \mapsto y      
    \end{aligned}
    \qquad\qquad
    y =
    \begin{pmatrix}
      \id \\
      &c&\cdots&s\\
      &\vdots&\id &\vdots\\
      &-s&\cdots&c\\
      &&&&\id
    \end{pmatrix}
    x
  \end{gather}
  mit
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{sonst}
    \end{cases}
  \end{gather}
  mit $c = \cos\theta$ und $s = \sin\theta$.
\end{Definition}

\begin{remark}
  Die Einträge $c$ und $s$ in der
  Matrixdarstellung~\eqref{eq:qr:givens-1} befinden sich in den Zeilen
  $j$ und $k$. Einheitsmatrizen $\id$ sollen dann jeweils die passende
  Dimension haben.

  Multipliziert man $\Omega_{jk}$ von links an die Matrix $A$, so
  ändern sich nur die Zeilen $j$ und $k$, es ist also ebenfalls eine
  Zeilenoperation wie bei der Gauß-Elimination.

  Die Wirkung von $\Omega_{jk}$ auf einen Vektor ist eine Rotation des
  Vektors in der Ebene, die von den Einheitsvektoren $e_j$ und $e_k$
  aufgespannt wird. Wenn wir das im Kopf behalten, genügt es, sie als
  $2\times2$-Matrix aufzufassen.

  Die Bezeichnung $\Omega_{jk}$ scheint unzulänglich, da der Winkel
  $\theta$ fehlt. Dies liegt daran, dass wir im folgenden die Rotation
  nur dazu verwenden, $y_k=0$ zu erzielen.
\end{remark}

\begin{Lemma}{givens-berechnung}
  Mit Hilfe der Givens-Rotation kann aus dem Vektor $(x_j,x_k)^T$ die
  zweite Komponente eliminiert werden, indem man wählt
  \begin{gather}
    r = \sqrt{x_j^2+x_k^2},\qquad
    c = \frac{x_j}r,\quad s=c = \frac{x_k}r.
  \end{gather}
  Man erhält dann
  \begin{gather}
    \begin{pmatrix}
      c & s \\ -s & c
    \end{pmatrix}
    \begin{pmatrix}
      x_j\\x_k
    \end{pmatrix}
    =
    \begin{pmatrix}
      r\\0
    \end{pmatrix}
    .
  \end{gather}
\end{Lemma}

\begin{remark}
  Die Berechnung von $r$ im vorherigen Lemma kann bei der
  Implementation zu numerischem Überlauf führen, wenn eins der
  Argumente sehr groß ist. Es gibt in der Literatur einige
  Veröffentlichungen zu diesem Thema. Wir können bei der
  Implementation die Funktion \lstinline!hypot! benutzen, die für die
  Längen der beiden Katheten die Länge der Hypothenuse eines
  rechtwinkligen Dreiecks zurückgibt.
\end{remark}



\begin{remark}
  Setzt man die Givens-Rotation zur Reduktion der Spalte $k$ einer
  Matrix $A$ ein, so beginnt man am unteren Ende und benutzt
  $\Omega_{n-1,n}$ zur Elimination von $a_{nk}$ und arbeitet sich dann nach oben.
\end{remark}

\begin{Lemma}{qr-givens}
  Die QR-Zerlegung mit Givens-Rotation berechnet mit $A^{(1)} = A$ eine Folge von Matrizen
  \begin{gather}
    A^{(k+1)} = Q_k^T A^{(k)}, \qquad k=1,\dots,n-1,
  \end{gather}
  so dass $A^{(n)}=R$ obere Dreiecksgestalt hat. Es gilt
  \begin{gather}
    Q_k^T = \Omega_{n-1,n}\Omega_{n-1,n-2}\cdots\Omega_{k,k+1}
  \end{gather}
  mit den Givens-Rotationen $\Omega_{j-1,j}$, die jeweils das aktuelle
  Element $a_{jk}$ zu null setzen.
\end{Lemma}

\begin{Algorithmus*}{givens-2}{QR-Zerlegung mit Givens-Rotation}
  
\end{Algorithmus*}

\begin{Lemma}{qr-givens-aufwand}
  Der Aufwand der QR-Zerlegung mit Givens-Rotation beträgt
  \begin{gather}
    \frac43 n^3 + \bigo(n^2)
  \end{gather}
  Multiplikationen und Additionen plus $n^2/2$ Quadratwurzeln.
\end{Lemma}

\begin{Aufgabe}{givens-hessenberg}
  Eine obere \define{Hessenbergmatrix} ist eine Matrix mit nur einer
  unteren Nebendiagonalen, also
  \begin{gather}
    a_{ij} = 0\qquad \forall i>j+1.
  \end{gather}
  \begin{enumerate}
  \item Zeigen Sie, dass die QR-Zerlegung einer solchen Matrix mit nur
    $n-1$ mit Givens-Rotationen möglich ist.
  \end{enumerate}
\end{Aufgabe}

\begin{Definition*}{householder-reflexion}{Householder-Reflexion}
  Ist ein Vektor $w\in \R^n$ gegeben, so beschreibt die Matrix
  \begin{gather}
    Q_w = \identity - 2\frac{ww^T}{w^Tw}
  \end{gather}
  die Abbildung, die einen Vektor $y$ an der Hyperebene senkrecht zu
  $w$ spiegelt.
\end{Definition*}

\begin{Lemma}{householder-konstruktion}
  Sei $y\in\R^n$ gegeben. Dann gibt es zwei Vektoren $w^+,w^-\in\R^n$
  und eine Zahl $\alpha\in \R$, so dass
  \begin{gather}
    \label{eq:qr:3}
    Q_{w^\pm} y = \pm \alpha e_1 =
    \begin{pmatrix}
      \pm \alpha\\0\\\vdots\\0
    \end{pmatrix}.
  \end{gather}
\end{Lemma}

\begin{proof}
  Orthogonale Abbildungen sind normerhaltend. Daher muss gelten
  \begin{gather}
    \abs{\alpha} = \norm{y}_2.
  \end{gather}
  oder $\alpha = \pm\norm{y}_2$.  Ferner gilt
  wegen~\eqref{eq:qr:3} für einen geeigneten Reflexionsvektor $w$:
  \begin{gather}
    Q_{w} y = y-2 \frac{ww^Ty}{w^Tw} = y-2\frac{w^Ty}{w^Tw}w = \alpha e_1.
  \end{gather}
  Daher ist $w$ ein Vielfaches von $y - \alpha e_1$. Da durch die Norm
  von $w$ geteilt wird, wählen wir
  \begin{gather}
    w^{\pm} = y\mp \norm{y}_2 e_1.
  \end{gather}
\end{proof}

\begin{remark}
  Zur vermeidung von Auslöschung in der ersten Stelle von $w$
  verwendet man in der QR-Zerlegung den Faktor $\alpha$ so dass
  \begin{gather}
    \operatorname{sign} (\alpha) = -\operatorname{sign} (y_1)
  \end{gather}
  und damit
  \begin{gather}
    w = y + \operatorname{sign} (y_1) \norm{y}_2 e_1
    =
    \begin{pmatrix}
      y_1 + \operatorname{sign} (y_1) \norm{y}_2\\
      y_2\\\vdots\\y_n
    \end{pmatrix}.
  \end{gather}
\end{remark}

\begin{Algorithmus*}{householder-1}{Householder-Reflexion}
  
\end{Algorithmus*}

\begin{Algorithmus*}{householder-2}{QR-Zerlegung mit Householder-Reflexion}
  
\end{Algorithmus*}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
