\begin{intro}
  Betrachten wir die Konstruktion der LR-Zerlegung als Folge von
  Operationen auf Matrizen, so ergibt sich das Bild
  \begin{gather}
    \begin{split}
      A^{(1)} & \mapsto L_{1}^{-1} A^{(2)} \\
      A^{(2)} & \mapsto L_{2}^{-1} A^{(3)} \\
      A^{(n-1)} & \mapsto L_{n-1}^{-1} R.
    \end{split}
  \end{gather}
  Es findet also in jedem Schritt eine Umformung der Matrix des
  aktuellen Schritts mit einer Frobeniusmatrix statt.

  In der Fehlerabschätzung steht der Faktor
  \begin{gather}
    \rho = \frac{\alpha_{\max}}{\max \abs{a_{ij}}}
    = \frac{\max \abs{a^{(k)}_{ij}}}{\max \abs{a_{ij}}},
  \end{gather}
  wobei das Wachstum der Zähler von den Eigenschaften der
  Frobeniusmatrizen abhängt. Aus \cite{DeuflhardHohmann08} zitieren
  wir dazu folgende Werte abhängig von der Struktur der Matrix
  \begin{center}
    \begin{tabular}{l|c}
      Matrix & $\rho$\\\hline
      invertierbar & $2^{n-1}$ \\
      diagonaldominant & 2 \\
      s.p.d. & 1
    \end{tabular}
  \end{center}
  Hierbei wird für allgemein invertierbare Matrizen Spaltenpivotierung
  angewandt, für die anderen nicht. Für allgemeine Matrizen kann
  dieser Faktor also sehr schnell anwachsen und der Algorithmus
  instabil werden.

  Dieser Abschnitt beschäftigt sich nun mit alternativen
  Transformationen, die immer zu einer stabilen Zerlegung
  führen. Gesucht werden dazu Matrizen so dass
  \begin{gather}
    \norm{A^{(n-1)}} = \dots = \norm{A^{(k)}} = \dots = \norm{A^{(1)}}
  \end{gather}
  für eine geeignete Norm gilt.
\end{intro}

\subsection{Orthogonale Matrizen}

\begin{Definition}{ortho-matrix}
  Eine \define{orthogonale Matrix} ist eine quadratische Matrix, deren
  Spaltenvektoren bzw. deren Zeilenvektoren eine Orthonormalbasis des
  $\R^n$ bilden.
\end{Definition}

\begin{Satz}{ortho-matrix}
  Für eine orthogonale Matrix $Q$ gilt
  \begin{gather}
    \label{eq:qr:1}
    Q^{-1} = Q^T.
  \end{gather}
  Umgekehrt folgt aus dieser Beziehung die Orthogonalität der
  Zeilenvektoren und Spaltenvektoren.
\end{Satz}

\begin{proof}
  Nehmen wir an, die Spaltenvektoren $q^{(1)},\dots,q^{(n)}$ von $Q$
  seinen eine ONB. Dann gilt für die Matrix $A = Q^TQ$:
  \begin{gather}
    a_{ij} = \sum_{k=1}^n q_{ki}q_{kj}
    = \sum_{k=1}^n q^{(i)}_k q^{(j)}_k
    = \bigl(q^{(i)}\bigr)^T q^{(j)} = \delta_{ij}.
  \end{gather}
  Daher gilt $Q^TQ=I$.  Multiplizieren wir diese Gleichung von rechts
  mit $Q^{-1}$, so erhalten wir~\eqref{eq:qr:1}. Setzen wir umgekehrt
  $Q^TQ=I$, so ergibt obige Rechnung die Orthogonalität der
  Spaltenvektoren.

  Aus $Q^T = Q^{-1}$ folgt aber durch Transponieren
  \begin{gather}
    Q = Q^{-T},
  \end{gather}
  wobei $Q^{-T}$ die Inverse von $Q^T$ ist. Multiplizieren wir die
  letzte Gleichung von rechts mit $Q^T$, so erhalten wir $QQ^T = I$,
  was äquivalent zur Orthonormalität der Zeilenvektoren ist.

  Wir hätten diesen Beweis auch mit den Zeilenvektoren beginnen können
  und $QQ^T=I$ folgern. Der Rest verläuft dann analog.
\end{proof}

\begin{example}
  Die Rotationsmatrix
  \begin{gather}
    Q = \begin{bmatrix}
      \cos \phi & \sin \phi\\-\sin\phi&\cos\phi
    \end{bmatrix}
  \end{gather}
  ist orthogonal. Dasselbe gilt für die Reflexionsmatrix an einem
  Vektor $w\in \R^n$,
  \begin{gather}
    Q = I-2\frac{ww^T}{w^Tw}
  \end{gather}
\end{example}

\begin{Lemma}{qr-norm}
  Für jede orthogonale Matrix $Q\in \R^{n\times n}$, jeden Vektor
  $x\in\R^n$ und jede beliebige Matrix $A\in \R^{n\times n}$ gilt
  \begin{gather}
    \norm{Qx}_2 = \norm{x}_2,
    \qquad \norm{QA}_2 = \norm{A}_2.
  \end{gather}
\end{Lemma}

\subsection{Existenz und Konstruktion}

\begin{Definition}{qr}
  Bei der \define{QR-Zerlegung} wird die Matrix $A\in \R^{n\times n}$
  in das Produkt
  \begin{gather}
    A = QR
  \end{gather}
  aus einer orthogonalen Matrix $Q$ und einer oberen Dreiecksmatrix
  $R$ zerlegt.
\end{Definition}

\begin{Lemma}{qr-spalten}
  Seien $q^{(1)},\dots,q^{(n)}$ die Spaltenvektoren von $Q$ und
  $a^{(1)},\dots,a^{(n)}$ die Spaltenvektoren von $A$.
  Dann gilt
  \begin{gather}
    \label{eq:qr:2}
    a^{(k)} = \sum_{i=1}^k r_{ik} q^{(i)}.
  \end{gather}
  Gilt $r_{ii}\neq 0$ für $i=1,\dots,k$ so ist die Beziehung eindeutig
  umkehrbar.  Insbesondere besteht dann die Folge der Spaltenvektoren von
  $Q$ aus den orthogonalisierten Spaltenvektoren von $A$ mit
  \begin{align}
    \operatorname{span}\bigl\{q^{(1)},\dots,q^{(k)}\bigr\}
    = \operatorname{span}\bigl\{a^{(1)},\dots,a^{(k)}\bigr\}
    \qquad k=1,\dots,n.
  \end{align}
\end{Lemma}

\begin{Satz}{qr}
  Zu jeder invertierbaren quadratischen Matrix $A\in \R^{n\times n}$
  gibt es eine QR-Zerlegung. Unter der Zusatzbedingung $r_{ii}>0$ ist
  diese eindeutig.
\end{Satz}

\begin{proof}
  Nach dem vorigen Lemma können die Spaltenvektoren $q^{(i)}$ der
  Matrix $Q$ mit dem Gram-Schmidt-Verfahren aus den Spaltenvektoren
  $a^{(i)}$ von $A$ gewonnen werden. Da die Spalten von $A$ linear
  unabhängig sind, bricht das Verfahren nicht ab.

  Daraus folgt insbesondere
  Gleichung~\eqref{eq:qr:2} mit $r_{ij} = \scal(a^{(j)},q^{(i)})$.

  Zur Eindeutigkeit nehmen wir an, es gelte $A = Q_1 R_1 = Q_2
  R_2$. Es gilt dann für die Hilfsmatrix $P = Q_2^T Q_1$
  \begin{gather}
    P = Q_2^{-1}Q_1 = R_2A^{-1}A R_1^{-1}
  \end{gather}
  und ebenso $P^T = R_1R_2^{-1}$. Beide Produkte auf der rechten Seite
  sind opere Dreiecksmatrizen, woraus folgt, dass $P$ diagonal sein
  muss. Wegen der Orthogonalität gilt $\abs{p_{ii}} = 1$ für
  $i=1,\dots,n$. Schließlich benutzen wir
  \begin{gather}
    PR_1 = Q_2^T Q_1 R_1 = Q_2^{T}A = Q_2^{T}Q_2R_2,
  \end{gather}
  woraus folgt $p_{ii} r_{1;ii} = r_{2;ii}$. Wegen der Positivität der
  Diagonalelemente von $R_1$ und $R_2$ ist damit $p_{ii}=1$ und
  $P=\id$. Aus $R_2R_1^{-1} = \id$ folgt dann $R_1 = R_2$ und
  \begin{gather}
    Q_1 = AR_1^{-1} = AR_2^{-1} = Q_2.
  \end{gather}
\end{proof}

\begin{Definition}{givens}
  Die \define{Givens-Rotation} $\Omega_{jk}$ zum Winkel $\theta$ bildet ab
  \begin{align}
    \Omega_{jk}\colon \R^n &\to \R^n\\
    x & \mapsto y
  \end{align}
  mit
  \begin{gather}
    y_i =
    \begin{cases}
      c x_j + s x_k & i=j\\
      -s x_j + c x_k & i=k\\
      x_i &\text{sonst}
    \end{cases}
  \end{gather}
  mit $c = \cos\theta$ und $s = \sin\theta$.
\end{Definition}

\begin{Algorithmus*}{givens-1}{Givens-Rotation}
  
\end{Algorithmus*}

\begin{Algorithmus*}{givens-2}{QR-Zerlegung mit Givens-Rotation}
  
\end{Algorithmus*}

\begin{Definition*}{householder-reflexion}{Householder-Reflexion}
  Ist ein Vektor $w\in \R^n$ gegeben, so beschreibt die Matrix
  \begin{gather}
    Q_w = \identity - 2\frac{ww^T}{w^Tw}
  \end{gather}
  die Abbildung, die einen Vektor $y$ an der Hyperebene senkrecht zu
  $w$ spiegelt.
\end{Definition*}

\begin{Lemma}{householder-konstruktion}
  Sei $y\in\R^n$ gegeben. Dann gibt es zwei Vektoren $w^+,w^-\in\R^n$
  und eine Zahl $\alpha\in \R$, so dass
  \begin{gather}
    \label{eq:qr:3}
    Q_{w^\pm} y = \pm \alpha e_1 =
    \begin{pmatrix}
      \pm \alpha\\0\\\vdots\\0
    \end{pmatrix}.
  \end{gather}
\end{Lemma}

\begin{proof}
  Orthogonale Abbildungen sind normerhaltend. Daher muss gelten
  \begin{gather}
    \abs{\alpha} = \norm{y}_2.
  \end{gather}
  oder $\alpha = \pm\norm{y}_2$.  Ferner gilt
  wegen~\eqref{eq:qr:3} für einen geeigneten Reflexionsvektor $w$:
  \begin{gather}
    Q_{w} y = y-2 \frac{ww^Ty}{w^Tw} = y-2\frac{w^Ty}{w^Tw}w = \alpha e_1.
  \end{gather}
  Daher ist $w$ ein Vielfaches von $y - \alpha e_1$. Da durch die Norm
  von $w$ geteilt wird, wählen wir
  \begin{gather}
    w^{\pm} = y\mp \norm{y}_2 e_1.
  \end{gather}
\end{proof}

\begin{remark}
  Zur vermeidung von Auslöschung in der ersten Stelle von $w$
  verwendet man in der QR-Zerlegung den Faktor $\alpha$ so dass
  \begin{gather}
    \operatorname{sign} (\alpha) = -\operatorname{sign} (y_1)
  \end{gather}
  und damit
  \begin{gather}
    w = y + \operatorname{sign} (y_1) \norm{y}_2 e_1
    =
    \begin{pmatrix}
      y_1 + \operatorname{sign} (y_1) \norm{y}_2\\
      y_2\\\vdots\\y_n
    \end{pmatrix}.
  \end{gather}
\end{remark}

\begin{Algorithmus*}{householder-1}{Householder-Reflexion}
  
\end{Algorithmus*}

\begin{Algorithmus*}{householder-2}{QR-Zerlegung mit Householder-Reflexion}
  
\end{Algorithmus*}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
