\begin{Notation}{diag}
  Mit $\diag(a_1,a_2,\dots,a_r) \in \R^{m\times n}$ sei allgemein die $m\times n$-Matrix $A$ bezeichnet, deren erste $r$ Diagonalelemente die Werte $a_i$ annehmen. Alle anderen Einträge sind null. Sie hat die Darstellung
  \begin{gather}
    A = \diag(a_1,a_2,\dots,a_r)
    = \left(
      \begin{array}{ccc|c}
        a_1&& \mathbf 0&\\
        &\ddots&& \mathbf 0\\
        \mathbf 0&&a_r&\\\hline
        & \mathbf 0 && \mathbf 0
      \end{array}
      \right),
    \end{gather}
    wobei untere und rechte Nullblöcke leer sein dürfen.
\end{Notation}

\begin{Definition}{svd}
  Die \define{Singulärwertzerlegung} (engl.: \define{singular value decomposition}, \define{SVD}) einer Matrix
  $A\in \R^{m\times n}$ hat die Form
  \begin{gather}
    \label{eq:svd:1}
    A = U \Sigma V^T
  \end{gather}
  mit orthogonalen Matrizen $U\in \R^{m\times m}$ und
  $V\in \R^{n\times n}$. Die Matrix $\Sigma\in\R^{m\times n}$ ist
  \begin{gather}
    \Sigma = \diag(\sigma_1,\dots,\sigma_r)
  \end{gather}
  mit positiven, reellen Einträgen $\sigma_1,\dots,\sigma_r$
  und $r\le \min\{m,n\}$, den \define{Singulärwerten}.
  Die Singulärwerte seien der Größe nach fallend sortiert.
\end{Definition}


\begin{Satz}{svd}
  Jede Matrix $A\in \R^{m\times n}$ besitzt eine Singulärwertzerlegung.
\end{Satz}

\begin{proof}
  Siehe auch~\cite[Satz 4.11]{Rannacher17}.
  Der Beweis läuft induktiv über die Spalten von $U$ und $V$. Wir
  bemerken zunächst, dass es wegen der Stetigkeit der Norm einen
  Vektor $x\in \R^n$ mit $\norm{x}_2 = 1$ gibt, so dass
  \begin{gather}
    \norm{Ax}_2 = \norm{A}_2\norm{x}_2.
  \end{gather}
  Wir definieren $\sigma_1 = \norm{A}$ und es sei $y\in \R^m$ so dass
  $\sigma_1y=Ax$. Wir ergänzen $x$ und $y$ jeweils zu Orthogonalbasen
  und nennen die Matrizen der Basisvektoren $U^{(1)}$ und
  $V^{(1)}$. Es gilt dann
  \begin{gather}
    \left(U^{(1)}\right)^T A^{(1)} V^{(1)} =
    \begin{pmatrix}
      \sigma& w^T\\0 & B
    \end{pmatrix}
  \end{gather}
  mit einem Vektor $w\in \R^{n-1}$ und einer Matrix
  $b\in \R^{m-1\times n-1}$. Da $U$ und $V$ orthogonal sind, gilt
  \begin{gather}
    \norm{A^{(1)}} = \norm{A} = \sigma.
  \end{gather}
  Multipliziert man die Matrix $A^{(1)}$ mit dem Vektor
  $ z=(\sigma,w)^T$, so erhält man
  \begin{gather}
    A^{(1)}z = 
    \begin{pmatrix}
    \sigma&w^T\\0&B
  \end{pmatrix}
  \begin{pmatrix}
    \sigma\\w
  \end{pmatrix}
  =
  \begin{pmatrix}
    \sigma^2+\norm{w}^2\\
    Bw
  \end{pmatrix}
\end{gather}
Daher gilt
\begin{gather}
  \norm{A^{(1)}z}_2^2 = \bigl(\sigma^2+\norm{w}^2\bigr)^2 + \norm{Bw}_2^2
  \ge (\sigma^2+\norm{w}^2) \norm{z}_2^2.
\end{gather}
Daher muss $\norm{w}=0$ und damit $w=0$ gelten. Die Matrix hat also
die Gestalt
  \begin{gather}
    \left(U^{(1)}\right)^T A^{(1)} V^{(1)} =
    \begin{pmatrix}
      \sigma& 0\\0 & B
    \end{pmatrix}.
  \end{gather}
  Nun wenden wir induktiv dasselbe Verfahren auf $B$ an.

  An einem Punkt kann es vorkommen, dass $\norm{B}=0$. In diesem Fall
  können wir die Konstruktion bereits abbrechen, da nun alle weiteren
  Vektoren im Kern der Matrix liegen. Zu diesen können wir beliebige
  Bildvektoren im orthogonalen Komplement des bereits konstruierten
  Raums wählen.
\end{proof}

\begin{Lemma}{svd-eigenschaften}
  Für die Singulärwertzerlegung $A=U\Sigma V^T$ gilt für die Spalten
  von $U$ und $V$
  \begin{gather}
    \label{eq:svd:2}
    A v^{(i)} = \sigma_i u^{(i)}, \qquad
    A^T u^{(i)} = \sigma v^{(i)},\qquad
    i \le \min\{m,n\}.
  \end{gather}
  Sei $\sigma_r\neq0$ der letzte von null verschiedene
  Singulärwert. Dann gilt für den \putindex{Rang} der Matrix $\rank A = r$
  und
  \begin{gather}
    \label{eq:svd:3}
    \begin{aligned}
      \range A &= \spann{u^{(1)},\dots,u^{(r)}}&
      \ker A &= \spann{v^{(r+1)},\dots,v^{(n)}}\\
      \range{A^T} &= \spann{v^{(1)},\dots,v^{(r)}}&
      \ker{A^T} &= \spann{u^{(r+1)},\dots,u^{(m)}}
    \end{aligned}
  \end{gather}
\end{Lemma}

\begin{proof}
  Die Eigenschaften~\eqref{eq:svd:2} liest man direkt der Darstellung
  ab, denn aufgrund der Orthogonalität gilt
  $V^T v^{(i)} = e_i \in \R^n$. Damit gilt
  $\Sigma V^Tv^{(i)} = \sigma_i e_i \in \R^m$. Schliesslich selektiert
  $Ue_i$ den $i$-ten Spaltenvektor von $U$. Für die transponierte
  Matrix gilt dies wegen
  \begin{gather}
    A^T = V \Sigma^T U^T,
  \end{gather}
  wobei bei der Transposition von $\Sigma$ nur die Dimensionen
  getauscht werden, die Diagonalelemente bleiben natürlich gleich.

  Die Aussage~\eqref{eq:svd:3} ist dann eine direkte
  Folge. Insbesondere ist der Rang der Matrix durch den Index des
  letzten positiven Singulärwerts charakterisiert.
\end{proof}

\begin{remark}
  Aus Gleichung~\eqref{eq:svd:3} lesen wir direkt die bekannten
  Beziehungen aus der linearen Algebra ab. Oder umgekehrt, die
  Singulärwertzerlegung erzeugt Orthonormalbasen der Vektorräume, die
  in Kern und orthogonales Komplement zerlegen. Zusätzlich wird der
  (eingeschränkte) Isomorphismus noch diagonalisiert.
\end{remark}

\begin{Satz}{minimalloesung}
  Sei $A\in\R^{m\times n}$ mit Singulärwertzerlegung $A=U\Sigma V^T$
  und Rang $r$. Sei
  \begin{gather}
    \Sigma^+ = \diag\left(\tfrac1{\sigma_1},\tfrac1{\sigma_2},\dots,\tfrac1{\sigma_r}\right) \in \R^{m\times n}.
  \end{gather}
  Dann ist der Vektor $x^*\in \R^n$ mit
  \begin{gather}
    \label{eq:svd:4}
    x^* = V \Sigma^+ U^T b
  \end{gather}
  die eindeutig bestimmte Lösung der Normalengleichungen mit minimaler
  Norm. Für das Residuum gilt
  \begin{gather}
    \label{eq:svd:5}
    \norm{Ax^*-b}_2^2 = \sum_{i=r+1}^m \Bigl(\bigl(u^{(i)}\bigr)^T b\Bigr)^2.
  \end{gather}
\end{Satz}

\begin{proof}
  Sei $x\in \R^n$ und $z=V^T x$ sei seine Koordinatendarstellung in
  der Basis $V$. Dann gilt
  \begin{align}
    \norm{Ax-b}_2^2
    &= \norm{AVV^Tx - b}_2^2\\
    &= \norm{U^TAV z - U^Tb}_2^2\\
    &= \norm{\Sigma z - U^T b}_2^2\\
    &= \sum_{i=1}^r \Bigl(\sigma_iz_i - \bigl(u^{(i)}\bigr)^T b\Bigr)^2
      +\sum_{i=r+1}^m \Bigr(\bigl(u^{(i)}\bigr)^T b\Bigr)^2.
  \end{align}
  Da alle Summanden nichtnegativ sind, wird das Minimum für
  \begin{gather}
    z_i = \frac1{\sigma_i}\bigl(u^{(i)}\bigr)^T b, \qquad i=1,\dots,r
  \end{gather}
  angenommen. Damit verschwindet die erste Summe und~\eqref{eq:svd:5}
  ist für den so bestimmten Vektor $z$ bewiesen. Offensichtlich ist
  die Norm des Vektors $z$ minimal, wenn alle weiteren Komponenten
  verschwinden, also
  \begin{gather}
    z_i=0,\qquad i=r+1,\dots,n.
  \end{gather}
  Damit können wir zusammenfassend schreiben
  \begin{gather}
    z = \Sigma^+ U^T b.
  \end{gather}
  Da $V$ orthogonal ist, überträgt sich die Minimalitätseigenschaft auf $x^*=Vz$
\end{proof}

\begin{remark}
  Für den Fall eine invertierbaren Matrix $A\in\R^{n\times n}$
  entspricht~\eqref{eq:svd:4} gerade der Inversen des Produkts. Im
  Falle das $A$ vollen Rank hat mit $m\ge n$ bekommen wir die
  eindeutige Lösung der Normalengleichungen und die Bedingung \glqq
  mit minimaler Norm\grqq{} entfällt.
\end{remark}

\begin{Definition}{pseudoinverse}
  Die Matrix $A^+ = V\Sigma^+ U^T \in \R^{n\times m}$ ist eine
  Verallgemeinerung der Inversen, die als \define{Pseudoinverse}, auch
  als \define{Moore-Penrose-Inverse} bezeichnet wird. Sie ist für jede
  Matrix $A\in \R^{m\times n}$ definiert.
\end{Definition}

\begin{Satz*}{penrose}{Penrose-Axiome}
  Für die Pseudoinverse $A^+\in \R^{n\times m}$ einer Matrix
  $A\in \R^{m\times n}$ gelten folgende Gleichungen:
  \begin{align}
    \bigl(A^+A\bigr)^T &= A^+A,\\
    \bigl(AA^+\bigr)^T &= AA^+,\\
    \label{eq:svd:6c}
    A^+AA^+ &= A^+,\\
    AA^+A &= A.
  \end{align}
  Insbesondere ist $A^+A$ die orthogonale Projektion auf $\range{A^T}$
  und $AA^+$ die orthogonale Projektion auf $\range A$.
\end{Satz*}

\begin{proof}
  Es gilt
  \begin{gather}
    A^+A = V\Sigma^+U^T U \Sigma V^T = V \Sigma^+\Sigma V^T = V E_r V^T,
  \end{gather}
  wobei $E_r\in \R^{n\times n}$ mit
  \begin{gather}
    E_r = \diag(\underbrace{1,\dots,1}_{r \text{ mal}}).
  \end{gather}
  Daraus folgen sofort Symmetrie und Projektionseigenschaft, sowie aus
  letzterer~\eqref{eq:svd:6c}. Dieseben Argumente bleiben korrekt,
  wenn man $A^+$ und $A$ vertauscht, wobei dann
  $E_r\in \R^{m\times m}$ ist.
\end{proof}

\begin{remark}
  Die Eigenschaft eines Vektors, im Nullraum der Matrix $A$ zu liegen
  ist natürlich nicht invariant unter Störungen von $A$. Im Gegenteil
  wird im Allgemeinen die kleinste Störung dazu führen, dass alle
  Eigenwerte von null verschieden sind. Daher benötigen wir für die
  stabile Zerlegung eines Vektorraums in den Kern und sein
  orthogonales Komplement ein neues Konzept des Nullraums bzw. des
  Rangs einer Matrix.
\end{remark}

\begin{Definition}{epsilon-rang}
  Zu einer positiven Zahl $\epsilon$ ist der $\epsilon$-Rang einer
  Matrix $A$ ist definiert als
  \begin{gather}
    \rank_\epsilon{A} = \min_{\norm{A-B}_2\le\epsilon} \rank B.
  \end{gather}
\end{Definition}

\begin{Satz}{rang-approximation}
  Sei $A\in \R^{m\times n}$ eine Matrix vom Rang $r$ mit
  Singulärwertzerlegung $A = U\Sigma V^T$. Sei dazu
  $A_k = U\Sigma_k V^T$ die abgeschnittene SVD mit
  \begin{gather}
    \Sigma_k = \diag(\sigma_1,\dots,\sigma_k).
  \end{gather}
  Dann ist $A_k$ die Bestapproximation zu $A$ von maximalem Rang $k$ in der
  Spektralnorm, also
  \begin{gather}
    \norm{A-A_k}_2 = \min_{\rank B\le k} \norm{A-B}_2.
  \end{gather}
  Es gilt ferner, dass
  \begin{gather}
    \norm{A-A_k}_2 = \sigma_{k+1}.
  \end{gather}
\end{Satz}

\begin{proof}
  Es gilt
  \begin{gather}
    U^T (A-A_k) V = \diag(0,\dots,0,\sigma_{k+1},\dots,\sigma_r).
  \end{gather}
  Da $U$ und $V$ orthogonal sind, gilt damit
  \begin{gather}
    \norm{A-A_k}_2 = \sigma_{k+1}.
  \end{gather}
  Nun müssen wir zeigen, dass für jede Matrix $B$ mit $\rank B\le k$ gilt
  \begin{gather}
    \norm{A-B}_2 \ge \sigma_{k+1}.
  \end{gather}
  Es gilt $\dim \ker B \ge n-k$. Daher hat dieser Nullraum einen
  nichtleeren Schnitt mit dem Erzeugnis der ersten $k+1$
  Spaltenvektoren von $V$. Sei $w$ ein Vektor in dieser Schnittmenge
  mit $\norm{w}_2=1$. Es gilt dann $Bw=0$. Schreiben wir
  \begin{gather}
    w = \sum_{i=1}^{k+1} z_i v^{(i)},
  \end{gather}
  so gilt nach der Parsevalschen Gleichung
  $\norm{z}_2 = \norm{w}_2 = 1$ und wir erhalten
  \begin{gather}
    Aw = \sum_{i=1}^{k+1} \sigma_i z_i u^{(i)}.
  \end{gather}
  Es folgt
  \begin{multline}
    \norm{A-B}_2^2
    \ge \norm{(A-B)w}_2^2
    = \norm{Aw}_2^2 \\
    = \sum_{i=1}^{k+1} \sigma_i^2 z_i^2
    \ge \sigma_{k+1}^2 \norm{z}_2^2 = \sigma_{k+1}^2.
  \end{multline}
  Für die zweite Ungleichung haben wir ausgenutzt, dass die
  Singulärwerte der Größe nach sortiert sind.
\end{proof}

\begin{Korollar}{epsilon-rang}
  Der $\epsilon$-Rang einer Matrix $A$ mit Singulärwertzerlegung
  $A=U\Sigma V^T$ ist die Anzahl $r$ der Singulärwerte $\sigma_k>\epsilon$. Es gilt also
  \begin{gather}
    \sigma_1 \ge \dots \ge \sigma_{r}
    > \epsilon \ge \sigma_{r+1} \ge \cdots
  \end{gather}
\end{Korollar}

\begin{proof}
  Im vorherigen Satz haben wir bewiesen, dass für die Matrix $A_{r} = U\Sigma_r V^T$ gilt
  \begin{gather}
    \norm{A-A_r}_2 = \sigma_{r+1} \le \epsilon.
  \end{gather}
  Damit gilt also bereits
  \begin{gather}
    \rank_\epsilon A \le \rank A_r = r.
  \end{gather}
  Gäbe es nun eine Matrix $B$ vom Rang $k<r$ mit
  $\norm{A-B}_2 \le \epsilon$, so gälte nach dem
  Bestapproximationsresultat
  \begin{gather}
    \sigma_{k+1} = \norm{A-A_k} \le \norm{A-B} \le \epsilon < \sigma_r
  \end{gather}
  im Widerspruch zur Annahme, dass die Singulärwerte der Größe nach
  sortiert sind.
\end{proof}

\begin{remark}
  Das Korollar bietet eine einfachere Definition des $\epsilon$-Rangs
  einer Matrix, nämlich als Anzahl aller Singulärwerte größer als
  $\epsilon$. Das ist konsistent mit der Aussage, dass der Rang einer
  Matrix die Anzahl der von null verschiedenen Singulärwerte ist.
\end{remark}

\begin{remark}
  Die Bedeutung des $\epsilon$-Rangs besteht darin, dass bei der
  Berechnung der Pseudoinversen die Invertierung der Diagonalelemente
  nicht erst bei $\sigma_k=0$, sondern bereits bei
  $\sigma_k\le \epsilon$ stoppen sollte, also
  \begin{gather}
    \Sigma^+_\epsilon = \diag\left(\tfrac1{\sigma_1},\tfrac1{\sigma_2},\dots,\tfrac1{\sigma_{r_\epsilon}}\right).
  \end{gather}
  Die folgenden von null verschiedenen Singulärwerte
  $\sigma_k\le \epsilon$ dürfen nicht invertiert werden, da sonst das
  Ergebnis der Multiplikation mit der Matrix
  $A^+ = V\Sigma^+_\epsilon U^T$ von diesen Werten dominiert
  würde. Die Elemente von $\Sigma^+$ an diesen Stellen setzen wir zu
  null, da wir die Singulärwerte ja numerisch als null ansehen.

  Die Singulärwertzerelgung bietet also neben der Möglichkeit, die
  Pseudoinverse zu berechnen auch Kontrolle darüber, wie groß die Norm
  der inversen einer fast singulären Matrix werden darf.
\end{remark}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
