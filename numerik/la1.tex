\subsection{Grundlagen}

\begin{Definition}{norm}
  Eine \define{Norm} $\norm{\cdot}$ auf dem Vektorraum $V$ ist eine Abbildung
  \begin{gather}
    \begin{split}
      \norm{\cdot}\colon V &\to \R\\
      x&\mapsto \norm{x}
    \end{split}
  \end{gather}
  mit den Eigenschaften
    \begin{xalignat}3
    &\text{Homogenität:}
    &\norm{\alpha x} &= \abs{\alpha}\norm{x}
    &\forall \alpha&\in\R,x\in V
    \\
    &\text{Dreiecksungleichung:}
    &\norm{x+y} &\le \norm{x}+\norm{y}
    &\forall x,y&\in V
    \\
    &\text{Definitheit:}
    &\norm{x} & \ge 0
    &\forall x&\in V
    \\
    &&\norm{x} &\neq 0
    &\forall x&\neq0
    \end{xalignat}
  Verzichtet man auf die zweite Definitheitsbedingung, so erhält man eine \define{Seminorm}.
\end{Definition}

\begin{Definition}{norm-aequivalenz}
  Sei $V$ ein reeller oder komplexer Vektorraum. Zwei Normen
  $\norm{\cdot}_X$ und $\norm{\cdot}_Y$ auf $V$ heißen äquivalent,
  wenn es Konstanten $c>0$ und $C>0$ gibt, so dass
  \begin{gather}
    c \norm{v}_X \le \norm{v}_Y \le C \norm{v}_X
    \qquad\forall v\in V.
  \end{gather}
\end{Definition}

\begin{Definition}{rn-konvergenz}
  Eine Folge $\{x^{(k)}\}\subset \R^n$ für $k=1,2,\dots$ heißt
  \textbf{komponentenweise konvergent} gegen $x\in \R^n$, wenn gilt
  \begin{gather}
    \forall \epsilon>0\;
    \exists k_0\in \mathbb N\;
    \forall k\ge k_0, i=1,\dots,n
    : \abs{x^{(k)}_i - x_i} < \epsilon.
  \end{gather}
  Die Folge heißt konvergent unter der Norm $\norm{\cdot}$ wenn gilt
  \begin{gather}
    \forall \epsilon>0\;
    \exists k_0\in \mathbb N\;
    \forall k\ge k_0
    : \norm{x^{(k)} - x} < \epsilon.
  \end{gather}
\end{Definition}

\begin{Lemma}{norm-aequivalenz}
  Sei $\norm{\cdot}$ eine beliebige Norm auf $\R_n$. Dann ist die Abbildung
  \begin{gather}
    f\colon x \mapsto \norm{x}
  \end{gather}
  stetig bezüglich der komponentenweisen Konvergenz. Ferner ist die
  Norm $\norm{\cdot}$ äquivalent zur Maximumsnorm.
\end{Lemma}

\begin{proof}
  Für den ersten Teil ist zu zeigen, dass zu einer komponentenweise
  konvergenten Folge von Vektoren auch deren Norm konvergiert. Sei
  $\{x^{(k)}\}$ eine solche Folge und dazu $k_0$ so gewählt, dass
  \begin{gather}
    \max_{i=1,\dots,n}\left\lvert\left(x_i^{(k)}-x_i\right) \norm{e_i}\right\rvert
      < \frac\epsilon n
    \qquad \forall k\ge k_0.
  \end{gather}
  Hier ist $e_i$ der $i$-te Einheitsvektor. Dann folgt
  \begin{align}
    \norm{x^{(k)}-x}
    &= \left\lVert\sum_{i=1}^n \left(x_i^{(k)}-x_i\right) e_i\right\rVert
    \\
    &\le \sum_{i=1}^n \left\lVert\left(x_i^{(k)}-x_i\right) e_i\right\rVert
    \\
    & < n \frac\epsilon n = \epsilon.
  \end{align}
  Hiermit haben wir bereits bewiesen, dass komponentenweise Konvergenz
  auch Normkonvergenz impliziert.

  Die \glqq{}Einheitssphäre\grqq{}
   \begin{gather}
     S = \bigl\{ x\in \R^n \big| \norm{x}_\infty = 1 \bigr\}
   \end{gather}
   ist beschränkt und bezüglich der komponentenweisen Konvergenz
   abgeschlossen. Die Norm $\norm.$ nimmt dort als stetige Funktion
   ihr Minimum $c$ und ihr Maximum $C$ an. Insbesondere gilt aber
   wegen der Definitheit $c > 0$. Für einen beliebigen Vektor
   $x\in \R^n$ ist $x/\norm{x}_\infty \in S$, so dass gilt
   \begin{gather}
     c \norm{x}_\infty \le \norm{x} \le C \norm{x}_\infty.
   \end{gather}
\end{proof}

\begin{Satz}{norm-aequivalenz}
  Auf $\R^n$ sind zwei beliebige Normen $\norm._X$ und $\norm._Y$ äquivalent.
\end{Satz}

\begin{proof}
  Nach dem vorherigen Lemma sind beide Normen äquivalent zur Maximumsnorm. Es gibt also Konstanten $c_X, c_Y, C_X, C_y>0$ mit
  \begin{gather}
    \begin{aligned}
     c_X \norm{x}_\infty &\le &\norm{x}_X \;&\le C_X \norm{x}_\infty\\
     c_Y \norm{x}_\infty &\le &\norm{x}_Y \;&\le C_Y \norm{x}_\infty.
    \end{aligned}
  \end{gather}
  Daher gilt
  \begin{gather}
    \begin{split}
      \norm{x}_Y \le C_Y\norm{x}_\infty &\le \frac{C_Y}{c_X} \norm{x}_X\\
      \norm{x}_X \le C_X\norm{x}_\infty &\le \frac{C_X}{c_y} \norm{x}_Y
    \end{split}
  \end{gather}
\end{proof}


\begin{Definition}{matrix-norm}
  Auf dem Vektorraum der Matrizen $\R^{m\times n}$ ist durch
  \slideref{Definition}{norm} eine Norm definiert. Gilt zusätzlich
  \begin{gather}
    \norm{Ax} \le \norm{A}\norm{x}
    \qquad\forall A\in \R^{m\times n}, x\in \R^n,
  \end{gather}
  so heißt die Norm $\norm.$ der Matrix \define{verträglich} mit der
  Vektornorm $\norm.$. Wir sprechen von einer \define{Matrixnorm},
  wenn sie zusätzlich \define{submultiplikativ} ist, dass heißt,
  für alle Matrizen $A,B$ passender Dimensionen gilt
  \begin{gather}
    \norm{AB} \le \norm{A}\norm{B}.
  \end{gather}
  Ferner definieren wir die \define{Operatornorm} oder \define{natürliche Norm}
  \begin{gather}
    \norm{A} = \sup_{\substack{x\in\R^n\\x\neq0}} \frac{\norm{Ax}}{\norm{x}}
    = \sup_{\norm{x}=1}\norm{Ax}.
  \end{gather}
\end{Definition}

\begin{Lemma}{operator-norm}
  Die Operatornorm ist verträglich und submultiplikativ.
\end{Lemma}

\begin{Beispiel}{Zeilensummen}
  Die Operatornormen zu den Vektornormen $\norm._1$ und
  $\norm._\infty$ sind die \define{Spaltensummennorm} und die
  \define{Zeilensummennorm}
  \begin{align}
    \norm{A}_1 &= \max_{i=1,\dots,n} \sum_{j=1}^n \abs{a_{ji}}\\
    \norm{A}_\infty &= \max_{i=1,\dots,n} \sum_{j=1}^n \abs{a_{ij}}
  \end{align}
\end{Beispiel}

\subsection{Eigenwerte und die Spektralnorm}

\begin{Definition}{eigenwert}
  Sei $A\in \R^{n\times n}$. Gilt für einen Vektor $0\neq x\in \R^n$
  \begin{gather}
    Ax = \lambda x,
  \end{gather}
  so nennen wir $\lambda$ \define{Eigenwert} von $A$ und $x$ einen
  zugehörigen \define{Eigenvektor}. Wir notieren die Zugehörigkeit zur
  Matrix $A$ auch explizit durch $\lambda(A)$.
\end{Definition}

\begin{Lemma}{ew-norm}
  Für alle Eigenwerte $\lambda\in\C$ einer Matrix $A\in\R^{n\times n}$ gilt
  \begin{gather}
    \abs{\lambda} \le \norm{A}
  \end{gather}
  für jede zu einer beliebigen Vektornorm verträglichen Norm.
\end{Lemma}

\begin{Satz}{onb-ev}
  Sei $A\in \R^n\times n$ eine symmetrische Matrix. Dann gibt es eine
  orthonormalbasis des $\R^n$ von Eigenvektoren $v^{(i)}$ mit zugehörigen
  reellen Eigenwerten $\lambda_i$.
\end{Satz}

\begin{proof}
  Resultat der linearen Algebra.
\end{proof}

\begin{Satz}{spektralnorm}
  Die Operatornorm zur euklidischen Norm\index{Norm|euklidisch} ist
  die \define{Spektralnorm}
  \begin{gather}
    \norm{A}_2 = \max_{\substack{x\in\R^n\\x\neq0}}
    \sqrt{\frac{x^TA^TAx}{x^Tx}} = \sqrt{\lambda_{\max}(A^TA)}.
  \end{gather}
  Insbesondere gilt für symmetrische Matrizen
  $\norm{A}_2 = \max_i\abs{\lambda_i(A)}$.
\end{Satz}

\begin{proof}
  Nach \slideref{Satz}{onb-ev} gibt es eine Basis des $\R^n$ von
  Eigenvektoren $v^{(i)}$ von $A^TA$. Jeder beliebige Vektor $x$
  besitzt damit die Darstellung
  \begin{gather}
    x = \sum_{i=1}^n \alpha_i v^{(i)}.
  \end{gather}
  Es gilt nach der Parsevalschen Gleichung
  $\norm{x}_2 = \norm{\alpha}_2$. Ferner gilt mit den Eigenwerten
  $\lambda_i = \lambda_i(A^TA)$
  \begin{gather}
    \norm{Ax}_2^2 = x^TA^TAx = \sum_{i=1}^n \lambda_i \alpha_i^2.
  \end{gather}
  Daher gilt
  \begin{gather}
    \norm{A}_2^2 = \max_{x\in \R^n} \frac{\norm{Ax}^2}{\norm{x}^2}
    = \max_{\alpha} \frac{\sum \lambda_i\alpha_i^2}{\sum\alpha_i^2}
    \le \lambda_{\max}(A^TA).
  \end{gather}
  Da für symmetrische Matrizen $A=A^T$, so ist
  \begin{gather}
    \lambda_{\max}(A^TA) = \lambda_{\max}(A^2) = \lambda_{\max}^2(A)
  \end{gather}
\end{proof}

\begin{Definition}{pos-def}
  Eine Matrix $A\in\R^{n\times n}$ heißt \define{positiv definit}, wenn
  \begin{gather}
    x^TAx > 0 \qquad\forall 0\neq x\in \R^n.
  \end{gather}
\end{Definition}

\begin{Satz}{spd}
  Eine symmetrische Matrix $A\in\R^{n\times n}$ ist positiv definit genau dann, wenn ihre Eigenwerte alle positiv sind.
\end{Satz}

\section{Fehleranalyse}

\subsection{Konditionierung der Lösung}

\begin{Definition}{aufgabe-loesung}
  Die Aufgabe, das lineare Gleichungssystem
  \begin{gather}
    Ax=b
  \end{gather}
  zu lösen wandelt die Eingabedaten $(A,b)$ in das Ausgabedatum $x$
  um. Die zugehörige gestörte Aufgabe ist
  \begin{gather}
    (A+\delta A) (x+\delta x) = b+ \delta b,
  \end{gather}
  wobei $\delta A$ und $\delta b$ eine Matrix und ein Vektor sind, um
  die die Eingabedaten gestört sind. $\delta x$ ist die resultierende
  Störung der Lösung.
  
  Die Untersuchung der Konditionierung dieser Aufgabe besteht in
  der Bestimmung einer relativen \putindex{Konditionszahl} $\kappa$, so dass
  \begin{gather}
    \frac{\norm{\delta x}}{\norm{x}}
    \le \kappa \left[\frac{\norm{\delta A}}{\norm{A}}
      +\frac{\norm{\delta b}}{\norm{b}}\right].
  \end{gather}
\end{Definition}

\begin{Lemma}{gestoert-invertierbar}
  Sei $B\in \R^{n\times n}$ mit $\norm{B} < 1$. Dann ist $I-B$
  invertierbar und es gilt
  \begin{gather}
    \norm{(I-B)^{-1}} \le (1-\norm{B})^{-1}
  \end{gather}
\end{Lemma}

\begin{proof}
  Siehe \cite[Hilfssatz 4.4]{Rannacher17}.
\end{proof}

\begin{Satz}{kondition-lgs}
  Sei die Matrix $A\in\R^{n\times n}$ invertierbar und
  \begin{gather}
    \norm{\delta A} < \norm{A}^{-1}.
  \end{gather}
  Dann ist die gestörte Matrix $A+\delta A$ ebenfalls invertierbar und
  es gilt die Fehlerabschätzung
  \begin{gather}
    \frac{\norm{\delta x}}{\norm{x}}
    \le \frac{\cond(A)}{1-\cond(A)\nicefrac{\norm{\delta A}}{\norm{A}}}
    \left[\frac{\norm{\delta A}}{\norm{A}}
      +\frac{\norm{\delta b}}{\norm{b}}\right].
  \end{gather}
  Hierzu definieren wir die \define{Konditionszahl} der Matrix $A$ zur
  Norm $\norm{\cdot}$
  \begin{gather}
    \cond(A) = \norm{A} \,\norm{A^{-1}}
  \end{gather}
\end{Satz}

\begin{proof}
  Siehe \cite[Satz 4.1]{Rannacher17}.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
