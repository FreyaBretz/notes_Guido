\subsection{Definition und Konditionsabschätzung}

\begin{Definition}{lagrange-interpolation}
  Die \define{Interpolation}saufgabe nach Lagrange lautet: seien $n+1$
  paarweise verschiedene \define{Stützstellen} $x_0,\dots,x_n$ mit
  zugehörigen Funktionswerten $f_i$ gegeben. Finde ein Polynom
  $p\in \P_n$ mit der Eigenschaft
  \begin{gather}
    p(x_i) = f_i.
  \end{gather}
  Alternativ ist die Interpolationsaufgabe aufzufassen als eine Abbildung
  \begin{gather}
    \begin{split}
      I_n\colon C[a,b] &\to \P_n\\
      p(x_i) &= f(x_i),
    \end{split}
  \end{gather}
  wobei das Interval $[a,b]$ alle Stützpunkte enthält. 
  Wir nennen diese Abbildung den
  \define{Lagrange-Interpolationsoperator} oder kurz
  \define{Lagrange-Interpolation}.
\end{Definition}

\begin{Satz}{lagrange-interpolation}
  Die Interpolationsaufgabe nach Lagrange hat eine eindeutige Lösung,
  bezeichnet als (Lagrange-)\define{Interpolierende} der Funktion $f$
  \begin{gather}
    p(x;f;x_0,\dots,x_n)
  \end{gather}
\end{Satz}
\begin{proof}
  Der Beweis ist eine direkte Konsequenz des folgenden Lemmas.
\end{proof}

\begin{Lemma}{lagrange-basis}
  Seien die Punkte $x_0,\dots,x_n$ paarweise verschieden. Dann gilt
  für die \define{Lagrange-Polynome}
  \begin{gather}
    \plagrange_i(x) = \plagrange_{i;n}(x) = \plagrange_{i;x_0,\dots,x_n}(x)
    = \prod_{\substack{j=0\\j\neq i}}^n \frac{x-x_j}{x_i-x_j}
  \end{gather}
  die \define{Interpolationseigenschaft}
  \begin{gather}
    \plagrange_i(x_j) = \delta_{ij},\qquad 0 \le i,j \le n.
  \end{gather}
  Sie sind linear unabhängig und formen eine Basis von $\P_n$.

  Die Lagrange-Polynome sind \putindex{orthonormal} bezüglich des
  Skalarprodukts
  \begin{gather}
    \scal(p,q) = \sum_{i=0}^n p(x_i)q(x_i).
  \end{gather}
\end{Lemma}

\begin{proof}
  Der Beweis der Basiseigenschaft ist Übungsaufgabe. Es bleibt zu
  zeigen, dass $\scal(\cdot,\cdot)$ ein Skalarprodukt ist und die
  Polynome orthogonal. Dabei sind die Symmetrie und Homogenität und
  Orthogonalität elementar nachzurechnen. Was zu zeigen bleibt, ist
  die Definitheit, also für $p\in \P_n$:
  \begin{gather}
    \scal(p,p) = \sum_{i=0}^n p(x_i)^2 = 0
    \qquad \Rightarrow\qquad
    p= 0.
  \end{gather}
  Dazu nutzen wir die Basiseigenschaft der Polynome $\plagrange_j$ und
  schreiben mit geeigneten Koeffizienten $\alpha_j$:
  \begin{gather}
    p(x) = \sum_{j=0}^n \alpha_j \plagrange_j(x).
  \end{gather}
  Es gilt dann wegen der Interpolationseigenschaft
  \begin{gather}
    \scal(p,p) = \sum_{i=0}^n  \left(\sum_{j=0}^n \plagrange_j(x_i)\right)^2
    = \sum_{i=0}^n \alpha_j^2.
  \end{gather}
  Offensichtlich ist diese Summe genau dann null, wenn dies für jedes $\alpha_j$ gilt, also $p=0$.
\end{proof}

\begin{Korollar}{lagrange-interpolation}
  Die Lösung der Interpolationsaufgabe nach Lagrange erlauben die Darstellung
  \begin{gather}
    p(x;f;x_0,\dots,x_n) = \sum_{i=0}^n f_i \plagrange_{i;x_0,\dots,x_n}(x).
  \end{gather}
  
  Die Lagrange-Interpolation eingeschränkt auf den Raum $\P_n$ ist die
  Identität
\end{Korollar}

\begin{intro}
  Die Lagrangesche Interpolationsaufgabe kann auch als Gaußsche
  Ausgleichsrechnung mit dem obigen Skalarprodukt aufgefasst
  werden. Allerdings erreichen wir es hier, dass der Abstand in der
  zugehörigen Norm zu null wird.

  Ein alternativer Beweis zur Eindeutigkeit der Lösung der
  Interpolationsaufgsbe benutzt nicht die explizite Darstellung in der
  Lagrange-Basis, sondern folgendes Resultat über die Nullstellen
  eines Polynoms.
\end{intro}

\begin{Satz}{polynom-nullstellen}
  Hat ein Polynom $p\in \P_n$ auf der reellen Achse $n+1$
  verschiedene Nullstellen, so ist es notwendig das Nullpolynom.
\end{Satz}

\begin{proof}
  Seien die Nullstellen $x_0 < x_1 < \dots < x_n$ sortiert nach
  Größe. Dann gilt nach dem Satz von Rolle, dass in jedem Intervall
  $(x_i, x_{i+1})$ mit $i=0,\dots, n-1$ eine Nullstelle der Ableitung
  $p'\in \P_{n-1}$ liegt, insgesamt $n$ Stück.

  Diesen Prozess setzen wir rekursiv fort und schließen, dass die $n$-te
  Ableitung $p^{(n)} \in \P_0$ eine Nullstelle hat. Damit ist muss
  $p^{(n)}\equiv 0$ gelten und daher ist $p^{(n-1)} \in \P_0$. Somit
  schließen wir dass $p^{(n-2)} = p^{(n-3)} = \dots = p \equiv 0$
\end{proof}


\begin{Lemma}{linear-bounded}
  Sei $f\colon X \to Y$ eine lineare Abbildung zwischen Vektorräumen
  $X$ und $Y$. Dann sind folgende Aussagen äquivalent:
  \begin{enumerate}
  \item In einem beliebigen Punkt $x\in X$ gilt für das gestörte Problem
    $y+\delta y = f(x+\delta x)$ die Abschätzung
    \begin{gather}
      \norm{\delta y} \le \kappa^{\text{abs}} \norm{\delta x}
      \qquad\forall \delta x \in X.
    \end{gather}
  \item Für $y = f(x)$ gilt die Abschätzung
    \begin{gather}
      \norm{y} \le \kappa^{\text{abs}} \norm{x}
      \qquad\forall x \in X.
    \end{gather}
  \end{enumerate}
  Insbesondere ist eine lineare Abbildung genau dann stetig, wenn es
  eine solche Schranke gibt.
\end{Lemma}
\begin{proof}
  Der Linearität und $y=f(x)$ wegen gilt
  \begin{gather}
    y + \delta y = f(x+\delta x) = f(x) + f(\delta x),
  \end{gather}
  beziehungsweise
  \begin{gather}
    \delta y = f(x) - y + f(\delta x) = f(\delta x).
  \end{gather}
  Gilt nun die zweite Aussage, so folgt sofort die erste. Betrachte
  ich die erste im Punkt $x=0$, so dass wegen der Linearität
  $y=f(x) = 0$ gilt, so ergibt sich aus der Bedingung \glqq für alle
  $\delta x$\grqq{} die zweite.
\end{proof}

\begin{remark}
  Es genügt also, die Konditionierung um die null zu untersuchen, was
  die Analyse vereinfacht.

  Nun gilt für eine lineare Abbildung $f(0) = 0$. In diesem Falle ist
  also die Konditionszahl für den relativen Fehler aus
  \slideref{Definition}{datenfehler}
  bzw. \slideref{Lemma}{diff-fehler} nicht sinnvoll definiert. Wir
  benutzen daher die Konditionszahlen für den absoluten Fehler. 
\end{remark}

\begin{Satz*}{lagrange-kondition}{Konditionszahl der Lagrange-Interpolation}
  Die Konditionszahl des absoluten Fehlers in der Supremumsorm der
  Lagrange-Interpolation zu den Punkten $a = x_0 < \dots < x_n = b$
  ist die \define{Lebesgue-Konstante}
  \begin{gather}
    \Lambda_{x_0,\dots,x_n} = \max_{x\in [a,b]}
    \sum_{i=0}^n \abs{\plagrange_{i;x_0,\dots,x_n}(x)}.
  \end{gather}
  Es gilt also
  \begin{gather}
    \max _{x\in[a,b]} \abs{I_n f(x)}
    \le \Lambda_{x_0,\dots,x_n} \max _{x\in[a,b]} \abs{f(x)}.
  \end{gather}
  Diese Abschätzung ist scharf.
\end{Satz*}

\begin{proof}
  Siehe auch \cite[Satz 7.3]{DeuflhardHohmann08}.
  
  Zunächst zeigen wir die Beschränktheit der Interpolation. Es gilt:
  \begin{align}
    \abs{I_n(f)(x)}
    &= \left\vert\sum_{i=0}^n f(x_i) \plagrange_{i;n}(x)\right\vert \\
    &\le\sum_{i=0}^n \left\vert f(x_i) \plagrange_{i;n}(x)\right\vert \\
    &\le \max_{i=0,\dots,n} \abs{f_i}
      \;\sum_{i=0}^n \left\vert \plagrange_{i;n}(x)\right\vert \\
    & \le \max_{x\in [a,b]} \abs{f(x)}
       \;\max_{x\in [a,b]} \sum_{i=0}^n \left\vert \plagrange_{i;n}(x)\right\vert.
  \end{align}
  Damit gilt also die behauptete Abschätzung
  \begin{gather}
    \norm{I_n(f)}_\infty
    \le \Lambda_{x_0,\dots,x_n} \norm{f}_\infty,
  \end{gather}
  wobei die Supremumsnorm auf dem Intervall $[a,b]$ genommen wurde.

  Wir zeigen, dass die Abschätzung scharf ist, indem wir für eine
  Funktion die Gleichheit zeigen, also: es existiert eine auf $[a,b]$
  stetige Funktion $g$, so dass es einen Wert $x\in [a,b]$ gibt an dem gilt:
  \begin{gather}
    \abs{I(g)(x)}
    = \norm{g}_\infty \max_{x\in [a,b]} \sum_{i=0}^n \left\vert \plagrange_{i;n}(x)\right\vert.
  \end{gather}
  Sei zunächst $\xi$ der Wert, an dem die Summe ihr Maximum annimt, also
  \begin{gather}
    \sum_{i=0}^n\left\vert \plagrange_{i;n}(\xi)\right\vert
    = \max_{x\in [a,b]} \sum_{i=0}^n\left\vert \plagrange_{i;n}(x)\right\vert.
  \end{gather}
  Nun wählen wir $g$ als die stückweise lineare Funktion, die in den
  Werten $x_i$ die Werte
  $g(x_i) = \operatorname{sgn} \plagrange_i(\xi)$ annimmt und links und rechts des Intervalls, das von den Punkten $\{x_i\}$ aufgespannt wird, konstant ist. Es gilt damit $\norm{g}_\infty = 1$ und
  \begin{align}
    I_n(g)(\xi)
    &= \sum_{i=0}^n  \operatorname{sgn} \plagrange_i(\xi) \plagrange_i(\xi) \\
    &= \sum_{i=0}^n  \abs{\plagrange_i(\xi)} \\
    &= \Lambda_{x_0,\dots,x_n} \norm{g}_\infty
  \end{align}
\end{proof}

\begin{Beispiel}{lagrange-kondition-equi}
  Für äquidistante Stützstellen erhält man exemplarisch die Konditionszahlen in der zweiten Spalte. Später entwickeln wir einen optimalen Satz von Stützstellen. Die Konditionszahlen dazu sind in der rechten Spalte.
  \begin{center}
    \begin{tabular}{r|rr}
      & \multicolumn{2}{c}{ $\Lambda_{0,\dots,n}$}\\
      $n$ & äquidistant & optimal\\\hline
      5 & 3.1 & 2.1\\
      10 & 30 & 2.5 \\
      15 & 512 & 2.7 \\
      20 & 10986 & 2.9
    \end{tabular}
  \end{center}
  Quelle: \cite{DeuflhardHohmann08}
\end{Beispiel}

\subsection{Rekursive Interpolation}

\begin{Lemma*}{Aitken}{Aitken}
  Für das Interpolationspolynom
  \begin{gather}
    p_{0,\dots,n}(x) = p(x;f;x_0,\dots,x_n)
  \end{gather}
  zu paarweise verschiedenen Stützstellen $x_0,\dots,x_n$ gilt die
  Rekursionsformel
  \begin{gather}
    p_{0,\dots,n}(x)
    = \frac{(x-x_0) p_{1,\dots,n}(x) - (x-x_n) p_{0,\dots,n-1}(x)}{x_n-x_0}.
  \end{gather}
\end{Lemma*}

\begin{proof}
  Der Beweis benutzt wieder Induktion. Für eine einzige Stützstelle ist das Interpolationspolynom konstant, $p_i(x) = f_i$ und daher $p_i\in P_0$.
  Sei nun $\phi(x)$ der Bruch auf der rechten Seite. Durch Induktion sehen wir sofort, dass $\phi\in \P_n$. Ferner gilt für $i=1,\dots,n-1$
  \begin{gather}
    \begin{split}
      \phi(x_i)
      &= \frac{(x_i-x_0) p_{1,\dots,n}(x_i) - (x_i-x_n)p_{0,\dots,n-1}(x_i)}{x_n-x_0}\\
      &= \frac{(x_i-x_0) f_i - (x_i-x_n) f_i}{x_n-x_0}\\
      &= f_i.
    \end{split}
  \end{gather}
  Ebenso verschwindet für $x_0$ und $x_n$ je ein Term und es gilt
  dieselbe Aussage.
\end{proof}

\begin{Algorithmus*}{Neville}{Neville}
  Sei für eine Stelle $x$ an der das Interpolationspolynom berechnet
  werden soll $p_{ik} = p_{i-k,\dots,i}(x)$ für $i\ge k$. Dann lässt
  sich $p_{0,\dots,n}(x) = p_{nn}$ rekursiv berechnen durch
  \begin{enumerate}
  \item Für $k=0$ setze
    \begin{gather}
      p_{i0} = f_i \qquad i=0,\dots,n.
    \end{gather}
  \item Für $k=1,\dots,n$ berechne
    \begin{gather}
      p_{ik} = p_{i,k-1} + \frac{x-x_i}{x_i-x_{i-k}}
      \bigl( p_{i,k-1} - p_{i-1,k-1} \bigr)
      \qquad i=k,\dots,n.
    \end{gather}
  \end{enumerate}
\end{Algorithmus*}

\begin{Definition}{newton-basis}
  Als \define{Newton-Basis} der Lagrange-Interpolation bezeichnen wir
  die Polynome
  \begin{gather}
    \omega_k(x)
    = \omega_{0,\dots,k-1}(x)
    = \prod_{j=0}^{k-1} (x-x_j),
    \qquad k=0,\dots,n
  \end{gather}
  wobei das leere Produkt für $k=0$ den Wert 1 annehme. Ebenso können
  wir Newton-Polynome mit Startindex verschieden von null definieren
  und erhalten für $k\ge 0$
  \begin{gather}
    \omega_{i,\dots,i+k} = \prod_{j=i}^{i+k} (x-x_j)
    = \frac{\omega_{i+k+1}(x)}{\omega_{i}(x)}.
  \end{gather}
\end{Definition}

\begin{Lemma}{newton-basis}
  Sei $p \in \P_n$ ein Polynom dargestellt bezüglich der Monombasis
  und der Newton-Basis durch
  \begin{gather}
    p(x) = \sum_{i=0}^n a_i x^i = \sum_{i=0}^n b_i \omega_i(x),
  \end{gather}
  Dann gilt $b_n = a_n$.
\end{Lemma}

\begin{proof}
  Es gilt
  \begin{gather}
    p(x) = q_n(x) = q_{n-1}(x) + b_n \omega_n(x).
  \end{gather}
  Da $q_{n-1}\in \P_{n-1}$ und der führende Koeffizient von $\omega_n$ eins ist, folgt sofort $b_n = a_n$.
\end{proof}

\begin{Definition}{div-diff-1}
  Als \define{dividierte Differenzen} zur
  Lagrange-Interpolationsaufgabe bezeichnen wir die rekursiv
  definierten Werte
  \begin{align}
    [x_i]f
    &= f_i \\
    [x_i,\dots,x_{i+k}]f
    &= \frac{[x_{i+1},\dots,x_{i+k}]f - [x_i,\dots,x_{i+k-1}]f}{x_{i+k}-x_i}
  \end{align}
\end{Definition}

\begin{Satz}{newton-lagrange}
  Für das Lagrange-Interpolationspolynom $p_{i,\dots,i+k}(x)$ zu den
  paarweise verschiedenen Stützpunkten $x_i,\dots,x_{i+k}$ gilt
  \begin{gather}
    p_{i,\dots,i+k}(x)
    = \sum_{j=i}^{i+k} [x_i,\dots,x_{j}]f\; \frac{\omega_j(x)}{\omega_i(x)}.
  \end{gather}
\end{Satz}

\begin{proof}
  In der Newton-Darstellung gilt
  \begin{gather}
    p_{i,\dots,i+k}(x) = p_{i,\dots,i+k-1}(x)
    + \alpha \frac{\omega_{i+k}(x)}{\omega_i(x)}.
  \end{gather}
  Zu zeigen ist also $\alpha = [x_i,\dots,x_{i+k}]f$, was nach
  \slideref{Lemma}{newton-basis} der Koeffizient vor $x^k$ ist. Nach
  Induktionsannahme ist
  \begin{gather}
    \begin{split}
      p_{i,\dots,i+k-1}(x) &= [x_i,\dots,x_{i+k-1}]f \,x^{k-1} +\bigo(x^{k-2})\\
      p_{i+1,\dots,i+k}(x) &= [x_{i+1},\dots,x_{i+k}]f \,x^{k-1} +\bigo(x^{k-2})
    \end{split}
  \end{gather}
  Nach dem Lemma von Aitken gilt
  \begin{gather}
    p_{i,\dots,i+k} = \frac{(x-x_i)p_{i+1,\dots,i+k}
      - (x-x_{i+k})p_{i,\dots,i+k-1}}{x_{i+k} - x_i}.
  \end{gather}
  Dessen höchster Koeffizient ist aber gerade die dividierte Differenz.
\end{proof}

\begin{remark}
  Der Bruch im vorherigen Satz ist nicht problematisch, da
  \begin{gather}
    \frac{\omega_j(x)}{\omega_i(x)} = \prod_{\ell=i}^{j-1} (x-x_\ell).
  \end{gather}
\end{remark}

\begin{Satz}{Lagrange-restglied}
  Sei $f \in C^{n+1}[a,b]$ und $p\in \P_n$ die
  Lagrange-Interpolierende zu den Stützstellen
  $x_0,\dots,x_n\in [a,b]$. Dann gibt es zu jedem $x\in [a,b]$ einen Punkt
  $\xi \in (a,b)$, so dass
  \begin{gather}
    f(x)- p(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \omega_{0,\dots,n}(x).
  \end{gather}
  Wir bezeichnen diese Aussage als \define{Fehlerdarstellung}, die
  rechte Seite auch als \define{Restglied}.
\end{Satz}

\begin{proof}
  Der Beweis folgt \cite[Satz 2.1.4.1]{Stoer83}.  Zunächst bemerken
  wir, dass für alle Stützstellen $x_i$ gilt, dass
  $f(x_i) - p(x_i) = 0$. Dort ist also nichts zu beweisen.
  Für die Fehlerfunktion $F(y)$ machen wir nun den Ansatz
  \begin{gather}
    \label{eq:interpolation:1}
    F(y) = f(y)-p(y) - \alpha \omega_{0,\dots,n}(y)
  \end{gather}
  und $\alpha$ soll so gewählt werden, dass $F(x) = 0$. Damit hat $F(y)$ im
  Intervall $I$ insgesamt die $n+2$ Nullstellen $x,x_0,\dots,x_n$.
  Wiederholte Anwendung des Satzes von Rolle ergibt, dass $F'(y)$
  insgesamt $n+1$ Nullstellen hat und das $F^{(n+1)}(y)$ eine
  Nullstelle $\xi$ besitzt. Da $p\in \P_n$ gilt
  \begin{gather}
    0 = F^{(n+1)}(\xi) = f^{(n+1)}(\xi) - \alpha (n+1)!
  \end{gather}
  und damit
  \begin{gather}
    \alpha = \frac{f^{(n+1)}(\xi)}{(n+1)!}.
  \end{gather}
\end{proof}

\begin{Korollar}{Lagrange-restglied}
  Sei $f \in C^{n+1}[a,b]$ und alle Stützstellen $x_i$ im Intervall
  $[a,b]$. Dann gibt es $\xi$ aus dem kleinsten Intervall, das die Punkte $a$, $b$ und $x$ enthält, so dass
  \begin{gather}
    [x_0,\dots,x_n]f = \frac{f^{(n)}}{n!}(\xi).
  \end{gather}
\end{Korollar}

\begin{proof}
  Wenden wir Formel~\eqref{eq:interpolation:1} auf $p_{0,\dots,n-1}$
und $x=x_n$ an, so sehen wir, dass $\alpha$ der Koeffizient vor dem
nächsten Newton-Basispolynom $\omega_{0,\dots,n-1}$ ist. Dieser ist
aber gerade die angegebene dividierte Differenz.
\end{proof}

\begin{Korollar}{Lagrange-fehler-1}
  Es gelten die Voraussetzungen von
  \slideref{Satz}{Lagrange-restglied}. Dann gibt es eine Konstante
  $C$, die nur von der Wahl der Stützstellen abhängt, so dass
  \begin{gather}
    \max_{x\in[a,b]} \abs{f(x)-p_{0,\dots,n}(x)}
    \le \frac{C \abs{b-a}^{n+1}}{(n+1)!} \max_{x\in[a,b]} \abs{f^{(n+1)}(x)} 
  \end{gather}
\end{Korollar}

\begin{remark}
  Die Fehlerabschätzung in \slideref{Korollar}{Lagrange-fehler-1}
  können wir auch kürzer schreiben als
  \begin{gather}
    \norm{f-p_{0,\dots,n}}_\infty \le \frac{C \abs{b-a}^{n+1}}{(n+1)!}
    \norm{f^{(n+1)}}_{\infty}.
  \end{gather}
  Die rechte Seite besteht dabei aus dem Produkt aus einem Teil, der
  nur von den Daten abhängt, $\norm{f^{(n+1)}}_{\infty}$ und einem
  Anteil, der durch das Verfahren bestimmt ist.

  Wir sehen, dass Interpolation auf einem Intervall um so genauer ist,
  je kürzer das Intervall ist.
\end{remark}

\begin{intro}
  Der Rest dieses Abschnitts befasst sich mit der Frage, wie die
  Stützstellen $x_0,\dots,x_n$ gewählt werden können, damit die
  Konstante $C$ in der Fehlerabschätzung optimal ist.  Aus der
  Fehlerdarstellung in \slideref{Satz}{Lagrange-restglied} folgt, dass
  wir dazu ein Polynom finden müssen, dessen führender Koeffizient 1
  ist, und das minimalen Betrag auf dem Intervall $[a,b]$ hat.
  Tatsächlich erlauben uns die Tschebyscheff-Polynome, diese
  Optimalität zu erreichen.

  Insbesondere erlaubt uns die Fehlerdarstellung, die Minimierung über
  die Stützpunkte durch eine Minimierung im Polynomraum zu
  ersetzen. Letztere hat Vektorraumstruktur und ist damit deutlich
  strukturierter.
\end{intro}

\begin{Lemma}{chebyshev-properties}
  Die Tschebyscheff-Polynome, die der Rekursionsformel in
  \slideref{Definition}{chebyshev-polynome} genügen, haben die
  Darstellung
  \begin{gather}
    T_k = \cos(k \operatorname{arccos} x)
  \end{gather}
  Insbesondere gilt
  \begin{alignat}3
    T_k(1)&=1\\
    T_k(-1) &= (-1)^k\\
    \abs{T_k(x)} &\le 1, &\quad x&\in [-1,1]\\
    T_k(x) &=(-1)^j,
                   &\quad x&=\cos\left(\frac{j}{k}\pi\right),
                   &\quad j&=0,\dots,k\\
    T_k(x) &= 0,
             &\quad x&=\cos\left(\frac{2j-1}{2k}\pi\right),
             &\quad j&=1,\dots,k
  \end{alignat}
\end{Lemma}

\begin{proof}
  Hausaufgabe
\end{proof}

\begin{Satz}{chebyshev-minimal-1}
  Jedes Polynom $p\in \P_n$ mit führendem Koeffizienten 1 nimmt im
  Intervall $[-1,1]$ einen Wert $\abs{p(x)} \ge \nicefrac1{2^{n-1}}$
  an und es gilt
  \begin{gather}
   \frac1{2^{n-1}} T_n(x)
   = \operatorname*{arg min}_{\substack{p\in\P_n\\p = x^n+\cdots}}
   \max_{x\in[-1,1]}\abs{p(x)}.
  \end{gather}
\end{Satz}

\begin{proof}
  Siehe auch \cite[Satz 7.19]{DeuflhardHohmann08}. Aus der
  Rekursionsformel für Tschebyscheff-Polynome folgt sofort,
  dass der höchste Koeffizient von
  $T_n$ den Wert $2^{n-1}$ annimmt. Sei nun als Widerspruchsannahme
  $p\in \P_n$ ein weiteres Polynom mit höchstem Koeffizienten
  $2^{n-1}$, so dass
  \begin{gather}
    \max_{x\in[-1,1]} \abs{p(x)} < 1.
  \end{gather}
  Dann ist $q_n = T_n-p \in \P_{n-1}$ und für die
  \define{Tschebyscheff-Abszisse}n
  $\tilde x_j = \cos(\nicefrac{j\pi}{n})$ mit $j=0,\dots,n$ gilt
  \begin{xalignat}4
    T_n(\tilde x_j) &= 1,
    & p(\tilde x_j) &< 1
    & q_n(\tilde x_j) &> 0,
    & j&\text{ gerade}\\
    T_n(\tilde x_j) &= -1,
    & p(\tilde x_j) &> -1
    & q_n(\tilde x_j) &< 0,
    & j&\text{ ungerade}.
  \end{xalignat}
  $q_n$ wechselt also an mindestens $n$ Stellen das Vorzeichen und hat
  damit als stetige Funktion mindestens ebensoviele Nullstellen. Aus
  $q_n\in\P_{n-1}$ folgt damit im Widerspruch $q_n=0$ und $p=T_n$.
  Damit gilt nach Skalierung um den Faktor $2^{n-1}$
  \begin{gather}
    \operatorname*{min}_{\substack{p\in\P_n\\p = x^n+\cdots}}
   \max_{x\in[-1,1]}\abs{p(x)} \ge 1,
  \end{gather}
  und Gleichheit für das skalierte Tschebyscheff-Polynom.
\end{proof}

\begin{Korollar}{Lagrange-chebychev}
  Wählt man als Stützstellen die Werte
  \begin{gather}
    x_i = \frac{a+b}2 + \frac{b-a}2 \cos\left(\frac{2i+1}{2n+2}\pi\right),
    \qquad i=0,\dots,n,
  \end{gather}
  So gilt für den Fehler der Lagrange-Interpolation
  \begin{gather}
    \norm{f-p_{0,\dots,n}}_\infty \le \frac{\abs{b-a}^{n+1}}{2^{2n+1}(n+1)!}
    \norm{f^{(n+1)}}_{\infty}.
  \end{gather}
\end{Korollar}

\begin{proof}
  Zunächst transformieren wir die Aufgabe vom Intervall $[a,b]$ auf
  das Intervall $[-1,1]$ durch die Abbildung
  \begin{gather}
    x = \Phi(\xi) = \frac{a+b}2 + \frac{b-a}2 \xi.
  \end{gather}
  Es gilt $\Phi(-1) = a$, $\Phi(1)=b$ und die Punkte $x_i$ sind die
  Bilder der Tschebyscheff-Knoten $\xi_i$ zu $T_{n+1}$. Insbesondere
  ist nun für diese Knoten
  \begin{gather}
    \omega_{0,\dots,n}(\xi) = \frac1{2^{n}} T_{n+1}(\xi).
  \end{gather}

  Es gilt
  $\Phi'(\xi) = (b-a)/2$ und für die Funktion $F(\xi) = f(\Phi(\xi))$
  gilt
  \begin{gather}
    \frac{d^k}{d\xi^k} F(\xi) = \left(\frac{b-a}2\right)^k
    \frac{d^k}{dx^k} f(x).
  \end{gather}
  Auf $[-1,1]$ folgern wir aus \slideref{Satz}{Lagrange-restglied} und
  \slideref{Satz}{chebyshev-minimal-1}, dass für die Interpolation gilt
  \begin{gather}
    \max_{\xi\in[-1,1]} \abs{F(\xi) - P(\xi)}
    \le \frac{2^{-n}}{(n+1)!} \max_{\xi\in[-1,1]}\abs{F^{(n+1)}(\xi)},
  \end{gather}
  woraus folgt
  \begin{gather}
    \max_{x\in[a,b]} \abs{f(x) - p(x)}
    \le \frac{2^{-n}}{(n+1)!} \left(\frac{b-a}2\right)^{n+1} \max_{x\in[a,b]}\abs{f^{(n+1)}(x)}.
  \end{gather}  
\end{proof}

\begin{Bemerkung}{extrapolation}
  Wird das Interpolationspolynom an einem Punkt $x$ ausgewertet, der
  nicht zwischen den Interpolationspunkten liegt, so sprechen wir von
  Extrapolation. Aus der Fehlerdarstellung mit den Newton-Polynomen
  können wir ablesen, dass die Abschätzungen sehr schnell schlechter
  werden, wenn sich $x$ von den Stützstellen entfernt.
\end{Bemerkung}


\subsection{Hermite-Interpolation}

\begin{Definition}{hermite-interpolation}
  Die \define{Hermite-Interpolation} benutzt neben Funktionswerten
  auch Ableitungswerte zur Interpolation. Das Interpolationspolynom
  $p\in \P_n$ genügt in $m+1$ paarweise verschiedenen Punkten den
  Bedingungen
  \begin{gather}
    \frac{d^j p}{dx^j}(x_i) = f_i^{j},
    \qquad i = 0,\dots, m, \quad j=0,\dots,n_i-1,
  \end{gather}
  und es gilt
  \begin{gather}
    \sum_{i} n_i = n+1.
  \end{gather}
  Die definierenden Funktionale\footnote{Als Funktional bezeichnet man
    eine Abbildung aus einem Vektorraum in den zugehörigen Körper} der Gestalt
  $\nicefrac{d^j}{dx^j} p(x_i)$ werden auch als \define{Knotenwerte}
  oder \define{Knotenfunktionale} bezeichnet.
\end{Definition}

\begin{Satz}{hermite-interpolation}
  \slideref{Definition}{hermite-interpolation} bestimmt das
  Interpolationspolynom eindeutig.
\end{Satz}

\begin{proof}
  Analog zur Lagrange-Interpolation identifizieren wir wieder eine
  Basis $\{H_{ij}(x)\}$, diesmal doppelt indiziert, die bezüglich der
  Interpolationsbedingungen orthogonal ist. Damit stellen wir das
  Interpolationspolynom dar als
  \begin{gather}
    \label{eq:interpolation:hermite-1}
    p(x) = \sum_{i=0}^m \sum_{j=0}^{n_i-1} f_i^j H_{ij}(x).
  \end{gather}
  Zunächst führen wir die Hilfspolynome
  \begin{gather}
    q_{ij}(x) = \frac{(x-x_i)^j}{j!}\prod_{k\neq i}
    \left(\frac{x-x_k}{x_i-x_k}\right)^{n_k}
  \end{gather}
  ein. Es gilt, dass $q_{ij}$ in jedem Punkt $x_k \neq x_i$ eine $n_k$-fache Nullstelle hat, also
  \begin{gather}
    \begin{aligned}
      \frac{d^m q_{ij}}{d x^m} (x_k) &=0,
      \quad &k\neq i,&\quad& m&=0,\dots,n_{k}-1.\\
      \intertext{Ferner hat $q_{ij}$ in $x_i$ eine $j$-fache Nullstelle, also}
      \frac{d^m q_{ij}}{d x^m} (x_i) &=0,
      \qquad &&& m&=0,\dots,j-1.\\
      \intertext{Schließlich gilt}
      \frac{d^j q_{ij}}{d x^j} (x_i) &=1.
    \end{aligned}
  \end{gather}
  Damit können wir im Punkt $x_i$ rekursiv definieren
  \begin{gather}
    \begin{aligned}
      H_{i,n_i-1}(x) &= q_{i,n_i-1}(x)\\
      H_{ij}(x) &= q_{ij}(x) - \sum_{m=j+1}^{n_i-1} q_{ij}^{(m)}(x_i) H_{im}(x),
    \end{aligned}
  \end{gather}
  wobei die letzte Zeile die Anwendung des Gram-Schmidt-Verfahrens
  ist. Per constructionem gilt für diese Polynome
  \begin{gather}
    \frac{d^\ell}{dx^\ell} H_{ij}(x_k) = \delta_{ik}\delta_{j\ell},
  \end{gather}
  was die lineare Unabhängigkeit impliziert. Da ihre Anzahl gleich der
  Raumdimension ist, bilden sie eine Basis. Die Aussage des Satzes
  ergibt sich nun aus der
  Basisdarstellung~\eqref{eq:interpolation:hermite-1}. Insbesondere
  impliziert $\equiv 0$ für alle Knotenwerte $f_i^j=0$.
\end{proof}

\begin{Notation}{interpolation-ascending}
  Bei der Polynominterpolation ist die Anordnung der
  Interpolationspunkte beliebig. Das ist auch weiterhin der Fall. Für
  die Darstellung der Resultate und Beweise ist es aber oft hilfreich
  anzunehmen, dass sie in aufsteigender Folge angeordnet sind. Wir
  nehmen daher ab jetzt an, dass
  \begin{gather}
    a = x_0 \le x_1 \le \dots \le x_n = b.
  \end{gather}
  Dabei sollen $k$-fach wiederholte Stützstellen bedeuten, dass dort
  nicht nur der der Funktionswert, sondern auch die ersten $k-1$
  Ableitungen interpoliert werden. Damit haben wir für die
  Interpolation in $\P_n$ immer eine Folge von $n+1$ Stützstellen.
\end{Notation}

\begin{Beispiel}{taylor-polynom}
  Sind alle Stützstellen $x_0 = \dots = x_n$ identisch, so erhalten
  wir duch Interpolation einer Funktion $f\in C^n[a,b]$ das
  Taylor-Polynom vom Grad $n$
  \begin{gather}
    p(x;f;x_0,\dots,x_n) = \sum_{k=0}^n \frac{(x-x_0)^k}{k!} f^{(k)}(x_0).
  \end{gather}
\end{Beispiel}

\begin{Beispiel}{hermite-kubisch}
  Die kubische Hermite-Interpolation auf dem Intervall $[a,b]$ ist
  definiert durch die Knotenwerte
  \begin{gather}
    p(a), p'(a), p(b), p'(b).
  \end{gather}
\end{Beispiel}

\begin{Satz}{Hermite-interpolation}
  Das Hermite-Interpolationspolynom genügt der Darstellung
  \begin{gather}
    p_{0,\dots,n}(x) = \sum_{j=0}^n [x_0,\dots,x_j]f\;\omega_j(x)
  \end{gather}
  mit den verallgemeinerten dividierten Differenzen definiert durch
  die Rekursion
  \begin{gather}
    [x_i,\dots,x_{i+k}]f =
    \begin{cases}
      \frac{f^{(k)}(x_i)}{k!} &x_i=x_{i+k}\\
      \frac{[x_{i+1},\dots,x_{i+k}]f - [x_i,\dots,x_{i+k-1}]f}{x_{i+k}-x_i}
      &x_i\neq x_{i+k}.
    \end{cases}
  \end{gather}
  Hier ist das Newton-Polynom mit wiederholten Stützstellen so
  definiert, dass die zugehörigen Linearfaktoren wiederholt werden.
\end{Satz}

\begin{proof}
  Der Beweis folgt im wesentlichen dem analogen
  \slideref{Satz}{newton-lagrange}. Wir müssen dort nur die Argumente
  anpassen, die auf paarweise verschiedenen Stützstellen beruhen.

  Dazu betrachten wir die Lagrange-Interpolation mit
  $x_i < x_{i+1}< \dots < x_{i+k}$ und zugehörigem
  Interpolationspolynom $p_{i,\dots,i+k} \in \P_k$. Wir benutzen
\slideref{Korollar}{Lagrange-restglied}, wonach gilt: es gibt ein
$\xi\in[x_{i+1},x_{i+k}]$ mit
  \begin{gather}
    [x_i,\dots,x_{i+k}]f = \frac{f^{(k)}(\xi)}{k!}.
  \end{gather}
  Da diese Eigenschaft unabhängig vom Abstand der Stützstellen gilt,
  können wir den Limes $x_j \to x_i$ für $j=1,\dots,k$ bilden, und es
  gilt im Limes $x_i=\dots=x_{i+k}$
  \begin{gather}
    [x_i,\dots,x_{i+k}]f \to \frac{f^{(k)}(x_i)}{k!},
  \end{gather}
  sowie
  \begin{gather}
    \omega_{i,\dots,i+k-1}(x) \to (x-x_i)^{k}.
  \end{gather}
  Für das zugehörige Interpolationspolynom gilt dann
  \begin{gather}
    \frac{d^k}{dx^k} p_{i,\dots,i+k}(x_i) = f^{k}(x_i).
  \end{gather}
  Ist $x_i$ ein mehrfacher Interpolationspunkt, so gilt dies Argument für alle $x=1,\dots,n_i-1$ (Achtung, wir wechseln hier in der Diskussion die Numerierung zwischen \slideref{Definition}{hermite-interpolation} und \slideref{Notation}{interpolation-ascending}).
  
  Damit haben wir im Neville-Schema den Induktionsanfang geschafft. Es
  bleibt zu zeigen, dass die Rekursionsformel von Aitken auch
  weiterhin für $x_i \neq x_{i+k}$ gilt. Das ist unmittelbar
  einsichtig, wenn $x_i \neq x_{i+1}$ und $x_{i+k-1} \neq x_{i+k}$, da
  dann beide Polynome in der Rekursion alle Zwischenpunkte $x_j$
  interpolieren.

  Sei nun zunächst
  $x_i=x_{i+1}= x_{i+r} < x_{r+1} \le \dots < x_{i+k}$. Es ist zu
  zeigen, dass das Polynom
  \begin{gather}
    q(x) = \frac{(x-x_i) p_{i+1,\dots,i+k}(x)
      - (x-x_k) p_{i,\dots,i+k-1}(x)}{x_{i+k}-x_i}
  \end{gather}
  alle Knotenfunktionale interpoliert. Für $x_j\neq x_i$ folgt dies
  wie bei der Lagrange-Interpolation aus der Induktionsannahme. Doch
  auch für $x_i$ gilt dies, da der erste Term in der Summe
  verschwindet und $p_{i,\dots,i+k-1}$ bereits alle geforderten
  Ableitungen interpoliert.
\end{proof}

\begin{Satz}{Hermite-restglied}
    Sei $f \in C^{n+1}[a,b]$ und $p\in \P_n$ die
  Hermite-Interpolierende zu den Stützstellen
  $a=x_0\le\dots\le x_n=b$. Dann gibt es zu jedem $x\in \R$ einen Punkt
  $\xi$ im kleinsten Intervall $I$, das die Punkte $x$, $a$ und $b$
  enthält, so dass
  \begin{gather}
    f(x)- p(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \omega_{0,\dots,n}(x).
  \end{gather}
\end{Satz}

\begin{proof}
  Der Beweis folgt exakt denselben Argumenten wie der von
  \slideref{Satz}{Lagrange-restglied}.
\end{proof}

\begin{Korollar}{Taylor-restglied}
  Für das \define{Taylor-Polynom} zu $f\in C^{n+1}(a,b)$ in einem
  Punkt $x_0\in(a,b)$,
  \begin{gather}
    p(x) = \sum_{i=0}^n \frac{f^{i}(x_0)}{i!} (x-x_0)^i
  \end{gather}
  gilt die folgende Fehlerdarstellung: es gibt ein $\xi\in[x_0,x]$ so dass gilt
  \begin{gather}
    f(x) - p(x) = \frac{f^{n+1}(\xi)}{(n+1)!} (x-x_0)^{n+1}.
  \end{gather}
\end{Korollar}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
