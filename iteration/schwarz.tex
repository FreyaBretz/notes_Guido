
\section{Additive Schwarz methods}

\begin{intro}
  In this section, we study preconditioners, which are related to
  subspace decompositions of the space $V$ or its finite dimensional
  subspaces. We will develop the theory in an abstract way, but always
  keep the model problem~\eqref{eq:itintro:1} in mind when we do so. In
  particular, the subspaces chosen will be associated with either
  coarser mesh levels or with meshes on subdomains of $\Omega$.
  
  This section follows in part~\cite[Chapter 7]{BrennerScott02}. A
  more detailed discussion with extension of the methods developed
  here can be found in~\cite{ToselliWidlund05}
\end{intro}

\subsection{The abstract framework}

\begin{intro}
  Let $V$ be a Hilbert space with inner product $\scal(.,.)$ and let
  $a(.,.): V\times V$ be a symmetric and $V$-elliptic not necessarily
  bounded bilinear form. Let a set of subspaces
  $\{V_j\}_{j=1,\dots,J}$ of $V$ be chosen such that
  \begin{gather*}
    V = \sum_{j=1}^J V_j.
  \end{gather*}
  The sum is not required to be direct, that is, a vector $v\in V$ may
  have several decompositions $v = \sum \alpha_j v_j$ with
  $v_j\in V_j$. Methods below based on the subspaces $V_j$ are called
  \define{subspace correction} methods.

  Alternatively, we consider methods based on abstract auxiliary
  spaces $\hat V_j$, such that
  \begin{gather*}
    V = \sum_{j=1}^J R_j^T \hat V_j,
  \end{gather*}
  where $R_j^T$ is an operator embedding the auxiliary space
  $\hat V_j$ into $V$.  Such methods are called \define{auxiliary
    space methods}. Subspace correction and auxiliary space methods
  are equivalent by considering $V_j = R_j^T \hat V_j$.
\end{intro}

\begin{assumption}
  \label{lemma:schwarz:1}
  There are bilinear forms $a_j(.,.)$ defined on $V_j$ such that the weak formulation
  \begin{gather}
    \label{eq:schwarz:1}
    a_j(u_j,v_j) = f(v_j),
    \quad\forall v_j\in V_j,
  \end{gather}
  has a unique solution $u_j\in V_j$ for all $f\in V_j^*$. In the
  context of auxiliary space methods, we assume that there are
  bilinear forms $\hat a_j(.,.)$ defined on $\hat V_j$ such
  that the weak formulation
  \begin{gather}
    \label{eq:schwarz:1a}
    \hat a_j(u_j,v_j) = f(v_j),
    \quad\forall v_j\in \hat V_j,
  \end{gather}
  has a unique solution $u_j\in \hat V_j$ for all $f\in \hat V_j^*$.
\end{assumption}

\begin{definition}[Subspace projection]
  \label{definition:schwarz:1}
  \index{Pj@$P_j$|see {Ritz projection}} Let the \define{subspace
    projection} operator $P_j: V \to V_j$ be defined such that
  $P_j u \in V_j$ is the unique (Assumption~\ref{lemma:schwarz:1})
  solution to the problem
  \begin{gather}
    \label{eq:schwarz:2}
    a_j(P_j u,v_j) = a(u,v_j),\quad\forall v_j\in V_j.
  \end{gather}
  Note that this is a projection only if $a_j(.,.)$ is the restriction
  of the bilinear form $a(.,.)$ to the subspace $V_j$. In this
  case,\index{A-orthogonal projection@$A$-orthogonal projection|see
    {Ritz projection}} we call $P_j$ the $A$-orthogonal projection or
  \define{Ritz projection} to $V_j$. If $a_j(.,.)$ is not the
  restriction of $a(.,.)$, the operator $P_j$ still maps into $V_j$,
  but it is not idempotent.

  Since $V_j$ is a subspace of $V$, we will also understand $P_j$ as
  an endomorphism of $V$ iteself. The left hand side of this equation
  induces an operator $A_j: V_j \to V_j^*$ by $A_j u_j = a_j(u_j,.)$.
\end{definition}

\begin{definition}[Auxiliary space projection]
  \label{definition:schwarz:1a}
  Let the operator $\hat P_j: V \to \hat V_j$ be defined such that $\hat P_j u \in \hat V_j$ is
  the unique (Assumption~\ref{lemma:schwarz:1}) solution to the problem
  \begin{gather}
    \label{eq:schwarz:2a}
    \hat a_j(\hat P_j u,v_j) = a(u,R_j^T v_j),\quad\forall v_j\in \hat V_j.
  \end{gather}
  The left hand side of this equation induces an operator $\hat A_j:
  \hat V_j \to \hat V_j^*$ by $\hat A_j u_j = \hat a_j(u_j,.)$.
\end{definition}

\begin{lemma}
  \label{lemma:schwarz:ritz}
  If the local bilinear forms $a_j(.,.)$ are the restrictions of
  $a(.,.)$ to $V_j$, then the projections $P_j$ as mappings from $V$
  to itself are self-adjoint with respect to the $a(.,.)$-inner
  product and positive semi-definite. Furthermore, $P_j$ acts as
  identity on $P_j$ and there holds $P_j^2 = P_j$.
\end{lemma}
\begin{proof}
  This is a well-known fact about orthogonal projections, which we
  will prove shortly.
  First, we note that by the uniqueness in
  Lemma~\ref{lemma:schwarz:1} $P_j u_j = u_j$
  for all $u_j\in V_j$. Thus, for all $u\in V$: $P_j P_j u = P_j u$.
  Let now $u,v\in V$ arbitrary. Then, there holds
  \begin{gather*}
    a(u, P_j v) = a(P_j u, P_j v) = a(P_j v, P_j u)
    = a(v, P_j u) = a(P_j u, v).
  \end{gather*}
  Furthermore,
  \begin{gather*}
    a(P_j u,u) = a(u, P_j u) = a(P_j u, P_j u)\ge 0,
  \end{gather*}
  since $a(.,.)$ is positive definite.
\end{proof}

\begin{definition}
  \label{definition:schwarz:1a}
  For subspace and auxiliary space methods, we define the operators
  \begin{gather}
    \begin{split}
      \Pi_j: V &\to V_j\\
      R_j: V &\to\hat V_j
    \end{split}
  \end{gather}
  \index{Pij@$\Pi_j$}
  such that $\Pi_j u\in V_j$ is the solution to the problem
  \begin{gather}
    \scal(\Pi_j u_j,v_j) = \scal(u,v_j),\quad\forall v_j\in V_j.
  \end{gather}

  For auxiliary spaces $\hat V_j$, we define the projection like
  operators  through
  \begin{gather}
    \scal(R_j u_j,v_j)_{\hat V_j} = \scal(u,R_j^T v_j),\quad\forall v_j\in \hat V_j.
  \end{gather}
  
  % \index{Pijt@$\Pi^T_j$}
  % We define its dual $\Pi_j^T: V_j^* \to V^*$ by
  % \begin{gather}
  %   \label{eq:schwarz:3}
  %   \scal(\Pi_j^T \phi_j,v)_{V^*\times V} =  \scal( \phi_j,\Pi_j
  %   v)_{V_j^*\times V_j}
  % \end{gather}
\end{definition}

% \begin{todo}
%   Show that $\Pi^T$ is an orthogonal projection, and onto which space.
% \end{todo}

\begin{lemma}
  \label{lemma:schwarz:2}  
  There holds
  \begin{gather}
    \label{eq:schwarz:15}
    A_j P_j = \Pi^T_j A.
  \end{gather}
\end{lemma}

\begin{proof}
  Let $u,v\in V$ arbitrary. Let $v_j=\Pi_j v$. We rewrite
  equation~\eqref{eq:schwarz:2} as
  \begin{gather*}
    \scal(A_j P_j u, v)_{V^*\times V}
    = \scal(A_j P_j u, v_j)_{V^*\times V}
    = \scal(A u, v_j)_{V^*\times V}
    = \scal(\Pi^T_j A u, v)_{V^*\times V}.
  \end{gather*}
\end{proof}

\begin{definition}
  The additive Schwarz preconditioner for the operator $A$ associated
  with the symmetric, and $V$-elliptic bilinear form $a(.,.)$ with
  respect to the subspace decomposition $V_j$ is the mapping $B:
  V \to V^*$ such that
  \begin{gather}
    \label{eq:schwarz:4}
    B^{-1} = \sum_{j=1}^J P_j A^{-1}.
  \end{gather}
\end{definition}

\begin{example}
  \label{example:schwarz:Jacobi}
  The \putindex{Jacobi method} may serve as a guiding example for the
  definition of these methods. To this end, let $V = \R^n$ with its
  Euclidean inner product $\scal(.,.)$. let $V_j =
  \operatorname{span}\{e_j\}$ be the space spanned by the $j$th unit
  vector. Let $A$ be a symmetric, positive definite matrix and $a(u,v)
  = v^T A u$. Then, equation~\eqref{eq:schwarz:2} becomes
  \begin{gather}
    \label{eq:schwarz:27}
    e_j^T A u_j = e_j^T A u
    \quad \Leftrightarrow \quad
    P_j u = u_j = \frac1{a_{j j}}(A u)_j.
  \end{gather}
  Since for this decomposition, the sum $V=\bigoplus V_j$ is direct,
  we obtain with $D=\operatorname{diag}(a_{11},\dots,a_{n n})$ the
  matrix representation
  \begin{gather*}
    (B^{-1} v)_j = \frac1{a_{j j}}(A A^{-1} v)_j = \frac1{a_{j j}} v_j
    \quad \Leftrightarrow \quad
    B^{-1} = D^{-1}.
  \end{gather*}
  We enter this preconditioner into the Richardson method in operator
  form~\eqref{eq:richardson:11} to obtain the iteration
  \begin{gather}
    \label{eq:schwarz:28}
    \begin{split}
      u^{(k+1)} &= u^{(k)} - \omega_k \sum_{j=1}^J P_j \bigl(u^{(k)} -
      A^{-1}f\bigr)\\
      &= u^{(k)} - \omega_k D^{-1} \bigl(A u^{(k)} - f\bigr).
    \end{split}
  \end{gather}
\end{example}

\begin{lemma}
  \label{lemma:schwarz:3}
  If $A$ is symmetric and positive definite, so is $B^{-1}$ as defined
  in~\eqref{eq:schwarz:4}.
\end{lemma}

\begin{proof}
  By Lemma~\ref{lemma:schwarz:2} and the fact that $P_j$ maps into $V_j$, we have that
  \begin{gather}
    \label{eq:schwarz:16}
    B^{-1} = \sum_{j=1}^J A_j^{-1} \Pi^T_j.
  \end{gather}
  Due to equation~\eqref{eq:schwarz:1}, $A_j$ inherits its symmetry
  and positive definiteness from $A$, and thus $A_j^{-1}$ is s.p.d.
  Therefore, for each term in this sum and arbitrary elements
  $\phi,\psi\in V^*$, we have
  \begin{gather*}
    \scal(A_j^{-1} \Pi^T_j \phi, \psi)_{V\times V^*}
    = \scal(A_j^{-1} \Pi^T_j \phi, \Pi^T_j \psi)_{V\times V^*}
    = \scal(\Pi^T_j \phi, A_j^{-1}\Pi^T_j \psi)
    = \scal(\phi, A_j^{-1}\Pi^T_j \psi).
  \end{gather*}
  The result now follows by linearity.
\end{proof}

\begin{lemma}
  \label{lemma:schwarz:5}
  For $v\in V$ holds
  \begin{gather}
    \label{eq:schwarz:5}
    b(v,v) \equiv \scal(B v,v) = \min_{v=\sum v_j} \sum_{j=1}^J a(v_j,v_j),
  \end{gather}
  where the minimum is taken over all possible decompositions of $v$
  into a sum of elements $v_j\in V_j$ with $j=1,\dots,J$.
\end{lemma}

\begin{proof}
  Since $B^{-1}$ is s.p.d., so is $B$. Therefore, $\scal(.,A_j^{-1}.)$
  is an inner product on $V_j^*$, for which the \putindex{Bunyakovsky-Cauchy-Schwarz
  inequality} holds. Thus, for an arbitrary decomposition $v=\sum v_j$
  with $v_j\in V_j$, the computation
  \begin{align*}
    b(v,v)
    &= \sum_{j=1}^J b(v, A_j^{-1} A_j v_j)
    = \sum_{j=1}^J \scal(\Pi^T_j B v, A_j^{-1} A_j v_j) \\
    &\le \sum_{j=1}^J \sqrt{\scal(\Pi^T_j B v, A_j^{-1}\Pi^T_j B v)}
    \sqrt{\scal(A_j v_j, A_j^{-1}A_j v_j)} \\
    & \le \sqrt{\sum_{j=1}^J \scal(\Pi^T_j B v, A_j^{-1}\Pi^T_j
      B v) }
    \sqrt{\sum_{j=1}^J \scal(A_j v_j,A_j^{-1}A_j v_j)} \\
    &= \sqrt{\scal(B v, {\sum A_j^{-1}\Pi^T_j} B v)}
    \sqrt{\sum_{j=1}^J \scal(A_j v_j, v_j)} \\
    &= \sqrt{b(v,v)} \sqrt{\sum_{j=1}^J a(v_j, v_j)},
  \end{align*}
  yields for arbitrary decompositions
  \begin{gather}
    \label{eq:schwarz:17}
    b(v,v) \le \sum_{j=1}^J a(v_j, v_j),
  \end{gather}
  and thus in particular, that the left hand side is bounded by the
  minimum of the right. Now we choose a special decomposition, showing
  that it cannot be less than the minimum. To this end, let
  \begin{gather}
    \label{eq:schwarz:18}
    v_j = A_j^{-1} \Pi^T_j B v.
  \end{gather}
  By Lemma~\ref{lemma:schwarz:2}, we have
  \begin{gather*}
    \sum v_j = \sum A_j^{-1} \Pi^T_j B v = B^{-1} B v = v.
  \end{gather*}
  Furthermore,
  \begin{align*}
    \sum_{j=1}^J \scal(A_j v_j, v_j)
    &= \sum_{j=1}^J \scal(A_j A_j^{-1} \Pi^T_j B v, A_j^{-1} \Pi^T_j
    B v) \\
    &= \sum_{j=1}^J \scal(\Pi^T_j B v, A_j^{-1} \Pi^T_j v) \\
    &= \scal(B v, \sum A_j^{-1} \Pi^T_j B v) = b(v, v).
  \end{align*}
\end{proof}

\begin{theorem}
  \label{theorem:schwarz:1}
  Let $A$ be s.p.d. and $B$ defined by
  equation~\eqref{eq:schwarz:4}. Then, the \putindex{spectral
    equivalence}~\eqref{eq:richardson:12} holds with
  positive constants
  \begin{gather}
    \label{eq:schwarz:19}
%    \begin{split}
    \Lambda(B,A) = \max_{v\in V} \frac{a(v,v)}{\min\limits_{v=\sum v_j}
      \sum\limits_{j=1}^J a(v_j, v_j)}
    ,\qquad
    \lambda(B,A) = \min\limits_{v\in V} \frac{a(v,v)}{\min\limits_{v=\sum v_j}
      \sum\limits_{j=1}^J a(v_j, v_j)}.
%    \end{split}
  \end{gather}
\end{theorem}

\begin{proof}
  Here we use the fact, that $b(.,.)$ is an inner product on
  $V$ and that by
  \begin{gather*}
    b(B^{-1}A v,v) = a(v,v) = b(v, B^{-1}A v),
  \end{gather*}
  the operator $B^{-1}A$ is symmetric with respect to this inner
  product. Thus, the \putindex{Rayleigh quotient} qualifies to
  estimate the extremal eigenvalues, for instance,
  \begin{gather*}
    \Lambda(B^{-1}A)
    = \max_{v\in V} \frac{b(B^{-1}A v,v)}{b(v,v)}
    = \max_{v\in V} \frac{a(v,v)}{\min\limits_{v=\sum v_j}
      \sum\limits_{j=1}^J a(v_j, v_j)},
  \end{gather*}
  and the same for the minimum.
\end{proof}

\begin{note}
  \label{note:schwarz:1}
  In order to estimate the condition number $\Lambda(B,A)/\lambda(B,A)$ of a Schwarz
  preconditioner, it is now sufficient to bound the two quotients
  in~\eqref{eq:schwarz:19} from above and below. In particular, in
  order to find a bound for $\Lambda(B,A)$, we have to find an estimate of
  the form
  \begin{gather}
    \label{eq:schwarz:23}
    a(v,v) \lesssim \min_{v=\sum v_j}\sum_{j=1}^J a(v_j, v_j),
  \end{gather}
  or in other words, $a(v,v)$ has to be bounded by the sum on the
  right for any decomposition $v=\sum v_j$. On the other hand, in
  order to bound $1/\lambda(B,A)$, we need an estimate in the opposite direction,
  where it is sufficient to find one decomposition $v=\sum v_j$ such
  that it holds. We reduce these
  conditions to the following two abstract assumptions, which guarantee that
  Theorem~\ref{theorem:schwarz:1} holds true.
\end{note}

\begin{assumption}[Stable decomposition]
  \label{assumption:schwarz:stable-decomposition}
  For each $v\in V$ there is a decomposition
  \begin{gather*}
    v = \sum_{j=1^J} v_j, \qquad v_j\in V_j,
  \end{gather*}
  such that there holds
  \begin{gather}
    \label{eq:schwarz:24}
     \min_{v=\sum v_j}\sum_{j=1}^J a(v_j, v_j) \lesssim a(v,v).
  \end{gather}  
\end{assumption}

\begin{assumption}[Strengthened Cauchy-Schwarz inequalities]
  \label{assumption:schwarz:1}
  \defindex{strengthened Cauchy-Schwarz inequalities}
  \index{E@$\mathcal E$} There is a symmetric $J\times J$-matrix
  $\mathcal E$ with entries $\epsilon_{ij} \in [0,1]$ and a constant
  $C$ independent of $J$, such that for the spectral radius
  $\rho(\mathcal E)$ there holds
  \begin{gather*}
    \rho(\mathcal E) \le C
  \end{gather*}
  and for all $1 \le i,j \le J$, $v_i \in V_i$ and $v_j \in V_j$ there
  holds
  \begin{gather}
    \label{eq:schwarz:25}
    \left| a(v_i, v_j)\right|
    \le \epsilon_{ij} \sqrt{a(v_i,v_{\phantom ji})} \sqrt{a(v_j,v_j)}.
  \end{gather}
\end{assumption}

\begin{note}
  Inequality~\eqref{eq:schwarz:25} with $\epsilon_{ij} \equiv 1$ holds by the
  regular \putindex{Bunyakovsky-Cauchy-Schwarz inequality}.
  But for such a matrix, the
  spectral radius is $J$. As the following lemma will reveal, it is
  necessary to obtain $\rho(\mathcal E)$ independent of $J$ to obtain
  estimate~\eqref{eq:schwarz:23}.
\end{note}

\begin{lemma}
  \label{lemma:schwarz:7}
  Let the estimate~\eqref{eq:schwarz:25} hold. Then,
  estimate~\eqref{eq:schwarz:23} holds with the constant
  $\rho(\mathcal E)$.
\end{lemma}

\begin{proof}
  Let $v\in V$ and its decomposition $v=\sum v_j$ with $v_j\in V_j$ be
  chosen arbitrarily. Then,
  \begin{multline*}
    a(v,v)
    = a\left(\sum_i v_i, \sum_j v_j\right)
    = \sum_{i,j=1}^J a(v_i, v_j)
    \le \sum_{i,j=1}^J \epsilon_{ij} \sqrt{a(v_{\vphantom{j}i},v_i)} \sqrt{a(v_j,v_j)}.
  \end{multline*}
  The latter sum corresponds to a matrix-vector product of the form
  $\vec x^T \mathcal E \vec x$, where the entries of $x$ are of the
  form $\sqrt{a(v_i,v_i)}$. Since $\mathcal E$ is symmetric positive
  definite, this product can be estimated by $\rho(\mathcal E) |\vec
  x|^2$, and thus
  \begin{gather}
    \label{eq:schwarz:38}
    a(v,v) \le \rho(\mathcal E) \sum_{j=1}^J a(v_j,v_j).
  \end{gather}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two-level additive Schwarz preconditioner}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  This preconditioner is in the class of \putindex{domain
    decomposition} methods. The attribute \putindex{two-level} refers
  to the fact that we are considering finite element discretizations
  of~\eqref{eq:itintro:1} on two finite element meshes, the
  \putindex{fine mesh} $\T_h$ on which we desire to compute the
  solution, and the auxiliary \putindex{coarse mesh} $\T_H$. Both
  meshes cover the whole domain $\Omega$ (see
  Figure~\ref{fig:schwarz:ddmeshes}), and each cell of the coarse mesh
  is the union of cells of the fine mesh ($4\times 4$ fine cells in
  the figure).

  In addition to these two meshes, we introduce subdomains
  $\Omega_1,\Omega_2,\dots,\Omega_J$ of $\Omega$ such that each
  $\Omega_j$ is the union of cells in $\T_h$. We require that those
  subdomains overlap each other like the three examples in
  Figure~\ref{fig:schwarz:ddmeshes} on the right. A more precise
  definition of the required overlap follows.
\end{intro}

\begin{figure}[tp]
  \centering
  \begin{tikzpicture}
    \draw[step=.25cm] (-2,-2) grid (2,2);
    \draw[step=1cm,thick] (-2,-2) grid (2,2);
    \draw[step=1cm,thick] (3,-2) grid (7,2);
    \draw[step=.25cm] (3.74,-1.25) grid (5.25,0.25);
    \draw[step=.25cm,red] (3,-2) grid (4.25,-0.75);
    \draw[step=.25cm,green] (4.74,-0.25) grid (7,1.25);
  \end{tikzpicture}
  \caption{Fine mesh and coarse mesh (left) for overlapping domain
    decomposition. Examples for a subdomain decomposition on the
    right.}
  \label{fig:schwarz:ddmeshes}
\end{figure}

\begin{definition}
  \label{definition:schwarz:overlap}
  A covering of $\Omega$ with subdomains $\Omega_j$ is called
  \define{overlapping} with minimal \define{overlap} $\delta$, if for
  each $\Omega_j$ and all $x\in\Omega_j$ holds:
  \begin{gather*}
    \operatorname{dist}(x,\partial\Omega_J\setminus\partial\Omega) <
    \delta\;
    \Rightarrow\; \exists k\neq j: x\in \Omega_k.
  \end{gather*}
\end{definition}

\begin{definition}
  \label{definition:schwarz:finite-covering}
\defindex{NO@$N_O$}
  \defindex{finite covering}
  We say that a family of coverings is finite, if
  there is a constant $N_O$ independent of $\T_h$ and the number of
  subdomains, such that for each $j$ the intersection
  $\Omega_j\cap\Omega_k$ is nonempty for at most $N_O$ subdomains
  $\Omega_k$.
\end{definition}

\begin{definition}
  A smooth \define{partition of unity} with respect to the subdomains
  $\Omega_1,\Omega_2,\dots,\Omega_J$ of $\Omega$ is a set of
  nonnegative functions $\{\phi_1,\dots,\phi_J\}\subset
  C^\infty(\overline\Omega)$ such that
  \begin{xalignat}2
    \label{eq:schwarz:6}
    \phi_j(x) &  = 0
    & \forall x & \in \Omega\setminus\Omega_j, \quad j=1,\dots,J
    \\
    \label{eq:schwarz:7}
    \sum_{j=1}^J \phi_j(x) &= 1
    & \forall x&\in\overline\Omega.
  \end{xalignat}
  Similarly, we can define partitions of unity in $H^1(\Omega)$ or
  partitions of unity which are piecewise $C^1$.
  
  Furthermore, we assume that there is a positive constant $\delta$,
  called \define{overlap}, such that for all $j=1,\dots,J$ there holds
  \begin{gather}
    \label{eq:schwarz:8}
    \norm{\nabla\phi j}_{L^\infty(\Omega)} \lesssim \frac 1\delta,
  \end{gather}
  where the implicit constant is independent of $h$, $\delta$ and $J$.
\end{definition}

\begin{note}
  The term overlap for $\delta$ is justified by the following
  consideration. Let $x \in \Omega_j$ be a point which is not in any
  other $\Omega_i$. Then, $\phi_j(x) = 1$. If~\eqref{eq:schwarz:8} is
  to hold, then it is necessary that
  $\operatorname{dist}(x,\partial\Omega_J) \ge \delta$ (up to a
  constant, but this constant is already
  in~\eqref{eq:schwarz:8}). Thus, the points of distance less than
  $\delta$ from $\partial\Omega_j$ must be elements of another
  subdomain as well, which is then said to overlap with $\Omega_j$.
\end{note}

\begin{example}
  \label{example:schwarz:2}
  On uniform meshes, overlapping subdomains with an overlap of
  $\delta = n h$ with $n=1,2,\ldots$ can be achieved easily by the
  following procedure:
  \begin{enumerate}
  \item Begin with a non overlapping subdivision $\{\Omega_j^0\}$,
    for instance aligned with the mesh cells of $\T_H$.
  \item Add all cells that share at least a vertex with a cell in
    $\Omega_j^0$ to obtain a domain $\Omega_j^1$. After this
    procedure, two neighboring domains will overlap by two cells,
    resulting in $\delta = 2h$.
  \item Repeat this procedure to obtain larger overlaps.
  \end{enumerate}
  
  On the resulting partitions, a partition of unity in $H^1$ can be
  constructed with piecewise linear (bilinear on quadrilaterals)
  functions. For instance for $\Omega_j^1$ this function is
  constructed as follows:
  \begin{enumerate}
  \item Choose $\phi_j(x) = 1/2$ in all vertices on $\partial \Omega_j^0$.
  \item Choose $\phi_j(x) = 0$ in all vertices on $\partial \Omega_j^1$ and outside
    $\Omega_j^1$.
  \item Choose $\phi_j(x) = 1$ in all remaining vertices inside
    $\Omega_j^0$.
    \item Connect these values by linear (on simplicial meshes), bilinear
      (quadrilateral meshes) of trilinear (hexahedral meshes)
      polynomials inside each mesh cell $T\in \T_h$.
  \end{enumerate}
  This partition of unity achieves the estimate~\eqref{eq:schwarz:8}
  with a constant of $1/2$.
\end{example}

\begin{notation}
  \label{par:schwarz:1}
  The solution space of our problem is the space $V=V_h$ given by the
  finite element space on the mesh $\T_h$. We define finite element
  spaces on $\Omega_j$ by
  \begin{gather}
    \label{eq:schwarz:9}
    V_j = \bigl\{ v\in V_h \big| \forall x\in\Omega\setminus\Omega_j :
    v(x) =0\bigr\}.
  \end{gather}
  Additionally, we define the space $V_0 \equiv V_H$ as the finite
  element space on the coarse mesh $\T_H$.  Since the meshes are
  nested, $V_H$ is indeed a subspace of $V_h$. Thus, we obtain a
  decomposition of $V_h$ into $J+1$ subspaces
  \begin{gather*}
    V_h = V_0 + \sum_{j=1}^J V_j,
  \end{gather*}
  where the last $J$ are associated with the subdomains. In fact,
  Lemma~\ref{lemma:schwarz:4} below states that already the spaces
  $V_1$ to $V_J$ are sufficient to span $V_h$. Nevertheless, the
  coarse grid space plays a crucial role in the efficiency of the
  method due to Lemma~\ref{lemma:schwarz:stable-decomposition}.
\end{notation}

\begin{definition}
  Let the spaces $V_j$, $j=0,\dots,J$ be defined as above. Then, the
  \define{two-level additive Schwarz preconditioner} is defined as
  \begin{gather}
    \label{eq:schwarz:10}
    B^{-1}_{\text{TLS}} = \sum_{j=0}^J P_j A^{-1} = \sum_{j=0}^J A_j^{-1} \Pi^T_j,
  \end{gather}
  where $P_j$ is defined according to~\eqref{eq:schwarz:2} and $A_j:
  V_j\to V_j^*$ by
  \begin{gather}
    \scal(A_j u_j, v_j)_V = a(u_j, v_j),\quad\forall u_j, v_j \in V_j.
  \end{gather}
\end{definition}

\begin{note}
  In order to simplify notation, we have assigned index zero to
  $V_H$. Thus, sums in future terms may either start at one, summing
  over subdomains, or at zero, summing over all subspaces.
\end{note}

\begin{lemma}
  \label{lemma:schwarz:4}
  There holds
  \begin{gather}
    \label{eq:schwarz:11}
    V_h = \sum_{j=1}^J V_j.
  \end{gather}
\end{lemma}

\begin{proof}
  Let $I_h: C(\overline\Omega) \to V_h$ be the interpolation operator
  of the finite element space. Then, for any given $v\in V_h$ define
  $v_j = I_h(\phi_j v)$, where $\phi_j$ is the function associated to
  $\Omega_j$ of a partition of unity for $\Omega_1,\dots,\Omega_J$.
  
  By definition of $\phi_j$, there holds $\phi_j v = 0$ on
  $\Omega\setminus\Omega_j$. Furthermore, we assumed that a mesh cell
  of $\T_h$ is either completely in $\Omega_j$ or completely in its
  complement. Since nodal values of a cell are located in the cell
  itself, this implies that $I_h (\phi_j v) = 0$ on
  $\Omega\setminus\Omega_j$. Therefore, $I_h (\phi_j v) \in V_j$.
  
  On the other hand, we use the linearity of the interpolation
  operator to obtain
  \begin{gather*}
    \sum_{j=1}^J v_j = \sum_{j=1}^J I_h(\phi_j v)
    = I_h\left(v\sum_{j=1}^J \phi_h\right)
    = I_h v = v,
  \end{gather*}
  thus, the $v_j$ are indeed a decomposition of $v$. Since $v\in V_h$
  was chosen arbitrarily, the lemma is proven.
\end{proof}

\begin{lemma}
  \label{lemma:schwarz:8}
  Let the covering $\{\Omega_j\}_{j=1,\dots,J}$ for $\Omega$ be finite
  according to
  Definition~\ref{definition:schwarz:finite-covering}. Then, the
  \putindex{strengthened Cauchy-Schwarz inequalities}~\eqref{eq:schwarz:25} hold
  with a spectral radius
  \begin{gather}
    \label{eq:schwarz:26}
    \rho(\mathcal E) \le N_O.
  \end{gather}
\end{lemma}

\begin{proof}
  The term $a(v_i, v_j)$ is nonzero only if the supports of the two
  functions have a nonempty intersection. Accordingly, for each index
  $i$ only a maximum of $N_O$ of the coefficients $\epsilon_{ij}$ are
  nonzero. We set these equal to one and use Gershgorin's theorem to
  estimate the greatest eigenvalue.
\end{proof}

\begin{lemma}
  \label{lemma:schwarz:6}
  Let $v_j\in V_j$ for $j=0,\dots,J$ be a composition of $v\in V_h$
  such that
  \begin{gather*}
    v=\sum_{j=0}^J v_j.
  \end{gather*}
  Then,
  \begin{gather}
    \label{eq:schwarz:12}
    a(v,v) \lesssim \sum_{j=0}^J a(v_j, v_j),
  \end{gather}
  where the implicit constant does not depend on $h$, $H$, or $J$.
\end{lemma}

\begin{proof}
  First a note: the inequality would be obvious, if $V_h$ was a direct
  sum of the spaces $V_j$, and it would hold with a constant of one if
  they were mutually orthogonal. Thus, we have to show some kind of
  orthogonality between the spaces.

  We start out by stating that
  \begin{align*}
    a(v,v) &= a\left(v_0+\sum_{j=1}^J v_j, v_0+\sum_{j=1}^J v_j\right)
    \\
    &\le 2 \left(a(v_0, v_0) + a\!\left(\sum_{j=1}^J v_j,\sum_{j=1}^J
        v_j\right)\right)
    \\
    &= 2 \left(a(v_0, v_0) + \sum_{j,k=1}^J a(v_j,v_k)\right) \\
    & \le 2 a(v_0, v_0) + 2 N_O \sum_{j=0}^J a(v_j, v_j),
  \end{align*}
  where the last inequality is due to Lemma~\ref{lemma:schwarz:7} and
  Lemma~\ref{lemma:schwarz:8}. Since $N_O$ is assumed independent of
  $h$, $H$, and $J$, the lemma is proven.
\end{proof}


\begin{lemma}[Stable decomposition]
  \index{stable decomposition}
  \label{lemma:schwarz:stable-decomposition}
  For each $v\in V_h$ there exists a decomposition $v=\sum_{j=0}^J
  v_j$ with $v_j\in V_j$, such that
  \begin{gather}
    \label{eq:schwarz:13}
    \sum_{j=0}^J a(v_j, v_j)
    \lesssim \left(1+\frac H\delta\right)^2 a(v,v).
  \end{gather}
\end{lemma}

\begin{proof}
  Let $\tilde I_H: H^1_0(\Omega) \to V_H$ be an interpolation operator
  continuous on $H^1(\Omega)$, for instance the interpolation operator
  by Clement or the one by Scott and Zhang. For a given function
  $v\in V_h$, let $v_H = \tilde I_H v$. Then
  \begin{gather}
    \label{eq:schwarz:20}
    \begin{split}
      |v_H|_1 &\lesssim |v|_1 \\
      \norm{v-v_H}_0 &\lesssim H |v|_1.
    \end{split}
  \end{gather}
  From~\eqref{eq:schwarz:20} there holds
  \begin{gather}
    \label{eq:schwarz:22}
    a(v_H,v_H) = |v_H|_1^2 \lesssim |v|_1^2 = a(v,v).
  \end{gather}
  Let $w=v-v_H$. Using a partition of unity $\{\phi_j\}$ for the
  subdomains $\{\Omega_j\}$ and the nodal interpolant $I_h$ as in the
  proof of Lemma~\ref{lemma:schwarz:4}, and let
  \begin{gather*}
    v_j = I_h(\phi_j w).
  \end{gather*}
  Thus, $v = \sum v_j$. We point out, that we can use the nodal
  interpolant, since $w$ is a finite element function on $\T_h$ and
  $\phi_j$ is either smooth or piecewise polynomial. For the remainder
  of this proof, we will assume the piecewise polynomial case (see
  Example~\ref{example:schwarz:2}) and leave the arguments for a
  smooth function $\phi_j$ to the reader.
  
  The interpolation operator is exact for polynomials of degree $k$
  (assuming such an order for the finite element being
  used). Therefore,
  \begin{gather*}
    a(v_j,v_j) = |v_j|_1^2 \lesssim |\phi_j w|_1^2
    \lesssim \norm{\nabla \phi_j w}_0^2 + \norm{\phi_j \nabla w}_0^2.
  \end{gather*}
  Using the properties of the partition of unity, we obtain
  \begin{gather*}
    a(v_j,v_j) \lesssim \frac1{\delta^2} \norm{\chi(\Omega_j) w}_0^2
    + |\chi(\Omega_j)w|_1^2.
  \end{gather*}
  Summing up yields
  \begin{gather}
    \label{eq:schwarz:21}
    \begin{split}
      \sum_{j=1}^J a(v_j, v_j)
      &\lesssim \sum_{j=1}^J \left(
        \frac1{\delta^2} \norm{\chi(\Omega_j) w}_0^2
        + |\chi(\Omega_j)w|_1^2\right) \\
      &\le N_O \left(\frac1{\delta^2}\norm{w}_0^2 + |w|_1^2\right) \\
      &=  N_O \left(\frac1{\delta^2} \norm{v-v_H}_0^2 + |v-v_H|_1^2\right)
      \\
      & \lesssim \frac{H^2}{\delta^2} |v|_1^2 + |v|_1^2 \\
      &= \left(1+\frac{H^2}{\delta^2}\right) a(v,v).      
    \end{split}
  \end{gather}
  The estimate~\eqref{eq:schwarz:13} now follows
  from~\eqref{eq:schwarz:22} and~\eqref{eq:schwarz:21}.
\end{proof}

\begin{theorem}
  \label{theorem:schwarz:two-level-convergence}
  Under the assumptions made so far in this section, there holds
  \begin{gather}
    \label{eq:schwarz:14}
    \kappa(B^{-1}_{\text{TLS}} A_h)
    = \frac{\Lambda(B_{\text{TLS}}, A_h)}{\lambda(B_{\text{TLS}},
      A_h)}
    \lesssim \left(1+\frac H\delta\right)^2,
  \end{gather}
  where the implicit constant is independent of $h$, $\delta$, $H$, and $J$.
\end{theorem}

\begin{proof}
  The proof follows Note~\ref{note:schwarz:1}. Indeed,
  Lemma~\ref{lemma:schwarz:6} proves inequality~\eqref{eq:schwarz:23}
  and Lemma~\ref{lemma:schwarz:stable-decomposition}
  proves~\eqref{eq:schwarz:24}.
\end{proof}

\begin{remark}
  We have constructed a preconditioner $B_{\text{TLS}}$ for the finite
  element discretization of the Poisson problem~\eqref{eq:itintro:1}
  such that the preconditioned system has a bounded condition number
  independent of the mesh size $h$. Thus, a Richardson iteration or a
  conjugate gradient method using this preconditioner will reduce the
  error by a certain amount within a fixed number of steps.
  
  Closer inspection of the estimate~\eqref{eq:schwarz:14} in view of
  Example~\ref{example:schwarz:2} reveals a problem though: typically,
  $\delta$ is of the order of $h$, such that the estimate becomes
  \begin{gather*}
    \kappa(B^{-1}_{\text{TLS}} A_h) \lesssim \left(1+\frac H h\right)^2.
  \end{gather*}
  If we choose $H$ constant, this is exactly as bad as the condition
  number of $A$ itself. Under mild further assumptions, the square on
  the right hand side can be avoided~\cite{DryjaWidlund94}, which
  is an improvement compared to the operator without preconditioning,
  but is not uniform with respect to $h$. Therefore, we are left with
  two options:
  \begin{enumerate}
  \item Increase the overlap such that it is $\mathcal O(H)$. This
    procedure yields a \putindex{uniform preconditioner}, but it
    introduces a problem: when refining $h$, more and more
    cells belong to several subdomains and thus computations on them
    have to be performed several times. Therefore, the effort per
    preconditioning step is increased considerably.
  \item Keep the overlap at a small multiple of $h$ and choose $H$
    such that $H/h$ is bounded by a constant. Then, the preconditioner
    remains uniform, the overlap remains small. This way, the
    difficulty has been transferred to the coarse grid problem on
    $\T_H$, since now this problem becomes more and more difficult to
    solve, when $h$ decreases.
  \end{enumerate}
  
  While the problems of the first option above are inherent and
  unavoidable, the second option is at least seemingly optimal and
  more creativity may be invested into the solution of the coarse grid
  problem. Therefore, the latter is usually preferred. The coarse grid
  problem then leads to the idea of multigrid methods, which will be
  dealt with in Chapter~\ref{cha:iteration:multigrid-methods}.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiplicative Schwarz methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{example}
  If we write the Jacobi method in equation~\eqref{eq:schwarz:28} for
  each line, we get for each $j=1,\ldots,J$:
  \begin{gather}
    \label{eq:schwarz:29}
      u_j^{(k+1)} = u_j^{(k)} - \omega_k P_j \bigl(u^{(k)} - A^{-1}f\bigr).
  \end{gather}
  Since the updates are orthogonal, this is equivalent to the sum
  in~\eqref{eq:schwarz:28}. In the \putindex{Gauss-Seidel method}, these
  projections are done consecutively. For instance, if we introduce
  broken indices, we can write it in the form
  \begin{gather}
    \label{eq:schwarz:30}
      u_j^{(k+\frac{j}{J})} = u_j^{(k+\frac{j-1}{J})}
      - \omega_k P_j \bigl(u^{(k+\frac{j-1}{J})} - A^{-1}f\bigr).
  \end{gather}
  This means, we apply the corrections consecutively one after the
  other. In order to understand convergence of this method, we are
  bringing it into a different form and study the propagation of the
  error. Let $uA^{-1}f$ be the solution of the problem. Then, the
  error propagates like
  \begin{multline}
    \label{eq:schwarz:31}
      u^{(k+\frac{j}{J})} - u = u^{(k+\frac{j-1}{J})} - u
      - \omega_k P_j \left(u^{(k+\frac{j-1}{J})} - u\right) \\
      = (I-\omega_k P_j) \left(u^{(k+\frac{j-1}{J})} - u\right).
  \end{multline}
  The error after a whole Gauss-Seidel step is
  \begin{gather}
    \label{eq:schwarz:32}
      u^{(k+1)} - u
      = (I-\omega_k P_J)(I-\omega_k P_{J-1})\dots(I-\omega_k P_1) \left(u^{(k)} - u\right).
  \end{gather}
  The corresponding error equation for the Jacobi method is
  \begin{gather}
    \label{eq:schwarz:33}
    u^{(k+1)} - u = \left(I-\omega_k \sum_{j=1}^J P_j\right) \left(u^{(k)} - u\right).
  \end{gather}
  Thus the notions of multiplicative and additive methods.
\end{example}

\begin{intro}
  The example of the Jacobi and Gauss-Seidel methods is generic in the
  way that for any subspace decomposition of $V$ into the sum of
  $V_j$, we can define an additive and a multiplicative method. Not
  surprisingly, their analysis also rests on the same ingredients.
\end{intro}

\begin{definition}
  The \define{multiplicative Schwarz preconditioner} for the operator $A$ associated
  with the symmetric, and $V$-elliptic bilinear form $a(.,.)$ with
  respect to the subspace decomposition $V_j$ is the mapping $B_m:
  V \to V^*$ such that
  \begin{gather}
    \label{eq:schwarz:34}
    B_m^{-1} = (I -  E_J) A^{-1},
  \end{gather}
  where $E_J$ is the multiplicative \define{error propagation operator}
  \begin{gather}
    \label{eq:schwarz:35}
    E_J = (I - P_J)(I - P_{J-1})\dots(I - P_1).
  \end{gather}
\end{definition}

\begin{lemma}
  The error after one step of the iteration
  \begin{gather*}
    u^{(k+1)} = u^{(k)} - B_m^{-1}\left(A u^{(k)} - f\right)
  \end{gather*}
  is given by
  \begin{gather*}
    u^{(k+1)} - u = E_J \left(u^{(k)} - u\right).
  \end{gather*}
\end{lemma}

\begin{proof}
  First, we use the definition of $B_m$ to obtain
  \begin{gather*}
    u^{(k+1)} = u^{(k)} - (I -  E_J) \left(u^{(k)} - u\right).
  \end{gather*}
  Therefore,
  \begin{gather*}
    u^{(k+1)} - u = u^{(k)} - u - (I -  E_J) \left(u^{(k)} - u\right)
    = (I-I+E_J) \left(u^{(k)} - u\right).
  \end{gather*}  
\end{proof}

\begin{remark}
  We can define the error propagation operator $E_j$ through the
  recursion
  \begin{gather}
    \label{eq:schwarz:45}
    E_{0} = I, \qquad E_{k} = (I - P_k) E_{k-1}, \quad k=1,\dots,J.
  \end{gather}
\end{remark}

\begin{lemma}
  \label{lemma:schwarz:9}
  The error propagation operator $E_j$ has the following porperties:
  \begin{align}
    \label{eq:schwarz:44}
    E^*_{j-1}E_{j-1} - E^*_{j}E_{j} &= E^*_{j-1} P_j E_{j-1}
    \\
    \label{eq:schwarz:46}
    I - E_j &= \sum_{k=1}^{j} P_k E_{k-1}.
  \end{align}
  Here, $E^*$ is the $a(.,.)$-adjoint of $E$.
\end{lemma}

\begin{proof}
  In order to prove the first identity, we use the recursion
  formula~\eqref{eq:schwarz:45} to obtain (using $P_j = P_j^*$
  and $P_j^2 = P_j$)
  \begin{align*}
    E^*_{j}E_{j} &= E^*_{j-1} (I-P_j^*) (I-P_j) E_{j-1} \\
    &= E^*_{j-1} E_{j-1} - E^*_{j-1}P_jE_{j-1}.
  \end{align*}
  The second identity is proven by induction with
  \begin{align*}
    (I-E_0) &= I - I = 0, \\
    (I-E_j) &= I-(I-P_j) E_{j-1} = P_j  E_{j-1} + I - E_{j-1} = P_j
    E_{j-1} +\sum_{k=1}^{j-1} P_k E_{k-1}.
  \end{align*}
\end{proof}

\begin{lemma}
  \label{lemma:schwarz:10}
  Let Assumption~\ref{assumption:schwarz:1} (\putindex{strengthened
    Cauchy-Schwarz inequalities}) be satisfied. Then the following
  inequalities hold for $0\le j,k \le J$ and for $u,v\in V$:
  \begin{align}
    a(P_j u,v) &\le \sqrt{a(P_j u,u)} \sqrt{a(P_j v,v)}, \\
    a(P_j u,P_k v) &\le \epsilon_{jk}
    \sqrt{a(P_j u,u)} \sqrt{a(P_k v,v)}.
  \end{align}
\end{lemma}

\begin{proof}
  By the definition of $P_j$, the fact that $P_j$ is $a(.,.)$-self
  adjoint (Lemma~\ref{lemma:schwarz:ritz}) and the \putindex{Bunyakovsky-Cauchy-Schwarz
  inequality}, we have
  \begin{multline*}
    a(P_j u,v) = a(u, P_j v) = a(P_j u, P_j v)
    \\
    \le \sqrt{a(P_j u,P_j u)} \sqrt{a(P_j v,P_j v)}
    \le \sqrt{a(P_j u,u)} \sqrt{a(P_j v,v)}.
  \end{multline*}
  The second inequality follows readily by
  \begin{gather*}
    a(P_j u,P_k v)
    \le \epsilon_{jk}\sqrt{a(P_j u,P_j u)} \sqrt{a(P_j v,P_j v)}
    = \epsilon_{jk}\sqrt{a(P_j u,u)} \sqrt{a(P_j v,v)}.
  \end{gather*}
\end{proof}

\begin{theorem}
  Let Assumptions~\ref{assumption:schwarz:1} and the
  estimate~\eqref{eq:schwarz:23} be satisfied. Then, the error
  propagation operator satisfies
  \begin{gather}
    \label{eq:schwarz:47}
    \norm{E_J}_A^2 \le 1 - \frac{1}{\rho(\mathcal E) \constref{eq:schwarz:24}} < 1,
  \end{gather}
\end{theorem}

\begin{proof}
  We use equation~\eqref{eq:schwarz:44} of Lemma~\ref{lemma:schwarz:9} to obtain
  \begin{gather*}
    I- E^*_J E_J = \sum_{j=1}^{j} \left(E^*_{j-1}E_{j-1} -
      E^*_{j}E_{j}\right)
    = \sum_{j=1}^J E^*_{j-1} P_j E_{j-1}.
  \end{gather*}
  Since the operators $P_j$ are positive semi-definite, by rearranging
  we obtain for all $v\in V$ the estimate
  \begin{gather}
    \label{eq:schwarz:49}
    a(E_J v, E_J v) \le a(v,v) - \sum_{j=1}^J a(E_{j-1}v, P_j E_{j-1}v).
  \end{gather}
  Thus, $\norm{E_J}_A^2 \le 1$, but we have to show that the sum on the
  right is sufficiently positive to get an estimate of the convergence
  rate. Therefore, we start with equation~\eqref{eq:schwarz:46} of
  Lemma~\ref{lemma:schwarz:9}, yielding by Lemma~\ref{lemma:schwarz:10}
  \begin{align*}
    a(P_j v,v)
    &= a(P_j v, E_{j-1}v) + \sum_{k=1}^{j-1} a(P_j v, P_k E_{k-1} v)
    \\
    &\le \sqrt{a(P_j v,v)} \left(\sqrt{a(P_j E_{j-1}v,E_{j-1}v)}
    + \sum_{k=1}^{j-1} \epsilon_{jk} \sqrt{a(P_k E_{k-1}v,E_{k-1}v)}
    \right) \\
    & \le \sqrt{a(P_j v,v)} \left(\sum_{k=1}^{j} \epsilon_{jk}
      \sqrt{a(P_k E_{k-1} v,E_{k-1} v)}
    \right)
  \end{align*}
  Now let $z \in \R^J$ be the vector with entries $z_k = \sqrt{a(P_k
    E_{k-1}v,E_{k-1}v)}$. Then, we rewrite the previous estimate as
  \begin{gather*}
    a(P_j v,v) \le (\mathcal E z)_j^2.
  \end{gather*}
  Summing over $j$ yields
  
  \begin{gather}
    \label{eq:schwarz:48}
    \sum_{j=1}^J a(P_j v,v) \le \norm{\mathcal E z}^2
    \le \rho(\mathcal E)^2 \norm{z}^2
    = \rho(\mathcal E)^2 \sum_{k=1}^{j} a(P_k E_{k-1} v,E_{k-1} v).
  \end{gather}
  At this point, we use Lemma~\ref{lemma:schwarz:11} to estimate
  \begin{gather*}
    \frac1{\constref{eq:schwarz:24}^2} a(v,v)
    \le \sum_{j=1}^J a(P_j v,v)
    \le \rho(\mathcal E)^2 \sum_{k=1}^{j} a(P_k E_{k-1} v,E_{k-1} v)
  \end{gather*}
  Finally, entering this estimate into~\eqref{eq:schwarz:49}, we
  obtain
  \begin{gather*}
    a(E_J v, E_J v)
    \le a(v,v) \left(1-\frac1{\rho(\mathcal
        E)^2\constref{eq:schwarz:24}^2}
      \right),
  \end{gather*}
  which is the statement of the theorem.
\end{proof}

\begin{lemma}
  \label{lemma:schwarz:11}
  Let inequality~\eqref{eq:schwarz:24} hold for some \putindex{stable
    decomposition} $v=\sum v_j$ with the constant
  $\constref{eq:schwarz:24}$. Then,
  \begin{gather}
    \label{eq:schwarz:50}
    \sum_{j=1}^J a(P_j v,v) \ge \frac1{\constref{eq:schwarz:24}^2} a(v,v).
  \end{gather}
\end{lemma}

\begin{proof}
  From inequality~\eqref{eq:schwarz:24} for the decomposition $v=\sum
  v_j$,\ and the definition of $P_j$, and the
  \putindex{Bunyakovsky-Cauchy-Schwarz inequality}, we obtain
  \begin{align*}
    a(v,v) &= \sum_{j=1}^J a(v,v_j) \\
    &= \sum_{j=1}^J a(P_j v, v_j) \\
    &\le \sqrt{\sum_{j=1}^J a(P_j v, P_j v)} \;\sqrt{\sum_{j=1}^J a(v_j,
      v_j)} \\
    & \le \sqrt{\sum_{j=1}^J a(P_j v, P_j v)}\;
    \constref{eq:schwarz:24} \sqrt{a(v,v)}.
  \end{align*}
  Thus,
  \begin{gather*}
    a(v,v) \le \constref{eq:schwarz:24}^2 \sum_{j=1}^J a(P_j v, P_j v)
    = \constref{eq:schwarz:24}^2 \sum_{j=1}^J a(v, P_j v).
  \end{gather*}
  This proves the lemma.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{intro}
  In order to keep the presentation and the proofs simple, we have
  presented Schwarz methods in their most straight forward form. The
  framework allows for extensions, which each add minor
  complications to the proofs and yield slightly modified results. We
  are going to mention a few of them.
\end{intro}

\begin{remark}
  The definition of the additive Schwarz operator in
  equation~\eqref{eq:schwarz:4} and the multiplicative Schwarz
  operator~\eqref{eq:schwarz:34} rely on the Ritz projections $P_j$,
  which in turn, through their definition in~\eqref{eq:schwarz:2}
  require solving the local problems with the operators $A_j$ exactly.
  
  In some cases, it might be advantageous either to solve a different
  local problem, or to solve the local problem approximately. In both
  cases, we can rewrite the algorithm as using a different local
  projection $\widetilde P_j$ which instead of the Ritz
  projection~\eqref{eq:schwarz:2} is defined by the equation
  \begin{gather}
    \label{eq:schwarz:42}
    \widetilde a_j (\widetilde P_j u_j,v_j) = a(u,v_j),\quad\forall v_j\in V_j,
  \end{gather}
  with a corresponding operator $\widetilde A_j: V_j \to V_j^*$. We
  will continue to assume that $\widetilde a_j (.,.)$ is symmetric and
  elliptic in the same way as $a(.,.)$ is, but possibly with different
  constants.

  It is obvious, that we will need assumptions on the modified
  bilinear forms $\widetilde a_j (.,.)$and their relationship with
  $a(.,.)$. But it turns out, that if we make these additional
  assumptions, we can make the replacements of $P_j$ by $\widetilde
  P_j$ in the algorithm and the analysis carries through with just one
  additional parameter involved.
  
  The modifications in the analysis are as follows: first, replace the
  stable decomposition lemma~\ref{lemma:schwarz:stable-decomposition}
  by assumption~\ref{assumption:schwarz:stable-decomposition-2}. Then,
  introduce the additional assumption
  \ref{assumption:schwarz:local-stability}, which is ellipticity of
  the modified forms in $V_j$ with respect to the norm established by the
  original bilinear form. Both assumptions together establish a
  relaxed form (only for the sum over $j$) of spectral equivalence for
  $\widetilde a_j(.,.)$ and $a(.,.)$ on $V_j$. The \putindex{strengthened
  Cauchy-Schwarz inequalities} in Assumption~\ref{assumption:schwarz:1}
  remain the same.
\end{remark}

\begin{assumption}[Stable decomposition]
  \index{stable decomposition}
  \label{assumption:schwarz:stable-decomposition-2}
    For each $v\in V_h$ there exists a decomposition $v=\sum_{j=0}^J
  v_j$ with $v_j\in V_j$, such that
  \begin{gather}
    \label{eq:schwarz:36}
    \sum_{j=0}^J \widetilde a_j(v_j, v_j)
    \lesssim a(v,v),
  \end{gather}
  where the implicit constant is independent of the number of
  subdomains $J$.
\end{assumption}

\begin{assumption}[Local stability]
  \index{local stability}
  \label{assumption:schwarz:local-stability}
  There is a constant $\omega > 0$ such that
  \begin{gather}
    \label{eq:schwarz:37}
    a(v_j, v_j) \le \omega \widetilde a_j(v_j, v_j)
    \quad\forall j=1,\dots,J \; \forall v_j\in V_j.
  \end{gather}
\end{assumption}

\begin{remark}
  With these two assumptions, the convergence estimates change as
  follows: first, using the \putindex{strengthened Cauchy-Schwarz
  inequalities}~\eqref{eq:schwarz:25}, we extend ~\eqref{eq:schwarz:38}
  to
  \begin{gather}
    \label{eq:schwarz:39}
    a(v,v) \le \omega \rho(\mathcal E) \sum_{j=1}^J \widetilde a_j(v_j, v_j).
  \end{gather}
  Then, the proof of Theorem~\ref{theorem:schwarz:1} can be conducted
  in the very same way as before, using~\eqref{eq:schwarz:36}
  and~\eqref{eq:schwarz:39}.
\end{remark}

\begin{example}
  A simple example is the introduction of a relaxation parameter
  $\omega$ such that
  \begin{gather*}
    \widetilde a_j(v_j, v_j) = \frac1\omega a(v_j, v_j).
  \end{gather*}
  Then, obviously, the local stability~\eqref{eq:schwarz:37}
  holds. Furthermore, the constant in the stable decomposition
  estimate changes by a factor $1/\omega$. Thus, in this case, the
  upper and lower bounds $\Lambda(B,A)$ and $\lambda(B,A)$ change by
  the same factor and the condition number stays the same.
\end{example}

\begin{remark}
  The second extension is, that we can replace the subspaces $V_j$ by
  \putindex{auxiliary space}s $X_j$, which are not subspaces of
  $V$. In such a situation, we have to require the existence of a
  \putindex{prolongation} or \putindex{extension operator} $R_j^T: X_j
  \to V$. This situation is adapted most easily to our existing
  framework by introducing subspaces $V_j$ as the range of $R_j^T$,
  namely,
  \begin{gather}
    \label{eq:schwarz:40}
    V_j = \bigl\{ R_j^T x \in V \big| x\in X_j \bigr\}.
  \end{gather}
  Then, the local forms are defined on the auxiliary spaces,
  \begin{gather}
    \label{eq:schwarz:41}
    \widetilde a_j(.,.) : X_j\times X_j \to \R, \quad j=1,\dots,J,
  \end{gather}
  and wherever we need a vector in $V_j$, we use the interpolation
  operator. Thus, we introduce decompositions of the form $v = \sum
  \alpha_j R_j^T x_j$ and the operator $\widetilde P_j: V \to X_j$ is
  defined by
  \begin{gather}
    \label{eq:schwarz:43}
    \widetilde a_j(\widetilde P_j u, y_j) = a(u, R_j^T y_j),
    \quad \forall y_j \in X_j.
  \end{gather}
  These modifications introduce new operators, but they do not affect
  the analysis.
\end{remark}

\begin{example}
  When we implement a block-Jacobi method, we have $V = \R^n$ and
  $m$-dimensional subspaces with $J=n/m$ (we assume the quotient is
  integer). In the standard formulation with subspaces $V_j$, the
  operators $A_j$, which have to be inverted, are $n\times n$-matrices
  with only $m$ rows and columns different from zero. This is not a
  useful description of the local problems. Instead, we want an
  invertible matrix $A_j \in \R^{m\times m}$. This can be achieved by
  choosing the extension operator
  \begin{gather}
    R_j^T : \R^m \to \R_n,
    \qquad
    \begin{pmatrix}
      x_1 \\ \vdots \\ x_m
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
      0\\ \vdots \\ 0 \\
      v_{mj+1} = x_1 \\ \vdots \\ v_{m(j+1)} = x_m
      \\ 0\\ \vdots \\ 0
    \end{pmatrix}.
  \end{gather}
\end{example}

\begin{example}
  Other examples are finite element methods with nonnested spaces on
  different levels, for instance, when the meshes of a hierarchy are
  not nested.
\end{example}

\begin{remark}
  We have discussed additive and multiplicative Schwarz methods, but
  we are not forced to consider only methods organized strictly in one
  way or the other. Instead, some of the operations can be performed
  in the additive, some in the multiplicative way.
  
  This is advantageous for instance on multicore hardware: an
  inspection of the algorithms shows, that application of all
  operators $P_j$ can be implemented in parallel, while the
  applications of the operators $(I-P_j)$ in the multiplicative method
  is sequential.
\end{remark}

\begin{example}
  Let the indices $j=1,\dots,J$ be grouped into $M$ subsets $I_m$,
  such that
  \begin{gather*}
    P_iP_k = P_k P_i = 0,\qquad \forall i,k\in I_m.
  \end{gather*}
  Such a distribution of indices is also called
  \define{coloring}. Then,
  \begin{gather*}
    (I-P_i)(I-P_k) = I- (P_i+P_k),
  \end{gather*}
  and by application to the whole subset and all subsets, the error
  propagation operator of the multiplicative method can be rewritten
  as
  \begin{gather*}
    E_J = \left(I-\sum_{j\in I_1} P_j \right)
    \cdots
    \left(I-\sum_{j\in I_m} P_j \right).
  \end{gather*}
  While the operations inside each ``color'' are arranged in an
  additive and thus parallelizable manner, the colers themselves are
  arranged in a multiplicative way.
\end{example}

\begin{example}
  Another example is a rearrangement of the two-level Schwarz method
  in a way, that the domain decomposition subspaces are still dealt
  with in an additive fashion, while the coarse space is connected
  multiplicatively. An example for a symmetric version of this method
  is described by the error propagation operator
  \begin{gather}
    \label{eq:schwarz:51}
    E_{TL} = \left(I-\sum_{j=1}^J P_j \right) (I-P_0)
    \left(I-\sum_{j=1}^J P_j \right),
  \end{gather}
  which is the two-level operator with Schwarz smoother discussed in
  the next chapter.
\end{example}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
